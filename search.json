[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Selected Projects",
    "section": "",
    "text": "1. Gender Disparities in Labor Force Analysis (2024)\nWebsite Research Paper PPT\n\nLabor Market Analytics Research Project | Sep 2025 – Dec 2025\n\nAnalyzed Lightcast and FRED data in Python, applying regression and ML models to forecast 2024 labor-market trends\nBuilt visualizations to assess skill demand, salary patterns, geographic variation, and gender-based differences across U.S. markets\nSynthesized model results with academic research to deliver actionable insights on hiring behavior and gender disparities\n\n\n\n\n2. Factors That Influence Airbnb Prices in Geneva\nResearch Paper PPT\n\nApplied Machine Learning for Airbnb Pricing in Geneva | Sep 2025 – Dec 2025\n\nApplied Python for data visualization, regression, k-NN, classification trees, NLP transformers, and clustering to analyze Airbnb pricing in Geneva\nEngineered features from structured and unstructured listing data to build predictive and market segmentation models\nGenerated actionable insights to inform Airbnb hosts’ pricing decisions and listing strategies, and to support platform-level market segmentation in the Geneva market\n\n\n\n\n3. Bangladesh Garment Industry: Health and Environmental Impacts\nResearch Paper\n\nEconomics Seminar Research Paper | Jan 2021 – Mar 2021\n\nConducted a literature review on short- and long-term effects of Bangladesh’s garment industry on health and environment\nExamined workers’ health, safety evaluations, and child labor issues using historical data\nProposed strategies to mitigate risks while supporting sustainable industry growth"
  },
  {
    "objectID": "certifications.html",
    "href": "certifications.html",
    "title": "Certifications",
    "section": "",
    "text": "Verified certifications that demonstrate expertise in technology, data analytics, and financial markets.\n\n  \n    AWS Academy Cloud Foundations\n  \n\n  \n    Developed a strong understanding of cloud computing concepts, AWS services, architecture, security, and pricing, enhancing my ability to leverage cloud technologies for business innovation and digital transformation.\n\n    \n      AWS\n      Cloud Computing\n      Security\n      Digital Transformation\n    \n\n    \n      View Credential\n    \n  \n\n\n  \n    Bloomberg Market Concepts (BMC)\n  \n\n  \n    Completed the self-paced BMC certification, covering core pillars of the financial markets including economics, currencies, fixed income, and equities through the Bloomberg Terminal. Strengthened financial literacy, market analysis, and data-driven decision-making skills.\n\n    \n      Financial Markets\n      Equities\n      Fixed Income\n      Data Analysis\n    \n\n    \n      View Credential"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "You can download my CV here: CV (PDF)"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Curriculum Vitae",
    "section": "Education",
    "text": "Education\nBoston University, Boston, MA\nM.S. in Business Analytics | Expected May 2026\nDrexel University, Philadelphia, PA\nB.S. in Economics | 2017 – 2021\nMinor: Finance | Concentration: Business Economics"
  },
  {
    "objectID": "cv.html#skills",
    "href": "cv.html#skills",
    "title": "Curriculum Vitae",
    "section": "Skills",
    "text": "Skills\n\nProgramming Languages: Python, SQL, Eviews, R\nData Analysis Tools: Bloomberg, Power BI, Tableau, Macrobond, Haver, Microsoft Office (Word, Excel, PowerPoint)\n\nTechnical Skills: Data Visualizations, Data Manipulation, Database Management, Macro Research, ML, GenAI, Regression, XGBoost, Random Forest"
  },
  {
    "objectID": "cv.html#professional-experience",
    "href": "cv.html#professional-experience",
    "title": "Curriculum Vitae",
    "section": "Professional Experience",
    "text": "Professional Experience\n\nBrandywine Global Investment Management, Philadelphia, PA\nGlobal Macro Research Analyst | Feb 2022 – Aug 2024\n\nManaged and maintained large economic datasets and visualizations to support research and analysis.\n\nConducted macroeconomic studies and built models to guide investment strategies.\n\nDeveloped client presentations and marketing materials to communicate research insights.\n\n\n\n\nKPIM Joint Stock Company, Hanoi, Vietnam\nBusiness Intelligence Analyst | Mar 2021 – Sep 2021\n\nDeveloped dashboards and reports to monitor financial performance and support decision-making.\n\nAnalyzed business data to identify trends, opportunities, and potential risks.\n\nStreamlined reporting processes and coordinated with teams to ensure accuracy.\n\n\n\n\nErnst & Young LLP, Hanoi, Vietnam\nIntern – Assurance Services | May 2020 – Sep 2020\n\nPerformed financial analysis and evaluation of client performance.\nImplemented process improvements and extracted data-driven insights for advisory projects.\nCollaborated with cross-functional teams to complete engagements efficiently.\n\n\n\n\nCity of Philadelphia – Streets Department, Philadelphia, PA\nIntern – Office of the Deputy Commissioner of Streets | Apr 2019 – Sep 2019\n\nMonitored project budgets and maintained financial records.\n\nOrganized and processed data to enhance reporting accuracy.\nPrepared performance reports to inform departmental decision-making."
  },
  {
    "objectID": "python_notebooks/market_segment_conjoint.html",
    "href": "python_notebooks/market_segment_conjoint.html",
    "title": "Market Segmentation & Conjoint Analysis",
    "section": "",
    "text": "Analyzed Lobster Land’s operations using k-means clustering to segment park days based on attendance, pricing, weather, and spending behavior, uncovering distinct day types to inform staffing, pricing, and marketing decisions. Conducted a conjoint analysis with a linear regression model to quantify how entertainment features, crowding, and experience design impact guest satisfaction, translating insights into actionable recommendations for revenue and experience optimization.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans"
  },
  {
    "objectID": "python_notebooks/market_segment_conjoint.html#part-i-segmentation",
    "href": "python_notebooks/market_segment_conjoint.html#part-i-segmentation",
    "title": "Market Segmentation & Conjoint Analysis",
    "section": "Part I: Segmentation",
    "text": "Part I: Segmentation\n\n# Import day_25.csv files into python for use\nday_25 = pd.read_csv(\"data/days25.csv\")\n\n\n# Then use info() and head() function to show how the data is stored\nday_25.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 99 entries, 0 to 98\nData columns (total 19 columns):\n #   Column                  Non-Null Count  Dtype  \n---  ------                  --------------  -----  \n 0   Day_ID                  99 non-null     int64  \n 1   Day_of_Week             99 non-null     object \n 2   season_week_rel         99 non-null     int64  \n 3   Is_Holiday              99 non-null     int64  \n 4   Total_Visitors          99 non-null     int64  \n 5   Passholder_Percentage   99 non-null     float64\n 6   Day_Tickets_Sold        99 non-null     int64  \n 7   Avg_Ticket_Price        99 non-null     float64\n 8   Gate_Revenue            99 non-null     float64\n 9   Revenue_Food            99 non-null     float64\n 10  Revenue_Merch           99 non-null     float64\n 11  Revenue_Arcade          99 non-null     float64\n 12  Total_Revenue           99 non-null     float64\n 13  Total_Labor_Hours       99 non-null     float64\n 14  International_Visitors  99 non-null     float64\n 15  High_Temperature        99 non-null     float64\n 16  Weather_Type            99 non-null     object \n 17  Is_Special_Event        99 non-null     int64  \n 18  Per_Capita_Spend        99 non-null     float64\ndtypes: float64(11), int64(6), object(2)\nmemory usage: 14.8+ KB\n\n\n\nday_25.head()\n\n\n\n\n\n\n\n\nDay_ID\nDay_of_Week\nseason_week_rel\nIs_Holiday\nTotal_Visitors\nPassholder_Percentage\nDay_Tickets_Sold\nAvg_Ticket_Price\nGate_Revenue\nRevenue_Food\nRevenue_Merch\nRevenue_Arcade\nTotal_Revenue\nTotal_Labor_Hours\nInternational_Visitors\nHigh_Temperature\nWeather_Type\nIs_Special_Event\nPer_Capita_Spend\n\n\n\n\n0\n1\nMonday\n1\n1\n1769\n48.65\n908\n77.42\n70297.36\n40521.91\n15881.290\n3195.46\n129896.02\n238.1\n114.0\n74.6\nThunderstorms\n0\n73.43\n\n\n1\n2\nTuesday\n1\n0\n1717\n60.58\n677\n69.80\n47254.60\n35298.92\n15882.665\n2977.63\n101812.36\n253.4\n255.0\n74.0\nPartly Cloudy\n0\n59.30\n\n\n2\n3\nWednesday\n1\n0\n1600\n30.84\n1107\n70.09\n77589.63\n36495.95\n14954.040\n2683.58\n131723.20\n213.3\n160.0\n76.8\nPartly Cloudy\n0\n82.33\n\n\n3\n4\nThursday\n1\n0\n1037\n19.02\n840\n72.30\n60732.00\n22486.65\n9517.060\n1831.98\n94567.69\n192.7\n83.0\n75.4\nShowers\n0\n91.19\n\n\n4\n5\nFriday\n1\n0\n1671\n66.18\n565\n68.11\n38482.15\n40705.15\n15727.620\n3115.94\n98030.86\n235.9\n158.0\n80.7\nPartly Cloudy\n0\n58.67\n\n\n\n\n\n\n\n\n# Convert binary flags (0,1) including: Is_Holiday and Is_Special_Event to categorical\nday_25[['Is_Holiday','Is_Special_Event']] = day_25[['Is_Holiday','Is_Special_Event']].astype('category')\n\n# Convert season_week_rel, Day_of_Week and Weather_Type from object to categorical variables\nday_25[['Day_of_Week','season_week_rel','Weather_Type']] = day_25[['Day_of_Week','season_week_rel','Weather_Type']].astype('category')\n\n\n# Drop the Day_ID variable\nday_25 = day_25.drop(columns=[\"Day_ID\"])\n\nIn this case, I decided to drop Day_ID because it’s just a sequential label and doesn’t seem to offer meaningful insight for analysis or prediction. Since it might unintentionally influence models by introducing artificial patterns, it felt more appropriate to leave it out of the feature set.\n\n# Double check to see if all the variables are stored as the appropriate data type\nday_25.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 99 entries, 0 to 98\nData columns (total 18 columns):\n #   Column                  Non-Null Count  Dtype   \n---  ------                  --------------  -----   \n 0   Day_of_Week             99 non-null     category\n 1   season_week_rel         99 non-null     category\n 2   Is_Holiday              99 non-null     category\n 3   Total_Visitors          99 non-null     int64   \n 4   Passholder_Percentage   99 non-null     float64 \n 5   Day_Tickets_Sold        99 non-null     int64   \n 6   Avg_Ticket_Price        99 non-null     float64 \n 7   Gate_Revenue            99 non-null     float64 \n 8   Revenue_Food            99 non-null     float64 \n 9   Revenue_Merch           99 non-null     float64 \n 10  Revenue_Arcade          99 non-null     float64 \n 11  Total_Revenue           99 non-null     float64 \n 12  Total_Labor_Hours       99 non-null     float64 \n 13  International_Visitors  99 non-null     float64 \n 14  High_Temperature        99 non-null     float64 \n 15  Weather_Type            99 non-null     category\n 16  Is_Special_Event        99 non-null     category\n 17  Per_Capita_Spend        99 non-null     float64 \ndtypes: category(5), float64(11), int64(2)\nmemory usage: 12.1 KB\n\n\n\nNumeric Variables: Total_Visitors, Passholder_Percentage, Day_Tickets_Sold, Avg_Ticket_Price, Gate_Revenue, Revenue_Food, Revenue_Merch, Revenue_Arcade, Total_Revenue, Total_Labor_Hours, International_Visitors, High_Temperature, Per_Capita_Spend\nCategorical Variables: Day_ID, Day_of_Week, season_week_rel, Is_Holiday, Weather_Type, Is_Special_Event\n\nIn my opinion, it’s not a great idea to use categorical inputs in a k-means model because the algorithm calculates distances between data points, and turning categories into numbers can be misleading. Take season_week_rel for example, it uses values from 1 to 15 to represent different weeks, but those numbers don’t really capture how similar or different the weeks are in terms of things like visitor patterns or revenue. The model might end up grouping week 1 and week 2 together just because their numbers are close, even if they behave very differently. Since k-means tries to minimize distance within clusters, it could create groups that look related mathematically but don’t actually make much sense in the real-world context of the data.\n\nCall the describe() function on your dataset.\n\nday_25.describe()\n\n\n\n\n\n\n\n\nTotal_Visitors\nPassholder_Percentage\nDay_Tickets_Sold\nAvg_Ticket_Price\nGate_Revenue\nRevenue_Food\nRevenue_Merch\nRevenue_Arcade\nTotal_Revenue\nTotal_Labor_Hours\nInternational_Visitors\nHigh_Temperature\nPer_Capita_Spend\n\n\n\n\ncount\n99.000000\n99.000000\n99.000000\n99.000000\n99.000000\n99.000000\n99.000000\n99.000000\n99.000000\n99.000000\n99.000000\n99.000000\n99.000000\n\n\nmean\n1870.707071\n41.973232\n1098.292929\n70.439697\n79484.778182\n54715.882626\n18691.392273\n4450.617879\n147911.785253\n256.535354\n178.464646\n77.126263\n76.930808\n\n\nstd\n641.078735\n15.581994\n466.750518\n10.680450\n37053.465251\n97737.730733\n8077.026667\n2992.738130\n62854.729274\n47.232843\n85.522800\n13.975537\n12.582334\n\n\nmin\n-50.000000\n11.720000\n322.000000\n-25.000000\n22350.020000\n22486.650000\n9517.060000\n1831.980000\n63247.570000\n192.700000\n-10.000000\n63.800000\n51.630000\n\n\n25%\n1482.000000\n28.895000\n771.000000\n68.250000\n53853.740000\n33291.365000\n13492.635000\n2693.595000\n103186.105000\n222.400000\n117.500000\n70.850000\n68.575000\n\n\n50%\n1704.000000\n40.900000\n995.000000\n70.480000\n70272.360000\n38054.660000\n15882.665000\n3026.520000\n129433.130000\n243.500000\n158.000000\n74.800000\n77.500000\n\n\n75%\n2080.000000\n54.590000\n1411.000000\n73.735000\n103147.720000\n55612.790000\n22567.790000\n5419.410000\n184173.685000\n272.000000\n232.500000\n81.350000\n84.385000\n\n\nmax\n3938.000000\n73.650000\n3029.000000\n82.280000\n242229.130000\n999999.990000\n51601.440000\n15755.610000\n423019.370000\n435.400000\n560.000000\n200.000000\n110.860000\n\n\n\n\n\n\n\nWhen I run the describe() function on this dataset, it provides a concise statistical summary of each numeric column, including the count, mean, standard deviation, and range values. This snapshot helps me quickly understand the distribution and behavior of each variable, which is essential for assessing data quality and preparing for modeling.\nIn this case, the summary reveals several problematic entries that need attention. For example, Total_Visitors, International_Visitors, and Avg_Ticket_Price contain negative values, which are not realistic in the context of theme park operations. Another noticeable issue in the summary table is that Revenue_Food shows a maximum value of 999,999.99, which exceeds the Total_Revenue maximum of 423,019.37, even though Total_Revenue should represent the combined daily revenue across all categories (gate, food, merchandise, and arcade). Identifying these issues early allows me to flag potential data entry errors or outliers and decide whether to clean, transform, or exclude certain records. This step is crucial for ensuring that the model is built on reliable and interpretable data.\n\n\nMissing values / Impossible values\n\nday_25.isnull().sum()\n\nDay_of_Week               0\nseason_week_rel           0\nIs_Holiday                0\nTotal_Visitors            0\nPassholder_Percentage     0\nDay_Tickets_Sold          0\nAvg_Ticket_Price          0\nGate_Revenue              0\nRevenue_Food              0\nRevenue_Merch             0\nRevenue_Arcade            0\nTotal_Revenue             0\nTotal_Labor_Hours         0\nInternational_Visitors    0\nHigh_Temperature          0\nWeather_Type              0\nIs_Special_Event          0\nPer_Capita_Spend          0\ndtype: int64\n\n\nAfter running the code and checking the results for missing values, the output indicates that all columns contain zero missing values, suggesting that the dataset is complete and no data cleaning through imputation or row removal is necessary.\n\nday_25.select_dtypes(include=\"number\").lt(0).sum()\n\nTotal_Visitors            1\nPassholder_Percentage     0\nDay_Tickets_Sold          0\nAvg_Ticket_Price          1\nGate_Revenue              0\nRevenue_Food              0\nRevenue_Merch             0\nRevenue_Arcade            0\nTotal_Revenue             0\nTotal_Labor_Hours         0\nInternational_Visitors    1\nHigh_Temperature          0\nPer_Capita_Spend          0\ndtype: int64\n\n\n\n# Look for rows with negative values in Total_Visitors, Avg_Ticket_Price and International_Visitors \nday_25[(day_25[\"Total_Visitors\"] &lt; 0) | \n       (day_25[\"Avg_Ticket_Price\"] &lt; 0) | \n       (day_25[\"International_Visitors\"] &lt; 0)]\n\n\n\n\n\n\n\n\nDay_of_Week\nseason_week_rel\nIs_Holiday\nTotal_Visitors\nPassholder_Percentage\nDay_Tickets_Sold\nAvg_Ticket_Price\nGate_Revenue\nRevenue_Food\nRevenue_Merch\nRevenue_Arcade\nTotal_Revenue\nTotal_Labor_Hours\nInternational_Visitors\nHigh_Temperature\nWeather_Type\nIs_Special_Event\nPer_Capita_Spend\n\n\n\n\n10\nThursday\n2\n0\n-50\n72.26\n444\n69.49\n30853.56\n40019.25\n15179.56\n2722.04\n88774.41\n228.6\n75.0\n82.4\nPartly Cloudy\n0\n55.52\n\n\n20\nSunday\n3\n0\n2691\n27.24\n1958\n-25.00\n146771.68\n67432.83\n26147.56\n7571.99\n247924.06\n305.3\n178.0\n79.5\nCloudy\n0\n92.13\n\n\n50\nTuesday\n8\n0\n1204\n72.60\n330\n68.39\n22568.70\n27999.37\n11246.96\n2202.88\n64017.91\n205.4\n-10.0\n78.0\nRain\n0\n53.17\n\n\n\n\n\n\n\n\n# Find rows with suspiciously high food revenue\nday_25[day_25[\"Revenue_Food\"] &gt; day_25[\"Total_Revenue\"]]\n\n\n\n\n\n\n\n\nDay_of_Week\nseason_week_rel\nIs_Holiday\nTotal_Visitors\nPassholder_Percentage\nDay_Tickets_Sold\nAvg_Ticket_Price\nGate_Revenue\nRevenue_Food\nRevenue_Merch\nRevenue_Arcade\nTotal_Revenue\nTotal_Labor_Hours\nInternational_Visitors\nHigh_Temperature\nWeather_Type\nIs_Special_Event\nPer_Capita_Spend\n\n\n\n\n30\nWednesday\n5\n0\n1772\n18.77\n1439\n69.82\n100470.98\n999999.99\n16262.36\n2974.24\n165467.64\n238.3\n137.0\n86.4\nCloudy\n0\n93.38\n\n\n\n\n\n\n\n\n# Find rows with suspiciosly high temperature \nday_25[day_25[\"High_Temperature\"] &gt; 120]\n\n\n\n\n\n\n\n\nDay_of_Week\nseason_week_rel\nIs_Holiday\nTotal_Visitors\nPassholder_Percentage\nDay_Tickets_Sold\nAvg_Ticket_Price\nGate_Revenue\nRevenue_Food\nRevenue_Merch\nRevenue_Arcade\nTotal_Revenue\nTotal_Labor_Hours\nInternational_Visitors\nHigh_Temperature\nWeather_Type\nIs_Special_Event\nPer_Capita_Spend\n\n\n\n\n40\nSaturday\n6\n0\n1933\n25.14\n1447\n74.99\n108510.53\n54861.67\n19528.12\n5038.63\n187938.95\n251.4\n194.0\n200.0\nRain\n0\n97.23\n\n\n\n\n\n\n\nTo identify impossible values, I focused on numeric variables and used the command day_25.select_dtypes(include=“number”).lt(0).sum(). This function isolates numeric columns and checks for any values less than zero, providing a quick count of negative entries per column. This approach is useful because variables like visitor counts, ticket sales, and revenue figures cannot logically be negative in a theme park setting. At the same time, Passholder_Percentage falls within a reasonable 0–100% range, so those values appear consistent.\nThe results showed three impossible values in Total_Visitors, International_Visitors, and Avg_Ticket_Price. These values are considered impossible because visitor counts cannot be negative - they must be zero or greater. Similarly, ticket prices are typically non-negative, unless there’s a rare case involving refunds or discounts. In this context, negative values likely indicate data entry errors or corrupted records.\nThe summary statistics also showed that Revenue_Food has a maximum value of 999,999.99, whereas Total_Revenue only has a maximum of 423,019.37. Because Total_Revenue should include all revenue categories, this should be corrected or reviewed before any further analysis.\nAnother entry that appears suspicious is High_Temperature, which shows a maximum value of 200°F. While temperature ranges can vary by region, typical readings in the United States generally fall between 0°F and a little over 100°F. Even Boston’s recent heatwave only reached a record high of 102°F, so a value of 200°F is unrealistic. To verify, I filtered the High_Temperature variable for values greater than 120°F and found only one record, suggesting this is likely a data entry or recording error.\n\n\nIf so, handle them in any way that you see fit.\n\n# Drop rows where Food_Revenue exceeds Total_Revenue\nday_25 = day_25[day_25[\"Revenue_Food\"] &lt;= day_25[\"Total_Revenue\"]]\n\nIn this case, the simplest and most reasonable solution is to remove the entire row, since even if I replaced Revenue_Food with a typical value (like the median), Total_Revenue would still end up smaller than the sum of its parts, which doesn’t make sense for this dataset.\n\n# Drop rows where  High_Temperature is abnormally high\nday_25 = day_25[day_25[\"High_Temperature\"] &lt;= 120]\n\nI decided to remove the row with the unusually high temperature value instead of imputing it with the median, since it appeared to be an outlier that could distort the results. By removing it, I ensure the dataset remains accurate and free from unrealistic values.\n\n# Filter so that all variables is non-negative\nday25 = day_25[\n    (day_25[\"Total_Visitors\"] &gt;= 0) &\n    (day_25[\"Avg_Ticket_Price\"] &gt;= 0) &\n    (day_25[\"International_Visitors\"] &gt;= 0)\n]\n\nday25.select_dtypes(include=\"number\").lt(0).sum()\n\nTotal_Visitors            0\nPassholder_Percentage     0\nDay_Tickets_Sold          0\nAvg_Ticket_Price          0\nGate_Revenue              0\nRevenue_Food              0\nRevenue_Merch             0\nRevenue_Arcade            0\nTotal_Revenue             0\nTotal_Labor_Hours         0\nInternational_Visitors    0\nHigh_Temperature          0\nPer_Capita_Spend          0\ndtype: int64\n\n\n\n# Now, day25 dataframe has 94 entries for each variable after dropping the 5 impossible rows.\nday25.count()\n\nDay_of_Week               94\nseason_week_rel           94\nIs_Holiday                94\nTotal_Visitors            94\nPassholder_Percentage     94\nDay_Tickets_Sold          94\nAvg_Ticket_Price          94\nGate_Revenue              94\nRevenue_Food              94\nRevenue_Merch             94\nRevenue_Arcade            94\nTotal_Revenue             94\nTotal_Labor_Hours         94\nInternational_Visitors    94\nHigh_Temperature          94\nWeather_Type              94\nIs_Special_Event          94\nPer_Capita_Spend          94\ndtype: int64\n\n\nI chose to drop the rows with impossible values rather than replace them with the median or mode because the errors are rare and clearly invalid. According to data_25.info() results above, I know that there are 99 entries for each variables. With only 5 affected entries out of 99, removing them avoids introducing assumptions that could distort relationships between variables. Replacing with typical values like the median might mask underlying issues or flatten meaningful variation, especially when the context behind the error is unknown. Dropping ensures the dataset remains clean and reasonable.\n\n\nVariable selection. Select any 5 variables from the potential set of inputs in order to build your k-means clustering model.\n\nvar_selected = day25[[\"Total_Visitors\",\"Avg_Ticket_Price\",\n                       \"High_Temperature\",\"Passholder_Percentage\",\"Per_Capita_Spend\"]]\n\nTo build a meaningful k-means clustering model, I selected the following five variables from the dataset: Total_Visitors, Avg_Ticket_Price, High_Temperature, Passholder_Percentage, and Per_Capita_Spend. These five variables were chosen to capture key aspects of daily park dynamics. Total_Visitors reflects overall attendance, giving a sense of how busy the park is. Avg_Ticket_Price ties into pricing strategy and its potential influence on visitor volume. High_Temperature displays a weather factor that can affect both attendance and spending. Passholder_Percentage helps distinguish between loyal visitors and one-time guests, which may impact behavior and spending habits. Per_Capita_Spend offers a normalized view of how much each visitor contributes financially, combining attendance and revenue into a single meaningful metric.\n\n\nData Scaling\n\n\nDo your variables need to be standardized? Why or why not?\nYes, I do think my variables should be standardized before applying k-means clustering. This is because they are measured on very different scales. For instance, Avg_Ticket_Price is no more than about $83, while Total_Visitors can reach nearly 4,000, which is a much larger numerical range. Similarly, Passholder_Percentage is expressed as a proportion (0–100%), and High_Temperature and Per_Capita_Spend also exist on their own distinct scales. Since k-means uses Euclidean distance to group observations, variables with larger numeric ranges would otherwise dominate the clustering process. Standardizing the data ensures that each variable contributes equally to distance calculations, leading to more balanced and meaningful clusters.\n\n\nIf your data requires standardization, use Python to convert your values into z-scores, and store the normalized data in a new dataframe. If not, proceed to the next step without changing the variables.\n\n# Import scaler function to standardize dataframe\nscaler = StandardScaler()\nvar_scaled = scaler.fit_transform(var_selected)\n# Create a new dataframe with standardized values\nvar_scaled_df = pd.DataFrame(var_scaled, columns = var_selected.columns)\n\nprint(var_scaled_df.head())\n\n   Total_Visitors  Avg_Ticket_Price  High_Temperature  Passholder_Percentage  \\\n0       -0.196940          1.312127         -0.163867               0.452158   \n1       -0.281650         -0.361277         -0.259344               1.252239   \n2       -0.472247         -0.297591          0.186213              -0.742262   \n3       -1.389393          0.187740         -0.036565              -1.534966   \n4       -0.356585         -0.732412          0.806810               1.627801   \n\n   Per_Capita_Spend  \n0         -0.285287  \n1         -1.460976  \n2          0.455238  \n3          1.192436  \n4         -1.513395  \n\n\n\n\nElbow Chart\n\n# Define X with 5 selected columns from the standardized data\nX = var_scaled_df[[\"Total_Visitors\",\n                   \"Avg_Ticket_Price\",\n                   \"High_Temperature\",\n                   \"Passholder_Percentage\",\n                   \"Per_Capita_Spend\"\n                  ]]\n\n# Build the elbow chart\nsse = []\ncluster_range = range(1, 15)\nfor k in cluster_range:\n    kmeans = KMeans(n_clusters=k, random_state=616)\n    kmeans.fit(X)\n    sse.append(kmeans.inertia_)\n\n# Plot the Elbow chart\nplt.figure(figsize=(8, 5))\nplt.plot(cluster_range, sse, marker='o')\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"SSE (Sum of Squared Errors)\")\nplt.title(\"Elbow Chart for K-means Clustering\")\nplt.xticks(cluster_range)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nHow many clusters will you use for your k-means model?\nThe elbow chart above shows the SSE decreasing sharply from around 480 to approximately 175 as the number of clusters increases. The most significant drop occurs between 1 and 4 clusters, after which the curve begins to flatten, suggesting diminishing returns from adding more clusters. Based on this pattern, I selected k = 4 for the k-means model, as it captures most of the variance in the data while maintaining a manageable number of segments.\nA smaller number of clusters (e.g., 2 or 3) would likely combine distinct park-day types, such as slower weekdays and peak weekends - into the same group, masking meaningful operational differences. By using four clusters, the park can better understand how key factors like visitor volume, pricing, weather, and spending behavior interact across different day types.\n\n\nBuild a k-means model with your desired number of clusters.\n\n# Build and fit the K-Means model on the standardized DataFrame 'X'\nkmeans_model = KMeans(n_clusters = 4, random_state = 616)\nkmeans_model.fit(X)\n\n# Add cluster labels to the original standardized df\nvar_scaled_df['Cluster'] = kmeans_model.labels_\n\nprint(\"K-Means Cluster Centers (for 4 clusters):\")\nprint(var_scaled_df)\n\nK-Means Cluster Centers (for 4 clusters):\n    Total_Visitors  Avg_Ticket_Price  High_Temperature  Passholder_Percentage  \\\n0        -0.196940          1.312127         -0.163867               0.452158   \n1        -0.281650         -0.361277         -0.259344               1.252239   \n2        -0.472247         -0.297591          0.186213              -0.742262   \n3        -1.389393          0.187740         -0.036565              -1.534966   \n4        -0.356585         -0.732412          0.806810               1.627801   \n..             ...               ...               ...                    ...   \n89       -0.742667         -0.567707         -0.084304              -0.679222   \n90       -0.242553         -0.211944         -1.325497              -0.760370   \n91        1.531466          1.031030         -0.609424              -0.212452   \n92       -0.009601         -0.225120         -0.800377               0.871312   \n93        0.837497          0.752130         -0.338907              -0.750310   \n\n    Per_Capita_Spend  Cluster  \n0          -0.285287        0  \n1          -1.460976        0  \n2           0.455238        3  \n3           1.192436        3  \n4          -1.513395        1  \n..               ...      ...  \n89          0.366209        3  \n90          0.213943        3  \n91          0.434437        2  \n92         -0.678848        0  \n93          0.615824        3  \n\n[94 rows x 6 columns]\n\n\nI built the k-means model using a data frame containing only the 5 selected variables. By setting random_state = 616, I ensured that the clustering results remain consistent across runs. I selected 3 clusters to achieve a more detailed and meaningful segmentation of the park.\n\n\nGenerate Centroid Values\n\n# Calculate centroid values for each cluster\ncentroids = var_scaled_df.groupby('Cluster').mean()\n\n# Display the centroids\nprint(centroids)\n\n         Total_Visitors  Avg_Ticket_Price  High_Temperature  \\\nCluster                                                       \n0             -0.592234         -0.358172         -0.670880   \n1             -0.295578         -0.381371          1.168029   \n2              1.977070          1.522274          0.319635   \n3             -0.081737         -0.055474         -0.251885   \n\n         Passholder_Percentage  Per_Capita_Spend  \nCluster                                           \n0                     0.643200         -0.830625  \n1                     0.596481         -0.552168  \n2                     0.303739          0.530955  \n3                    -1.079094          0.882159  \n\n\n\ncluster_counts = var_scaled_df['Cluster'].value_counts().sort_index()\nprint(\"\\nCluster Counts:\")\nprint(cluster_counts)\n\n\nCluster Counts:\nCluster\n0    29\n1    20\n2    13\n3    32\nName: count, dtype: int64\n\n\n\n\nBuild four simple visualizations to help management better understand your clusters\n\n\nVisualization 1: Histogram of Total Visitors by Cluster\n\nplt.figure(figsize=(8, 5))\nsns.histplot(data=var_scaled_df, x=\"Total_Visitors\", hue=\"Cluster\", multiple=\"stack\", palette=\"Set2\")\nplt.title(\"Distribution of Total Visitors by Cluster\")\nplt.xlabel(\"Total Visitors (Standardized)\")\nplt.ylabel(\"Frequency\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis histogram shows the distribution of Total Visitors(standardized) across the four clusters groupped by the model, with the x-axis representing Total Visitors to Lobster Land and the y-axis showing their frequency. Cluster 0 (green) is concentrated around lower standardized visitor counts, suggesting it represents quieter, low-traffic days. Cluster 1 (orange) sits in the middle, reflecting moderate visitor days, not too busy nor too quiet. Cluster 2 (blue) dominates the higher end of the scale, capturing peak attendance periods, while Cluster 3 (pink) spans wider range, suggesting it includes a more diverse mix of visitors who vary from occasional guests to highly engaged ones.\nAlthough this is a simple chart, it’s crucial for Lobster Land, as it clearly shows how each cluster reacts differently in terms of attendance and helps the team plan targeted promotions and engagement strategies tailored to each group’s behavior.\n\n\nVisualization 2: Barplot Average Ticket Price per Cluster\n\nplt.figure(figsize=(8, 5))\nsns.barplot(data=var_scaled_df, x=\"Cluster\", y=\"Avg_Ticket_Price\", hue=\"Cluster\", palette=\"Set2\", dodge=False, legend=False, edgecolor=\"black\")\nplt.title(\"Average Ticket Price per Cluster\")\nplt.xlabel(\"Cluster\")\nplt.ylabel(\"Average Ticket Price (Standardized)\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis bar chart illustrates the mean standardized average ticket price across the four clusters identified by the model. Cluster 2 (blue) shows the highest average ticket price, representing premium or high-demand days. Clusters 0 (green) and 1 (orange) display below-average ticket prices, with Cluster 1 slightly lower than Cluster 0, indicating more budget-friendly or passholder-heavy days. Cluster 3 (pink) remains close to the standardized mean, reflecting more typical pricing conditions across Lobster Land’s visitor segments. This chart helps Lobster Land better understand how ticket prices vary across different visitor groups, allowing the team to align pricing strategies and promotions with each segment’s spending behavior.\n\n\nVisualization 3: Scatterplot of Passholder Percentage vs Per Capita Spend colored by Cluster\n\nplt.figure(figsize=(8, 5))\nsns.scatterplot(data=var_scaled_df, x=\"Passholder_Percentage\", y=\"Per_Capita_Spend\", hue=\"Cluster\", palette=\"Set2\")\nplt.title(\"Passholder Percentage vs Per Capita Spend by Cluster\")\nplt.xlabel(\"Passholder Percentage\")\nplt.ylabel(\"Per Capita Spend\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis scatter plot shows how Passholder Percentage relates to Per Capita Spend across the four clusters identified by the model. I noticed a clear pattern where clusters with a higher share of passholders tend to have lower spending per person, while those with fewer passholders show higher spending levels. Cluster 0 (green) has a wide range of passholder percentages but generally lower spending, whereas Cluster 3 (pink) stands out with a smaller share of passholders and higher spending, likely representing one-time or premium visitors. Whereas Cluster 2 (blue) shows a distinct profile, with moderate passholder percentages and positive per capita spending, reflecting a balanced mix of loyal and casual visitors at Lobster Land. From this chart, we can see how different visitor types behave at Lobster Land, helping identify ways to balance loyalty programs with opportunities to attract higher-value guests.\n\n\nVisualization 4: Boxplot High_Temperature per Cluster\n\nplt.figure(figsize=(8, 5))\nsns.boxplot(data=var_scaled_df, x=\"Cluster\", y=\"High_Temperature\", hue=\"Cluster\", palette=\"Set2\", dodge=False)\nplt.title(\"High Temperature by Cluster\")\nplt.xlabel(\"Cluster\")\nplt.ylabel(\"High Temperature (Standardized)\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis boxplot shows how standardized high temperatures vary across the four clusters identified by the model. Cluster 1 (orange) has the highest median temperature, representing hotter days, while Clusters 0 (green) and 3 (pink) are generally cooler. Cluster 2 (blue) shows the widest temperature range, including both hot and cool days, which may reflect mixed or transitional seasonal conditions at Lobster Land.\n\n\nCluster Descriptions\n\n\nCluster 0: “Chill & Sparse”\nThese are the cool, calm days with lower visitor counts, below-average ticket prices, and a higher share of passholders. Spending per person is lighter, creating an easygoing park atmosphere that appeals to loyal guests and locals who enjoy a quieter visit. - Targeting: LobsterLand can focus on community connections and rewarding frequent visitors on these days. Promotions highlighting relaxed park experiences, quiet attractions, or weekday benefits can draw local families and passholders seeking a slower, more comfortable visit.\n\n\nCluster 1: “Scorch & Stroll”\nThis cluster displays the warmest temperatures, moderate attendance, and lower ticket prices. Spending per capita is modest, and many visitors hold passes, creating a steady but easygoing flow of activity across the park. - Targeting: The marketing team can emphasize refreshment and comfort through shaded seating, cool treat bundles or special water rides. Bright and refreshing summer campaigns that highlight leisurely fun and family-friendly attractions can reach guests looking for simple, enjoyable park days.\n\n\nCluster 2: “Gold Rush Days”\nCluster 2 represents the park’s busiest, most profitable days with high visitor counts, the highest standardized ticket prices, and strong per capita spending. With a balanced mix of loyal and casual visitors, these are the moments when the park operates at full capacity and captures its greatest market potential. - Targeting: These days can spotlight premium experiences such as fast passes, dining packages, and exclusive access areas. Promotions on travel platforms and event partnerships can reach visitors planning full-day trips or group outings, aligning offerings with elevated park energy and spending activity.\n\n\nCluster 3: “Balanced Buzz”\nThis cluster shows moderate temperatures, mixed visitor counts, and steady spending levels. Ticket prices remain close to the average, and the lower passholder percentage suggests a healthy mix of new and returning guests. - Targeting: With a blend of loyal customers and newcomers, LobsterLand can use this opportunity to run limited-time offers, themed events, or new experiences can help identify emerging interests.\n\n\nHow can Lobster Land use this model?\nLobster Land can use this day-type clustering model as a practical tool to help the team make better decisions about staffing, pricing, and creating great experiences for guests. By figuring out which cluster an upcoming day fits into, like a busy “Gold Rush Day” or a quieter “Chill & Sparse” day, the team can get ahead of operations instead of just reacting. On high-traffic days, the park can bring in extra staff, stay open later, and offer special upgrades that guests will actually want. On slower days, there’s room to focus on rewarding regulars, running leaner operations, or taking care of behind-the-scenes maintenance that’s hard to do when the park is packed.\nThe marketing team can also use these insights to shape promotions that actually fit the day. Take “Scorch & Stroll” days, for instance. These hot, leisurely days might be perfect for promoting cold drink bundles or access to shaded seating areas. “Balannced Buzz” days, with their unexpected energy, could be a testing ground for new attractions or digital campaigns to see what resonates. As Lobster Land builds out this approach, weaving in weather forecasts and calendar events could help predict cluster assignments days or even weeks ahead. That shifts the park from reactive scrambling to a proactive, rhythm-based planning style that aligns operations with real guest patterns and business priorities.\nIf the team could predict which cluster a day might fall into, it would make planning feel a lot more purposeful and efficient. For instance, if an upcoming weekend looks like a “Gold Rush Day,” the park could bring in extra staff, stock up on food and drinks, and roll out seasonal specials like fall-inspired treats or Halloween-themed beverages to attract both new visitors and returning fans. On the other hand, if a slower Tuesday seems to fall into the “Chill & Sparse” cluster, Lobster Land could use the opportunity to run local promotions, offer discounted afternoon passes, or host a cozy themed dining night for family and friends. This kind of planning helps the park stay balanced—keeping operations efficient, guests happy, and business decisions aligned with real demand."
  },
  {
    "objectID": "python_notebooks/market_segment_conjoint.html#part-ii-conjoint-analysis-with-a-linear-model",
    "href": "python_notebooks/market_segment_conjoint.html#part-ii-conjoint-analysis-with-a-linear-model",
    "title": "Market Segmentation & Conjoint Analysis",
    "section": "Part II: Conjoint Analysis with a Linear Model",
    "text": "Part II: Conjoint Analysis with a Linear Model\n\n# Import night_show.csv files into python for use\nnight_show = pd.read_csv(\"/Users/nhattran/Downloads/MSBA/METAD654 - Marketing Analytics/assignment 2/night_show.csv\",keep_default_na=False, na_values=[])\n\n\nnight_show.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 73728 entries, 0 to 73727\nData columns (total 12 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   bundleID          73728 non-null  int64  \n 1   headliner         73728 non-null  object \n 2   music_atmosphere  73728 non-null  object \n 3   show_length       73728 non-null  int64  \n 4   peak_dB_Level     73728 non-null  int64  \n 5   crowding_index    73728 non-null  int64  \n 6   viewing_zone      73728 non-null  object \n 7   dining_bundle     73728 non-null  object \n 8   re_entry          73728 non-null  object \n 9   live_host         73728 non-null  object \n 10  charity_tie_in    73728 non-null  object \n 11  ratings           73728 non-null  float64\ndtypes: float64(1), int64(4), object(7)\nmemory usage: 6.8+ MB\n\n\n\nnight_show.head()\n\n\n\n\n\n\n\n\nbundleID\nheadliner\nmusic_atmosphere\nshow_length\npeak_dB_Level\ncrowding_index\nviewing_zone\ndining_bundle\nre_entry\nlive_host\ncharity_tie_in\nratings\n\n\n\n\n0\n1\nFireworks\nPop Soundtrack\n30\n65\n40\nfront plaza\nNone\nNone\nYes\nYes\n4.249\n\n\n1\n2\nFireworks\nPop Soundtrack\n30\n65\n40\nfront plaza\nNone\nNone\nYes\nNo/Doesn't Matter\n4.689\n\n\n2\n3\nFireworks\nPop Soundtrack\n30\n65\n40\nfront plaza\nNone\nNone\nNo/Doesn't Matter\nYes\n7.999\n\n\n3\n4\nFireworks\nPop Soundtrack\n30\n65\n40\nfront plaza\nNone\nNone\nNo/Doesn't Matter\nNo/Doesn't Matter\n4.309\n\n\n4\n5\nFireworks\nPop Soundtrack\n30\n65\n40\nfront plaza\nNone\nOne\nYes\nYes\n6.239\n\n\n\n\n\n\n\nBased on the 12 variables provided in the dataset, here’s the list of numeric and categorical variables: - Numerical Variables: show_length (in minutes); peak_dB_Level (decibels level); crowding_index (crowd density); ratings (average ratings from 0 t0 10).\n\nCategorical Variables: bundleID, headliner, music_atmosphere, viewing_zone, dining_bundle, re_entry, live_host, and charity_tie_in, as they represent distinct categories or labels rather than quantities with mathematical meaning.\nI also dropped bundleID from the dataset because it functions solely as a sequential identifier without providing any analytical or predictive value. Including it could potentially create misleading patterns in the model, so excluding it prevents future problems.\n\n\nnight_show = night_show.drop(columns=[\"bundleID\"])\n\n\n# Define the columns to convert\ncategorical_cols = [\"headliner\", \"music_atmosphere\", \"viewing_zone\", \"dining_bundle\",\n                    \"re_entry\", \"live_host\", \"charity_tie_in\"]\n# Convert to category dtype\nnight_show[categorical_cols] = night_show[categorical_cols].astype('category')\n\n\nnight_show.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 73728 entries, 0 to 73727\nData columns (total 11 columns):\n #   Column            Non-Null Count  Dtype   \n---  ------            --------------  -----   \n 0   headliner         73728 non-null  category\n 1   music_atmosphere  73728 non-null  category\n 2   show_length       73728 non-null  int64   \n 3   peak_dB_Level     73728 non-null  int64   \n 4   crowding_index    73728 non-null  int64   \n 5   viewing_zone      73728 non-null  category\n 6   dining_bundle     73728 non-null  category\n 7   re_entry          73728 non-null  category\n 8   live_host         73728 non-null  category\n 9   charity_tie_in    73728 non-null  category\n 10  ratings           73728 non-null  float64 \ndtypes: category(7), float64(1), int64(3)\nmemory usage: 2.7 MB\n\n\n\nUse the pandas get_dummies() function in order to prepare these variables for use in a linear model. Inside this function, include this argument: drop_first = True. Doing this will save us from the multicollinearity problem that would make our model unreliable.\n\n# Before dummifying the variables, check to see if the dataset has any null input\nnight_show.isnull().values.any()\n\nnp.False_\n\n\n\nnight_show.columns\n\nIndex(['headliner', 'music_atmosphere', 'show_length', 'peak_dB_Level',\n       'crowding_index', 'viewing_zone', 'dining_bundle', 're_entry',\n       'live_host', 'charity_tie_in', 'ratings'],\n      dtype='object')\n\n\n\nnight_show_dm = pd.get_dummies(night_show, drop_first=True, columns=['headliner', 'music_atmosphere', 'show_length', 'peak_dB_Level',\n       'crowding_index', 'viewing_zone', 'dining_bundle', 're_entry','live_host', 'charity_tie_in'])\n\n\nnight_show_dm.head()\n\n\n\n\n\n\n\n\nratings\nheadliner_Drone Light Show\nheadliner_Fireworks\nheadliner_Live Musical\nmusic_atmosphere_Pop Soundtrack\nshow_length_70\nshow_length_110\nshow_length_150\npeak_dB_Level_85\npeak_dB_Level_105\n...\nviewing_zone_boardwalk\nviewing_zone_castle_courtyard\nviewing_zone_front plaza\ndining_bundle_dessert_party\ndining_bundle_meal_package\ndining_bundle_snack_voucher\nre_entry_One\nre_entry_Unlimited\nlive_host_Yes\ncharity_tie_in_Yes\n\n\n\n\n0\n4.249\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nTrue\n\n\n1\n4.689\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n2\n7.999\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n3\n4.309\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n4\n6.239\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\n\n\n\n\n5 rows × 23 columns\n\n\n\n\nnight_show_dm.columns\n\nIndex(['ratings', 'headliner_Drone Light Show', 'headliner_Fireworks',\n       'headliner_Live Musical', 'music_atmosphere_Pop Soundtrack',\n       'show_length_70', 'show_length_110', 'show_length_150',\n       'peak_dB_Level_85', 'peak_dB_Level_105', 'crowding_index_60',\n       'crowding_index_80', 'crowding_index_100', 'viewing_zone_boardwalk',\n       'viewing_zone_castle_courtyard', 'viewing_zone_front plaza',\n       'dining_bundle_dessert_party', 'dining_bundle_meal_package',\n       'dining_bundle_snack_voucher', 're_entry_One', 're_entry_Unlimited',\n       'live_host_Yes', 'charity_tie_in_Yes'],\n      dtype='object')\n\n\n\n\nWhy should the numeric input variables based on this survey data be dummified?\nNumeric input variables from survey data should be converted into dummy variables because even though they appear as numbers, they represent discrete categories rather than continuous measurements. Take show_length for example, it might include values such as 30, 70, 110, and 150 minutes. These values are not part of a linear scale but rather specific options provided by the survey design. Treating them as continuous would incorrectly assume that the difference between 30 and 70 minutes has the same proportional relationship as the difference between 110 and 150 minutes, which may not reflect reality.\nBy converting these numeric inputs into dummy variables, the model can assign independent effects to each category without imposing misleading linear assumptions. This approach improves both the accuracy and interpretability of the model while ensuring that the analysis respects the categorical structure inherent in survey-based numeric data.\n\n\nIt might seem tempting here to just skip the model-building step – instead of building a model, we could just rank the bundles by average rating, in descending order, and then just tell Lobster Land to implement all the features in the highest-rated bundle.\nRanking bundles by their average rating might seem ideal, but it hides the true drivers of guest satisfaction. Without building a model, it’s difficult to tell which specific features within each bundle actually influence ratings or how they interact with one another. For example, a highly rated bundle might include both a live musical and unlimited re-entry, but we would not know that the musical boosts satisfaction while the re-entry option reduces it. A conjoint analysis model solves this by estimating the independent effect of each feature across all bundles, allowing Lobster Land to predict how new combinations would perform and design offerings that maximize impact while keeping costs manageable. In short, the model provides a deeper, data-driven understanding of guest preferences that simple ranking cannot achieve.\n\n\nBuild a linear model with your data, using the average rating as the outcome variable, and with all of your other variables (other than the bundleID) as inputs.\nI build a linear regression model using all dummified input features to predict the average rating. This allows to estimate how each individual feature contributes to guest satisfaction.\n\n# Create Linear Regression model\nX = night_show_dm[['headliner_Drone Light Show', 'headliner_Fireworks',\n       'headliner_Live Musical', 'music_atmosphere_Pop Soundtrack',\n       'show_length_70', 'show_length_110', 'show_length_150',\n       'peak_dB_Level_85', 'peak_dB_Level_105', 'crowding_index_60',\n       'crowding_index_80', 'crowding_index_100', 'viewing_zone_boardwalk',\n       'viewing_zone_castle_courtyard', 'viewing_zone_front plaza',\n       'dining_bundle_dessert_party', 'dining_bundle_meal_package',\n       'dining_bundle_snack_voucher', 're_entry_One', 're_entry_Unlimited',\n       'live_host_Yes', 'charity_tie_in_Yes']]\ny = night_show_dm[\"ratings\"]\n\n\n# Import LinearRegression function from scikit‑learn\nfrom sklearn.linear_model import LinearRegression\n\nregressor = LinearRegression()\nregressor.fit(X, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\n\nregressor.intercept_\nprint(f\"Intercept: {regressor.intercept_:.3f}\")\n\nIntercept: 4.953\n\n\n\nimport statsmodels.api as sm\n\n# Define outcome and predictors\ny = night_show_dm['ratings']\nX_model = night_show_dm.drop(columns=['ratings'])\n\n# Add intercept term\nX_model = sm.add_constant(X_model)\n# Convert boolean columns to numeric\nbool_cols = X_model.select_dtypes(include=['bool']).columns\nX_model[bool_cols] = X_model[bool_cols].astype(int)\nX_model = X_model.astype(float)\n\n# Fit the model\nlinear_model = sm.OLS(y, X_model).fit()\nprint(linear_model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                ratings   R-squared:                       0.345\nModel:                            OLS   Adj. R-squared:                  0.345\nMethod:                 Least Squares   F-statistic:                     1766.\nDate:                Mon, 12 Jan 2026   Prob (F-statistic):               0.00\nTime:                        23:18:18   Log-Likelihood:            -1.4678e+05\nNo. Observations:               73728   AIC:                         2.936e+05\nDf Residuals:                   73705   BIC:                         2.938e+05\nDf Model:                          22                                         \nCovariance Type:            nonrobust                                         \n===================================================================================================\n                                      coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------------------------\nconst                               4.9533      0.031    158.283      0.000       4.892       5.015\nheadliner_Drone Light Show          0.0178      0.018      0.964      0.335      -0.018       0.054\nheadliner_Fireworks                 0.0009      0.018      0.046      0.963      -0.035       0.037\nheadliner_Live Musical              1.0299      0.018     55.801      0.000       0.994       1.066\nmusic_atmosphere_Pop Soundtrack    -0.7130      0.013    -54.634      0.000      -0.739      -0.687\nshow_length_70                      0.7179      0.018     38.898      0.000       0.682       0.754\nshow_length_110                     0.9709      0.018     52.608      0.000       0.935       1.007\nshow_length_150                     0.7038      0.018     38.136      0.000       0.668       0.740\npeak_dB_Level_85                    0.0165      0.016      1.034      0.301      -0.015       0.048\npeak_dB_Level_105                  -0.8872      0.016    -55.507      0.000      -0.919      -0.856\ncrowding_index_60                   0.2230      0.018     12.085      0.000       0.187       0.259\ncrowding_index_80                   0.2323      0.018     12.588      0.000       0.196       0.268\ncrowding_index_100                 -0.6046      0.018    -32.760      0.000      -0.641      -0.568\nviewing_zone_boardwalk              0.3889      0.018     21.070      0.000       0.353       0.425\nviewing_zone_castle_courtyard       0.3560      0.018     19.291      0.000       0.320       0.392\nviewing_zone_front plaza            0.3389      0.018     18.362      0.000       0.303       0.375\ndining_bundle_dessert_party        -0.0028      0.018     -0.154      0.877      -0.039       0.033\ndining_bundle_meal_package          0.0033      0.018      0.177      0.860      -0.033       0.039\ndining_bundle_snack_voucher        -0.0922      0.018     -4.993      0.000      -0.128      -0.056\nre_entry_One                        0.2968      0.016     18.569      0.000       0.265       0.328\nre_entry_Unlimited                 -1.5920      0.016    -99.604      0.000      -1.623      -1.561\nlive_host_Yes                       0.8425      0.013     64.555      0.000       0.817       0.868\ncharity_tie_in_Yes                  0.2418      0.013     18.529      0.000       0.216       0.267\n==============================================================================\nOmnibus:                      584.467   Durbin-Watson:                   2.006\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              394.485\nSkew:                          -0.030   Prob(JB):                     2.18e-86\nKurtosis:                       2.647   Cond. No.                         10.8\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\nDisplay the coefficient values of your model inputs.\nI extract and display the model’s coefficients to understand the direction and strength of each feature’s impact on ratings.\n\ncoef_df = pd.DataFrame(regressor.coef_, X.columns, columns=['Coefficient'])\ncoef_df\n\n\n\n\n\n\n\n\nCoefficient\n\n\n\n\nheadliner_Drone Light Show\n0.017789\n\n\nheadliner_Fireworks\n0.000853\n\n\nheadliner_Live Musical\n1.029869\n\n\nmusic_atmosphere_Pop Soundtrack\n-0.713001\n\n\nshow_length_70\n0.717907\n\n\nshow_length_110\n0.970937\n\n\nshow_length_150\n0.703843\n\n\npeak_dB_Level_85\n0.016527\n\n\npeak_dB_Level_105\n-0.887189\n\n\ncrowding_index_60\n0.223038\n\n\ncrowding_index_80\n0.232322\n\n\ncrowding_index_100\n-0.604624\n\n\nviewing_zone_boardwalk\n0.388865\n\n\nviewing_zone_castle_courtyard\n0.356038\n\n\nviewing_zone_front plaza\n0.338891\n\n\ndining_bundle_dessert_party\n-0.002845\n\n\ndining_bundle_meal_package\n0.003259\n\n\ndining_bundle_snack_voucher\n-0.092152\n\n\nre_entry_One\n0.296802\n\n\nre_entry_Unlimited\n-1.592003\n\n\nlive_host_Yes\n0.842467\n\n\ncharity_tie_in_Yes\n0.241810\n\n\n\n\n\n\n\n\n\nWrite two-three paragraphs for Lobster Land management about what your model is showing you.\nDear Lobster Land Management Team,\nI wanted to share some important findings from our conjoint analysis that can help guide decisions around evening entertainment programming. By examining guest survey responses across different features—headliner type, show length, sound levels, re-entry policies, and more—the analysis provides a clearer picture of what actually drives satisfaction and what might be holding the park back. The results show some strong patterns. Live Musical performances (coefficient of 1.03) and having a live host (0.84) make a significant positive impact on guest ratings, while unlimited re-entry (-1.59) and extremely loud sound at 105 dB (-0.89) actively hurt the experience. Shows running around 110 minutes (0.97) appear to be the ideal length, and high crowding levels (negative 0.60 at maximum capacity) clearly diminish satisfaction. These numbers help clarify not just what guests prefer, but how much each element matters when prioritizing investments and operational changes.\nFrom a short-term planning perspective, there are several actionable takeaways. The 110-minute show format seems to be the best option where guests feel they’re getting a full experience without it dragging on or feeling rushed. Although the unlimited re-entry sounds captivating, guests actually prefer having just one opportunity to leave and return, likely because unlimited access signals overcrowding or diminishes the sense of exclusivity. A simple re-entry policy could make guests feel they’re getting more value while also helping manage crowds better. Live musicals clearly offer the best experiences, but they also come with higher costs, tighter schedules, and more moving parts than drone shows or fireworks. The team will need to think carefully about when and how often to host live performances, especially during busy seasons when multiple shows a night could stretch both staff and resources.\nLooking at longer-term strategy, the data suggests some interesting opportunities for tiered experiences. The park should offer premium ticket packages which allow guests to book particular less crowded viewing spots within standard capacity areas. This allows guests who value personal space to pay for that comfort while offering more budget-conscious visitors access at lower price points. Live hosts and charity partnerships, though subtle in impact, show how genuine connection and purpose-driven storytelling can make experiences more memorable and unique. On the dining side, bundles don’t boost satisfaction much, and snack vouchers may even hurt it, as guests often see them as low-value add-ons rather than real enhancements.\nThe key take-away is: prioritize live performances with engaging hosts, keep 110-minute show lengths, keep the sound levels part of the experience without being too overpowering, and manage crowds so visitors have room to enjoy themselves. By focusing on what matters most to customers (authentic experiences, good pacing, and making the evening special), the park can differentiate and create the evenings guests will come back for."
  },
  {
    "objectID": "python_notebooks/time_series_interaction_model.html",
    "href": "python_notebooks/time_series_interaction_model.html",
    "title": "Handling Time Series Data & Modeling with an Interaction Term",
    "section": "",
    "text": "Analyzed Six Flags stock price time series data through November 2025 using Python to identify trends, volatility, and seasonality via differencing, rolling statistics, and resampling. Built a marketing mix regression with an interaction term to quantify how influencer and email spending jointly drive conversions and improve model performance."
  },
  {
    "objectID": "python_notebooks/time_series_interaction_model.html#part-i-working-with-time-series-data",
    "href": "python_notebooks/time_series_interaction_model.html#part-i-working-with-time-series-data",
    "title": "Handling Time Series Data & Modeling with an Interaction Term",
    "section": "Part I: Working with Time Series Data",
    "text": "Part I: Working with Time Series Data\n\n# Import necessary functions to use\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nimport statsmodels.api as sm\n\n\nPick any publicly-traded company that trades on the Nasdaq or the NYSE.\nI selected Six Flags Entertainment Corporation Common Stock New (ticker symbol: FUN). I chose this company because its business operations are closely aligned with Lobster Land, and it offers a useful point of comparison. It is also interesting to see how a major attraction and theme park operator performs in the market and responds to industry conditions.\n\n\nGo to the Nasdaq homepage: www.nasdaq.com. Enter your company’s ticker symbol in “Symbol Search” area near the top of your screen.\n\nsix_flags = pd.read_csv(\"data/six_flags_ytd.csv\", index_col=\"Date\", parse_dates=True)\n\n\nsix_flags.head()\n\n\n\n\n\n\n\n\nClose/Last\nVolume\nOpen\nHigh\nLow\n\n\nDate\n\n\n\n\n\n\n\n\n\n2025-11-14\n$14.60\n4632733\n$14.93\n$15.11\n$14.13\n\n\n2025-11-13\n$15.06\n4082496\n$16.40\n$16.84\n$15.04\n\n\n2025-11-12\n$16.15\n6184328\n$15.63\n$16.55\n$15.5665\n\n\n2025-11-11\n$15.44\n7981932\n$16.23\n$16.245\n$14.805\n\n\n2025-11-10\n$16.35\n7033631\n$17.91\n$17.985\n$16.22\n\n\n\n\n\n\n\n\n\nRename the ‘Close/Last’ column to ‘Close’\n\nsix_flags = six_flags.rename(columns={\"Close/Last\": \"Close\"})\n\n\n\nNext, call the info() function on your dataset, and show your results.\n\nsix_flags.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 249 entries, 2025-11-14 to 2024-11-18\nData columns (total 5 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   Close   249 non-null    object\n 1   Volume  249 non-null    int64 \n 2   Open    249 non-null    object\n 3   High    249 non-null    object\n 4   Low     249 non-null    object\ndtypes: int64(1), object(4)\nmemory usage: 11.7+ KB\n\n\n\nsix_flags.describe()\n\n\n\n\n\n\n\n\nVolume\n\n\n\n\ncount\n2.490000e+02\n\n\nmean\n2.667437e+06\n\n\nstd\n2.392663e+06\n\n\nmin\n3.633340e+05\n\n\n25%\n1.251373e+06\n\n\n50%\n2.139382e+06\n\n\n75%\n3.332256e+06\n\n\nmax\n2.684338e+07\n\n\n\n\n\n\n\n\n# Remove all dollar signs and change the data type to float\nsix_flags = six_flags.replace({'\\$': ''}, regex=True)\nsix_flags = six_flags.astype({'Close': float, 'Open': float, 'High': float, 'Low': float})\nsix_flags.head()\n\n\n\n\n\n\n\n\nClose\nVolume\nOpen\nHigh\nLow\n\n\nDate\n\n\n\n\n\n\n\n\n\n2025-11-14\n14.60\n4632733\n14.93\n15.110\n14.1300\n\n\n2025-11-13\n15.06\n4082496\n16.40\n16.840\n15.0400\n\n\n2025-11-12\n16.15\n6184328\n15.63\n16.550\n15.5665\n\n\n2025-11-11\n15.44\n7981932\n16.23\n16.245\n14.8050\n\n\n2025-11-10\n16.35\n7033631\n17.91\n17.985\n16.2200\n\n\n\n\n\n\n\nIn this step, I removed the dollar signs from the price columns and converted them into float values so the data can be analyzed properly. Once cleaned, the dataset is ready for any calculations or modeling.\n\n\nIs this dataframe indexed by time values? How do you know this?\nYes, this dataframe is indexed by time. I can tell because the index is made up of dates (e.g. “2025-11-14,” “2025-11-13”), which indicates that each row is organized and labeled by a specific point in time rather than by a simple numeric index.\n\n\nVisualize the entire time series.\n\n\nFirst, just call .plot() on your dataframe object.\n\nsix_flags.plot()\n\n\n\n\n\n\n\n\n\nDescribe what you see here. Why is this a challeging graph to interpret? What would make it easier to understand?\nThis graph shows five stock metrics: Close, Volume, Open, High, and Low, plotted from November 2024 to November 2025. Each metric is shown as a different colored line, but the y-axis combines both price values and Volume, which reaches into over the millions units. Because the price-based metrics stay within a much smaller range, their lines appear compressed while Volume dominates the scale.\nThe graph is hard to interpret because the shared y-axis has to stretch to fit the huge Volume values, which makes the price lines look almost flat. It would be much easier to read if Volume had its own y-axis or subplot, and if the chart included clearer labels or a legend to show which line represents each metric.\n\n\n\nNow, re-run the .plot() function, but this time, call that function on the ‘Close’ variable only.\n\nsix_flags[\"Close\"].plot()\n\n\n\n\n\n\n\n\n\nNow, in a couple sentences, describe what you see. Why is this graph more easily interpretable than the one you plotted in the previous step?\nWhen I only plot the ‘Close’ variable, the graph clearly shows a downward trend in closing prices over time, with a slight fluctuations. The line is clear easy to follow, making it simple to see how the stock’s value declined from November 2024 to November 2025. This graph is easier to interpret than the previous plot because it focuses on a single variable with a consistent scale. By removing Volume, the y-axis now matches the scale of the prices, making it easy to see how the closing price changes over time.\n\n\n\nRe-plot the ‘Close’ variable, and add a horizontal line representing the average closing price for the year.\n\n# Calculate average closing price\navg_close = six_flags[\"Close\"].mean()\n\nax = six_flags[\"Close\"].plot(figsize=(8, 5), title=\"Six Flags Close Price\")\nax.axhline(y=avg_close, color='red', linestyle='--', label=f'Average: ${avg_close:.2f}')\nax.set_xlabel(\"Date\")\nax.set_ylabel(\"Close Price ($)\")\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nWhat does this line help you see more clearly?\nAdding a horizontal line for the average closing price makes the graph much easier to read by providing a clear benchmark. In this case, the red dashed line at $33.92 shows that the stock spent most of the year below its average, especially in the second half. This also makes it easy to compare the stock’s daily movements to its overall trend, highlighting when it was consistently above or below the average. Therefore, it easy to spot long-term trends and periods of underperformance or outperformance.\n\n\n\nUse the .diff() method to calculate the daily change in the ‘Close’ price, and store the result in a new column called ‘daily_change’.\n\nsix_flags[\"daily_change\"] = six_flags[\"Close\"].diff()\nsix_flags.head()\n\n\n\n\n\n\n\n\nClose\nVolume\nOpen\nHigh\nLow\ndaily_change\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n2025-11-14\n14.60\n4632733\n14.93\n15.110\n14.1300\nNaN\n\n\n2025-11-13\n15.06\n4082496\n16.40\n16.840\n15.0400\n0.46\n\n\n2025-11-12\n16.15\n6184328\n15.63\n16.550\n15.5665\n1.09\n\n\n2025-11-11\n15.44\n7981932\n16.23\n16.245\n14.8050\n-0.71\n\n\n2025-11-10\n16.35\n7033631\n17.91\n17.985\n16.2200\n0.91\n\n\n\n\n\n\n\n\nPlot the ‘daily_change’ variable.\n\nsix_flags[\"daily_change\"].plot()\n\n\n\n\n\n\n\n\n\n\nWhat does this show that the original closing price plot did not?\nFor Six Flags, plotting the daily_change variable reveals the day-to-day volatility in the stock that the original closing price graph did not show. Instead of a smooth downward trend, the values oscillate around zero with both positive and negative spikes, highlighting short-term gains and losses. This makes it clear that Six Flags’ stock movement was more turbulent than the overall decline suggests, providing insight into its momentum and risk.\n\n\nDo you notice any periods of unusually high or low movement?\nYes, there are periods of unusually high and low movement. For example, there’s a sharp upward spike followed closely by a steep drop around mid-2025, suggesting a possible earnings announcement or noticable event that quickly affected market reactions. These significant swings stand out clearly in the daily_change graph and would be difficult to detect in the original closing price plot. Such periods are critical for analysts to investigate further, as they may signal turning points or underlying volatility that affects investment decisions.\n\n\n\nPlotting a subset of your data\n\nUsing a slice operation, plot the daily ‘Close’ variable from your dataset for any one-month period of your choice.\n\n# Sort the dataframe index\nsix_flags = six_flags.sort_index()\n\n# Slice the data for September 2025\nsix_flags_sep = six_flags[\"Close\"][\"2025-09-01\":\"2025-09-30\"]\nsix_flags_sep.plot(title=\"Six Flags Close Price (September 2025)\")\n\n\n\n\n\n\n\n\n\n\nNow, show the plot you drew with the previous step, but with a new figsize, line color, and style\n\nplt.figure(figsize=(8, 5))\nplt.plot(six_flags_sep.index, six_flags_sep, color='blue', label='Close')\nplt.title(\"Six Flags Close Price (September 2025)\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Price ($)\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThis chart shows Six Flags’ closing stock price throughout September 2025, fluctuating between roughly \\$21 and \\$26. The overall movement appears choppy, with no clear upward or downward trend, suggesting short-term volatility rather than directional momentum during that month.\n\n\n\nRolling windows\n\n\nGenerate a 20-period moving average for your ‘Close’ variable, and create a plot that overlays this 20-period average atop the actual daily closing prices.\n\nsix_flags[\"moving_avg_20\"] = six_flags[\"Close\"].rolling(window=20).mean()\nsix_flags.tail()\n\n\n\n\n\n\n\n\nClose\nVolume\nOpen\nHigh\nLow\ndaily_change\nmoving_avg_20\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n2025-11-10\n16.35\n7033631\n17.91\n17.985\n16.2200\n0.91\n22.2870\n\n\n2025-11-11\n15.44\n7981932\n16.23\n16.245\n14.8050\n-0.71\n21.9905\n\n\n2025-11-12\n16.15\n6184328\n15.63\n16.550\n15.5665\n1.09\n21.7680\n\n\n2025-11-13\n15.06\n4082496\n16.40\n16.840\n15.0400\n0.46\n21.4580\n\n\n2025-11-14\n14.60\n4632733\n14.93\n15.110\n14.1300\nNaN\n21.1440\n\n\n\n\n\n\n\n\nplt.figure(figsize=(8,5))\nplt.plot(six_flags[\"Close\"], label=\"Close\")\nplt.plot(six_flags[\"moving_avg_20\"], label=\"20-period MA\")\nplt.legend()\nplt.xlabel(\"Date\")\nplt.ylabel(\"Price\")\nplt.title(\"Closing Price with 20-Period Moving Average\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNext, generate a 75-period moving average for your ‘Close’ variable, and create a plot that overlays this 75-period average atop the actual daily closing prices.\n\nsix_flags[\"moving_avg_75\"] = six_flags[\"Close\"].rolling(window=75).mean()\nsix_flags.tail()\n\n\n\n\n\n\n\n\nClose\nVolume\nOpen\nHigh\nLow\ndaily_change\nmoving_avg_20\nmoving_avg_75\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n2025-11-10\n16.35\n7033631\n17.91\n17.985\n16.2200\n0.91\n22.2870\n23.840800\n\n\n2025-11-11\n15.44\n7981932\n16.23\n16.245\n14.8050\n-0.71\n21.9905\n23.628000\n\n\n2025-11-12\n16.15\n6184328\n15.63\n16.550\n15.5665\n1.09\n21.7680\n23.437600\n\n\n2025-11-13\n15.06\n4082496\n16.40\n16.840\n15.0400\n0.46\n21.4580\n23.233333\n\n\n2025-11-14\n14.60\n4632733\n14.93\n15.110\n14.1300\nNaN\n21.1440\n23.028533\n\n\n\n\n\n\n\n\nplt.figure(figsize=(8,5))\nplt.plot(six_flags[\"Close\"], label=\"Close\")\nplt.plot(six_flags[\"moving_avg_75\"], label=\"75-period MA\")\nplt.legend()\nplt.xlabel(\"Date\")\nplt.ylabel(\"Price\")\nplt.title(\"Closing Price with 75-Period Moving Average\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nHow are your two moving average plots different from one another? What are some pros and cons of shorter and longer moving average windows?\nThe two moving average plots differ in the length of the window used to smooth the closing price data. The 20-period moving average reacts quickly to short-term fluctuations and closely follows the daily ups and downs, making it more jagged. The 75-period moving average smooths out more noise, highlighting the broader trend over time, but it lags behind recent price movements and adjusts more slowly.\nShorter windows like 20 periods are helpful for spotting short-term momentum and quick shifts in direction, though they can be sensitive to noise and produce false signals. Longer windows like 75 periods are better for identifying long-term trends and filtering out daily volatility, but they respond more slowly, which can delay recognition of turning points.\n\n\nPlot a rolling standard deviation for your time series data.\n\n# Create rolling standard deviation (20-day window)\nsix_flags[\"rolling_std_20\"] = six_flags[\"Close\"].rolling(window=20).std()\n\nplt.figure(figsize=(8, 5))\nplt.plot(six_flags.index, six_flags[\"rolling_std_20\"], color='blue')\nplt.title(\"Six Flags Close Price: 20-Day Rolling Standard Deviation\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Standard Deviation\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nWhat does your rolling standard deviation plot show you about your time series?\nThe rolling standard deviation plot for ‘Close’ prices shows how the Six Flags’ stock volatility changed throughout 2025. Using a 20-day window, the graph highlights peaks and troughs, with spikes in mid-2025 indicating periods of higher uncertainty, and lower stretches where volatility drops closer to 1.\nThese bursts of volatility correspond to real-world events. In Q2 2025, Six Flags reported a net loss due to adverse weather and merger-related challenges, and in Q3 2025, the company recorded a large non-cash impairment charge. Such events likely caused rapid day-to-day price movements, explaining the spikes seen in the rolling standard deviation. Overall, the plot shows that the stock’s decline was not smooth or predictable. High-volatility periods meant greater uncertainty for investors, while low-volatility stretches offered more stable performance, providing a clearer picture of risk throughout the year.\nReferences: https://www.tipranks.com/news/company-announcements/six-flags-reports-q2-2025-earnings-amid-weather-challenges\n\n\nResampling.\n\n\nResample your time series so that its values are based on quarterly time periods’ mean values for ‘Close’, rather than daily periods.\n\n# Resample the 'Close' column to quarterly frequency and calculate the mean\nsix_flags_closeq = six_flags['Close'].resample('QE').mean()\nsix_flags_closeq.head()\n\nDate\n2024-12-31    47.097333\n2025-03-31    42.545333\n2025-06-30    33.510484\n2025-09-30    26.555937\n2025-12-31    21.339091\nFreq: QE-DEC, Name: Close, dtype: float64\n\n\n\nPlot this newly-resampled time series, with the dates on the x-axis, and the Close values on the y-axis.\n\nplt.figure(figsize=(8, 5))\nsix_flags_closeq.plot(marker='o')\nplt.title('Six Flags Average Close Price (Quarterly)')\nplt.xlabel('Date')\nplt.ylabel('Average Close Price ($)')\nplt.grid(True)\n# Annotate the data points\nfor date, value in zip(six_flags_closeq.index, six_flags_closeq.values):\n    plt.text(date, value + 0.5, f'{value:.2f}', ha='center', va='bottom', fontsize=10)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis chart shows a steady decline in Six Flags’ average closing stock price over five consecutive quarters, falling from \\$47.10 in late 2024 to \\$21.34 by Q4 2025, with the steepest drop between Q1 and Q2 of 2025. By using quarterly averages, the chart smooths out daily fluctuations and highlights the overall trend more clearly. This makes it easier to see sustained downward pressure on the stock and identify periods of significant change.\n\n\nProvide an example that explains why someone might care about resampling a time series.\nA retail analyst tracking daily sales at a chain of toy stores can see a lot of fluctuations in the raw data. Resampling the data into weekly or monthly averages makes it easier to compare different periods, such as the holiday season or Black Friday versus regular months. For example, December and the week of Black Friday consistently show higher average sales than other months, helping the business plan inventory, staffing, and promotions more effectively. By smoothing out daily noise, resampling provides a clearer picture of these seasonal patterns and supports smarter operational decisions."
  },
  {
    "objectID": "python_notebooks/time_series_interaction_model.html#part-ii-marketing-mix-modeling-with-an-interaction-term",
    "href": "python_notebooks/time_series_interaction_model.html#part-ii-marketing-mix-modeling-with-an-interaction-term",
    "title": "Handling Time Series Data & Modeling with an Interaction Term",
    "section": "Part II: Marketing Mix Modeling with an Interaction Term",
    "text": "Part II: Marketing Mix Modeling with an Interaction Term\n\nAfter reading the file into your environment, the first question that you will explore here is whether there is any relationship between marketing spending and conversions.\n\nlobster_campaign = pd.read_csv(\"/Users/nhattran/Downloads/MSBA/METAD654 - Marketing Analytics/assignment 5/marketing_campaign_lobster.csv\")\nlobster_campaign.head()\n\n\n\n\n\n\n\n\ninfluencer_spend\nemail_spend\nradio_spend\nconversions\n\n\n\n\n0\n28.97\n62.85\n28.73\n45\n\n\n1\n134.30\n61.11\n74.28\n194\n\n\n2\n140.56\n9.43\n33.48\n100\n\n\n3\n72.38\n21.88\n76.27\n73\n\n\n4\n64.35\n81.06\n49.26\n112\n\n\n\n\n\n\n\n\n\nTo explore this, first create a new variable that shows the total spending. This variable’s value should be the sum of influencer_spend, email_spend, and radio_spend.\n\nlobster_campaign[\"total_spending\"] = lobster_campaign[[\"influencer_spend\", \"email_spend\", \"radio_spend\"]].sum(axis=1)\nlobster_campaign.head()\n\n\n\n\n\n\n\n\ninfluencer_spend\nemail_spend\nradio_spend\nconversions\ntotal_spending\n\n\n\n\n0\n28.97\n62.85\n28.73\n45\n120.55\n\n\n1\n134.30\n61.11\n74.28\n194\n269.69\n\n\n2\n140.56\n9.43\n33.48\n100\n183.47\n\n\n3\n72.38\n21.88\n76.27\n73\n170.53\n\n\n4\n64.35\n81.06\n49.26\n112\n194.67\n\n\n\n\n\n\n\n\n\nNow, find the correlation between this new total spending variable and conversions.\n\nlobster_campaign_corr = lobster_campaign[\"total_spending\"].corr(lobster_campaign[\"conversions\"])\nprint(f\"The correlation between total spending and conversions: {lobster_campaign_corr:.4f}\")\n\nThe correlation between total spending and conversions: 0.8832\n\n\n\nWhat is the correlation between these variables?\nThe correlation between total marketing spending (sum of influencer, email, and radio spend) and conversions is 0.8832, which indicates a strong positive relationship.\n\n\nWhat does this correlation suggest about the relationship between total marketing spending and conversion?\nThis strong correlation shows that higher total marketing spending is generally associated with more conversions. In other words, when more is invested in advertising, the dataset suggests conversion numbers tend to be higher.\n\n\nWhy can’t we conclude from this that more ad spending leads to more conversions?\nHowever, this does not prove that higher ad spending directly causes more conversions. Other factors, such as seasonality, product quality, or external events, could be influencing both spending and results. Without testing the relationship through a controlled experiment or using a causal model, it is impossible to determine whether higher ad spending actually drives more conversions or if other factors are the main reason.\n\n\n\nNext, let’s explore the relationship among the influencer ad spending, email ad spending, and radio ad spending variables. Examine the correlations among these three variables.\n\n# Compute correlation matrix for the three spending variables\ncorr_matrix = lobster_campaign[['influencer_spend','email_spend','radio_spend']].corr()\n\n# Plot the heatmap\nplt.figure(figsize=(6,4))\nsns.heatmap(corr_matrix, annot=True, cmap='Blues', fmt=\".2f\")\nplt.title(\"Correlation Heatmap of Ad Spending Variables\")\nplt.show()\n\n\n\n\n\n\n\n\n\nAre any of these correlations so high that we might not be able to use them together in a linear model?\nNone of the ad spending variables are highly correlated, so multicollinearity is not a concern for a linear model. The highest correlation is only 0.11 between influencer and radio spend, which is very low. This means each variable gives mostly separate information, so they can be used together in a regression without messing up the results.\n\n\n\nNow, build a model that uses conversions as the outcome variable, with influencer spending, email ad spending, and radio ad spending as the input variables.\n\n# Define predictors (X) and outcome (y)\nX = lobster_campaign[['influencer_spend', 'email_spend', 'radio_spend']]\ny = lobster_campaign['conversions']\n\nX = sm.add_constant(X)\nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            conversions   R-squared:                       0.902\nModel:                            OLS   Adj. R-squared:                  0.900\nMethod:                 Least Squares   F-statistic:                     449.0\nDate:                Tue, 13 Jan 2026   Prob (F-statistic):           1.80e-73\nTime:                        00:23:16   Log-Likelihood:                -679.66\nNo. Observations:                 150   AIC:                             1367.\nDf Residuals:                     146   BIC:                             1379.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n====================================================================================\n                       coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------------\nconst              -72.2859      6.100    -11.850      0.000     -84.342     -60.230\ninfluencer_spend     1.2547      0.044     28.483      0.000       1.168       1.342\nemail_spend          1.5346      0.071     21.600      0.000       1.394       1.675\nradio_spend          0.0667      0.084      0.791      0.430      -0.100       0.233\n==============================================================================\nOmnibus:                        0.657   Durbin-Watson:                   1.986\nProb(Omnibus):                  0.720   Jarque-Bera (JB):                0.748\nSkew:                          -0.000   Prob(JB):                        0.688\nKurtosis:                       2.654   Cond. No.                         362.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nWhat’s the p-value and the F-statistic for this model? What does this suggest about the model?\nThe F-statistic for the model is 449.0, with a p-value of 1.80e-73, which is extremely small. This indicates that the overall model is statistically significant, meaning at least one of the predictors meaningfully explains variation in conversions and the model does a good job fitting the data.\n\n\nWhat are the p-values for each of the individual predictors used in this model? What does this suggest about these predictors?\nLooking at the individual predictors, influencer spend and email spend both have p-values of 0.000, showing they are significant contributors to conversions. Radio spend, on the other hand, has a p-value of 0.430, which is not significant. This suggests that radio spend does not have a meaningful effect on conversions in this model and may not be as helpful as other variables with prediction.\n\n\n\nLobster Land’s marketing team is investigating how different types of advertising spend impact the number of conversions. They need you to create an interaction plot to visualize the relationship between influencer_spend and conversions, across different levels of email_spend.\n\nFirst, create a new variable called email_spend_group by dividing email_spend into three groups: low, medium, and high.\n\nlobster_campaign['email_spend_group'] = pd.qcut(\n    lobster_campaign['email_spend'],\n    q=3,\n    labels=['Low','Medium','High']\n)\n\nprint(lobster_campaign[['email_spend','email_spend_group']])\n\n     email_spend email_spend_group\n0          62.85            Medium\n1          61.11            Medium\n2           9.43               Low\n3          21.88               Low\n4          81.06              High\n..           ...               ...\n145        24.52               Low\n146        68.60            Medium\n147         9.64               Low\n148        57.53            Medium\n149        31.74               Low\n\n[150 rows x 2 columns]\n\n\n\n\nThen, plot conversions against influencer_spend, using different colored lines for each email_spend_group.\n\n\nUse seaborn.lmplot() or something similar to draw trend lines for each group.\n\nsns.lmplot(\n    data=lobster_campaign,\n    x='influencer_spend',\n    y='conversions',\n    hue='email_spend_group',\n    height=5,\n    aspect=1.2,\n    palette='Set1'\n)\nplt.title(\"Interaction Plot: Influencer Spend vs Conversions by Email Spend Group\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nWhat đo the different slopes tell you? Is there visual evidence of an interaction between influencer_spend and email_spend? How would the lines appear if there were no interaction?\nThe different slopes show that the effect of influencer spend on conversions depends on the level of email spend. For instance, the green “High” group has the steepest slope, meaning influencer marketing drives the most conversions when email spend is also high. The red “Low” group has a much flatter slope, indicating that influencer spend is less effective at lower levels of email spend.\nThere is visual evidence of an interaction because the lines diverge rather than staying parallel, showing that the impact of influencer spend varies with email spend. If there were no interaction, the lines would be roughly parallel, indicating that influencer spend affects conversions the same way regardless of email spending level. The vertical position of each line might differ (due to additive effects), but the slopes would be similar.\n\n\n\nBuild yet another model – this time, you will again use conversions as the outcome variable. Your inputs will be influencer ad spending, email ad spending, and an interaction variable for influencer ad spending & email ad spending.\n\n# Create an interaction variable for influencer ad spendinng and email ad spending\nlobster_campaign['interaction'] = (lobster_campaign['influencer_spend'] * lobster_campaign['email_spend'])\nprint(lobster_campaign[['influencer_spend','email_spend','interaction']])\n\n     influencer_spend  email_spend  interaction\n0               28.97        62.85    1820.7645\n1              134.30        61.11    8207.0730\n2              140.56         9.43    1325.4808\n3               72.38        21.88    1583.6744\n4               64.35        81.06    5216.2110\n..                ...          ...          ...\n145             18.67        24.52     457.7884\n146             57.64        68.60    3954.1040\n147             78.77         9.64     759.3428\n148            119.89        57.53    6897.2717\n149             14.93        31.74     473.8782\n\n[150 rows x 3 columns]\n\n\n\n# Define predictors (X) and outcome (y)\nX = lobster_campaign[['influencer_spend', 'email_spend', 'interaction']]\ny = lobster_campaign['conversions']\n\nX = sm.add_constant(X)\nmodel_interaction = sm.OLS(y, X).fit()\nprint(model_interaction.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            conversions   R-squared:                       0.954\nModel:                            OLS   Adj. R-squared:                  0.953\nMethod:                 Least Squares   F-statistic:                     1011.\nDate:                Tue, 13 Jan 2026   Prob (F-statistic):           2.03e-97\nTime:                        00:23:17   Log-Likelihood:                -622.98\nNo. Observations:                 150   AIC:                             1254.\nDf Residuals:                     146   BIC:                             1266.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n====================================================================================\n                       coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------------\nconst               -1.4199      6.480     -0.219      0.827     -14.226      11.386\ninfluencer_spend     0.4262      0.071      5.985      0.000       0.285       0.567\nemail_spend          0.3122      0.107      2.922      0.004       0.101       0.523\ninteraction          0.0147      0.001     12.892      0.000       0.012       0.017\n==============================================================================\nOmnibus:                        0.195   Durbin-Watson:                   2.134\nProb(Omnibus):                  0.907   Jarque-Bera (JB):                0.301\nSkew:                           0.079   Prob(JB):                        0.860\nKurtosis:                       2.847   Cond. No.                     2.81e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.81e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\n\nWhat do you notice about the p-values for each of these predictors?\nIn the interaction model, all three predictors are statistically significant. Influencer spend has a positive effect on conversions (coef = 0.4262, p = 0.000), and email spend also contributes meaningfully (coef = 0.3122, p = 0.004). The interaction term is positive and highly significant (coef = 0.0147, p = 0.000), showing that conversions are not driven by either channel on its own. Instead, the two variables complement with each other, and the impact of influencer marketing becomes stronger when email marketing spend is higher. This combined effect definitely improves performance.\n\n\nHow does the R-squared of this model compared to the R-squared of the model built to predict conversions, but with only influencer speding and email ad spending, but without the interaction?\nThe model with the interaction term explains about 95 percent of the variation in conversions (R² = 0.954). In comparison, the model without the interaction shows about 90 percent (R² = 0.902). Both models perform well, but the interaction model captures the data more effectively. The increase in R² indicates that adding the interaction term provides meaningful predictive value and offers a more realistic view of how the marketing team at Lobster Land would optimize their budget.\n\n\nDemonstrate what your model would predict for a marketer using 100 units of influencer ad spending and 50 units of email ad spending. What conversions outcome should this marketer expect to see?\n\\[\n\\text{conversions} = \\beta_0\n+ \\beta_1 \\cdot \\text{influencer\\_spend}\n+ \\beta_2 \\cdot \\text{email\\_spend}\n+ \\beta_3 \\cdot (\\text{influencer\\_spend} \\times \\text{email\\_spend})\n\\]\nWhereas, the coefficients are: - Intercept (\\(\\beta_0\\)) = -1.4199\n- Influencer spend (\\(\\beta_1\\)) = 0.4262\n- Email spend (\\(\\beta_2\\)) = 0.3122\n- Interaction (\\(\\beta_3\\)) = 0.0147\nAnd given the Influencer spend = 100; Email spend = 50 - Interaction = 100 × 50 = 5000\nThe predicted conversion is: \\[\n\\text{conversions} = -1.4199\n+ (0.4262 \\times 100)\n+ (0.3122 \\times 50)\n+ (0.0147 \\times 5000)\n\\]\n\\[\n= -1.4199 + 42.62 + 15.61 + 73.5\n\\]\n\\[\n= 130.3\n\\]\nThis prediction translates the regression results into a practical forecast: with 100 units of influencer spend and 50 units of email spend, the team should expect about 131 conversions. The interaction term alone contributes roughly 73.5 of these conversions, which is more than the individual contributions from influencer or email spend. This shows that the combined effect of the two channels is a major driver of total conversions.\nThe interaction effect shows that influencer spend becomes more effective when it is paired with an increase in email spend. Instead of operating independently, the two channels complement one another, meaning the impact of influencer campaigns increases when email marketing is also strong. For Lobster Land, this suggests that bundling influencer and email efforts will produce better conversion outcomes than investing in either channel on its own. One suggestion is that Lobster Land could boost results by running influencer promotions that drive users to sign up for an email offer, then following up with targeted emails that convert that traffic into purchases."
  },
  {
    "objectID": "python_notebooks/data_exploration_visualization.html",
    "href": "python_notebooks/data_exploration_visualization.html",
    "title": "Data Exploration & Visualization",
    "section": "",
    "text": "Analyzed Lobsterland’s 2025 visitor and revenue data using Python to uncover trends in attendance, spending, and event performance. Applied exploratory data analysis and data visualization techniques to generate actionable business insights."
  },
  {
    "objectID": "python_notebooks/data_exploration_visualization.html#part-i-exploratory-data-analysis-exploration-manipulation",
    "href": "python_notebooks/data_exploration_visualization.html#part-i-exploratory-data-analysis-exploration-manipulation",
    "title": "Data Exploration & Visualization",
    "section": "Part I: Exploratory Data Analysis: Exploration & Manipulation",
    "text": "Part I: Exploratory Data Analysis: Exploration & Manipulation\n\n# Importing necessary functions\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nlobster_25 = pd.read_csv(\"data/lobster_25.csv\")\n\n\nlobster_25.head()\n\n\n\n\n\n\n\n\nDate\nDay_of_Week\nseason_week_rel\nIs_Holiday\nTotal_Visitors\nPassholder_Percentage\nDay_Tickets_Sold\nAvg_Ticket_Price\nGate_Revenue\nRevenue_Food\n...\nWeather_Type\nWeather_Code\nPromo_Flag\nPromo_Type\nCruise_Docked\nIs_Special_Event\nSpecial_Events\nAttraction_Tier\nZone_ID\nPer_Capita_Spend\n\n\n\n\n0\n2025-05-26\nMonday\n1\n1\n1769\n48.65\n908\n77.42\n70297.36\n40521.91\n...\nThunderstorms\n4\n0\nNaN\n0\n0\nNaN\n2\n101\n73.43\n\n\n1\n2025-05-27\nTuesday\n1\n0\n1717\n60.58\n677\n69.80\n47254.60\n35298.92\n...\nPartly Cloudy\n2\n0\nNaN\n0\n0\nNaN\n2\n101\n59.30\n\n\n2\n2025-05-28\nWednesday\n1\n0\n1600\n30.84\n1107\n70.09\n77589.63\n36495.95\n...\nPartly Cloudy\n2\n0\nNaN\n0\n0\nNaN\n3\n103\n82.33\n\n\n3\n2025-05-29\nThursday\n1\n0\n1037\n19.02\n840\n72.30\n60732.00\n22486.65\n...\nShowers\n3\n0\nNaN\n0\n0\nNaN\n2\n101\n91.19\n\n\n4\n2025-05-30\nFriday\n1\n0\n1671\n66.18\n565\n68.11\n38482.15\n40705.15\n...\nPartly Cloudy\n2\n0\nNaN\n0\n0\nNaN\n1\n104\n58.67\n\n\n\n\n5 rows × 27 columns\n\n\n\n\nlobster_25.shape\n\n(99, 27)\n\n\nThere are 99 rows and 27 columns in this Lobsterland data set.\n\nCategorical variables: Date, Day of Week, season_week_rel, is_holiday, Precipitation, Weather Type, Weather Code, Promo Flag, Promo Type, Cruise Docked, Is_Special_Event, Special_Events, Attraction_Tier, Zone_ID\nNumeric variables: Total visitors, Passholder percentage, Day Tickets Sold, Avg Ticket Price, Gate Revenue, Revenue Food, Revenue Merch, Revenue Arcade, Total Revenue, Total Labor Hours, International Visitors, High temperature, Per Capita Spend\n\n\nChecking for NaN values.\n\nlobster_25.isnull().sum()\n\nDate                       0\nDay_of_Week                0\nseason_week_rel            0\nIs_Holiday                 0\nTotal_Visitors             0\nPassholder_Percentage      0\nDay_Tickets_Sold           0\nAvg_Ticket_Price           0\nGate_Revenue               0\nRevenue_Food               0\nRevenue_Merch              5\nRevenue_Arcade             0\nTotal_Revenue              0\nTotal_Labor_Hours          0\nInternational_Visitors     5\nHigh_Temperature           0\nPrecipitation              0\nWeather_Type               0\nWeather_Code               0\nPromo_Flag                 0\nPromo_Type                79\nCruise_Docked              0\nIs_Special_Event           0\nSpecial_Events            84\nAttraction_Tier            0\nZone_ID                    0\nPer_Capita_Spend           0\ndtype: int64\n\n\nTo check for NaN values, I used the code lobster25.isna().sum(). This showed the number of missing values in each column. The columns that contain missing values are Revenue_Merch with 5 missing entries, International_Visitors with 5 missing entries, Promo_Type with 79 missing entries, and Special_Events with 84 missing entries. These missing values are understandable because the columns represent optional events, promotions, or international visitors.\n\nFor numeric variables like Revenue_Merch and International_Visitors, which are recorded in numbers, NaN likely means no merchandise revenue or no international visitors that day, so it can be replaced with 0.\nAs for categorial variables like Promo_Type and Special_Events, NaN means no promotion/special event was active, so it’s better to fill them with descriptive text rather than 0. So, for Promo_Type, I replace NaN with “No Promo” and for Special_Events with “No Event” respectively.\n\n\n# Create and copy from the original dataframe so not to mess with it\nlobster25 = lobster_25.copy() \n\n# Filling numeric variables with NaN to 0\nlobster25['Revenue_Merch'] = lobster25['Revenue_Merch'].fillna(0.0)\nlobster25['International_Visitors'] = lobster25['International_Visitors'].fillna(0.0)\n\n\nlobster25[\"Promo_Type\"] = lobster25[\"Promo_Type\"].fillna(\"No Promo\")\nlobster25[\"Special_Events\"] = lobster25[\"Special_Events\"].fillna(\"No Event\")\n\n\n# Double-checking to see if there's still NaN in the data set\nlobster25.isnull().sum()\n\nDate                      0\nDay_of_Week               0\nseason_week_rel           0\nIs_Holiday                0\nTotal_Visitors            0\nPassholder_Percentage     0\nDay_Tickets_Sold          0\nAvg_Ticket_Price          0\nGate_Revenue              0\nRevenue_Food              0\nRevenue_Merch             0\nRevenue_Arcade            0\nTotal_Revenue             0\nTotal_Labor_Hours         0\nInternational_Visitors    0\nHigh_Temperature          0\nPrecipitation             0\nWeather_Type              0\nWeather_Code              0\nPromo_Flag                0\nPromo_Type                0\nCruise_Docked             0\nIs_Special_Event          0\nSpecial_Events            0\nAttraction_Tier           0\nZone_ID                   0\nPer_Capita_Spend          0\ndtype: int64\n\n\nI replaced NaNs in Revenue_Merch and International_Visitors with 0, assuming missing values indicate no revenue or no visitors. For Promo_Type and Special_Events, I used “No Promo” and “No Event” because missing entries mean no promotion or event occurred. I made these choices by considering what each column represents. This approach makes the dataset more informative and descriptive.\n\n\nRename the ‘Food_Rev’ column to ‘FoodBev_Rev’.\n\nlobster25 = lobster25.rename(columns={'Revenue_Food':'Revenue_FoodBev'}) #rename the columns\n# There is no 'Food_Rev' so I'm assuming it's 'Revenue_Food' and replace it with 'Revenue_FoodBev'\nprint(lobster25.columns)\n\nIndex(['Date', 'Day_of_Week', 'season_week_rel', 'Is_Holiday',\n       'Total_Visitors', 'Passholder_Percentage', 'Day_Tickets_Sold',\n       'Avg_Ticket_Price', 'Gate_Revenue', 'Revenue_FoodBev', 'Revenue_Merch',\n       'Revenue_Arcade', 'Total_Revenue', 'Total_Labor_Hours',\n       'International_Visitors', 'High_Temperature', 'Precipitation',\n       'Weather_Type', 'Weather_Code', 'Promo_Flag', 'Promo_Type',\n       'Cruise_Docked', 'Is_Special_Event', 'Special_Events',\n       'Attraction_Tier', 'Zone_ID', 'Per_Capita_Spend'],\n      dtype='object')\n\n\n\n\nIn the dataset, there are four components of total revenue: gate revenue, revenue food, revenue merch, and revenue arcade. Generate a table that shows average spending in each of those areas, based on Special Event types.\n\navg_rev_by_event = lobster25.groupby('Special_Events')[['Gate_Revenue','Revenue_FoodBev','Revenue_Merch','Revenue_Arcade']].mean()\nprint(avg_rev_by_event)\n\n                         Gate_Revenue  Revenue_FoodBev  Revenue_Merch  \\\nSpecial_Events                                                          \nBeatles Tribute         146630.870000     86767.966667   21431.026667   \nMusicFest               107296.750000     83544.713333   33065.490000   \nNight Glow              107957.660000     59590.776667   31730.376667   \nNo Event                 73213.523333     40692.877500   15931.317500   \nTaylor Swift Lookalike   96895.336667     67165.716667   33479.076667   \nVintage Days            114238.410000     51074.406667   24561.976667   \n\n                        Revenue_Arcade  \nSpecial_Events                          \nBeatles Tribute           10488.050000  \nMusicFest                  9745.910000  \nNight Glow                 8637.820000  \nNo Event                   3624.482500  \nTaylor Swift Lookalike     9691.783333  \nVintage Days               6821.316667  \n\n\nTable analysis: Stand out points & limitations  Looking at the table, it’s clear that special events like Beatles Tribute and MusicFest bring in noticeably higher revenue across all areas compared to days with no events. This makes sense, as popular events tend to draw bigger crowds and encourage people to spend more on tickets, food, and merchandise. Some of the standout numbers are probably because certain events are more well-known or heavily promoted. That said, this analysis only looks at averages and doesn’t consider other factors like weather, day of the week, or seasonal trends. Because of these other influences, it’s hard to say for sure how much of the revenue boost is truly caused by the events themselves.\n\n\nA ‘Date’ variable type. Use the info() function to determine the data type for the‘Date’ variable.\n\nlobster25['Date'].info()\n\n&lt;class 'pandas.core.series.Series'&gt;\nRangeIndex: 99 entries, 0 to 98\nSeries name: Date\nNon-Null Count  Dtype \n--------------  ----- \n99 non-null     object\ndtypes: object(1)\nmemory usage: 924.0+ bytes\n\n\nPython currently views this variable as an object.\n\n# Converting the variable into a datetime object.\nlobster25['Date'] = pd.to_datetime(lobster25['Date'])\n\n# Check to see if the conversion is successful\nprint(lobster25['Date'].dtype)\n\ndatetime64[ns]\n\n\nConverting a column to datetime lets Python treat it as a real date, enabling operations like filtering by month, extracting weekdays, calculating durations, and plotting time-series data accurately. Without it, dates are just text and harder to analyze.\n\n\nUsing the groupby() function, along with the describe() function, explore the relationship between day of the week and total revenue.\n\nlobster25.groupby('Day_of_Week')[['Total_Revenue']].describe()\n\n\n\n\n\n\n\n\nTotal_Revenue\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nDay_of_Week\n\n\n\n\n\n\n\n\n\n\n\n\nFriday\n14.0\n124348.351429\n25766.281104\n68899.07\n113828.1850\n126337.460\n134174.9625\n180408.42\n\n\nMonday\n15.0\n123881.241333\n30711.357993\n88753.13\n101210.0300\n110997.990\n143166.6050\n202565.15\n\n\nSaturday\n14.0\n233435.688571\n63186.045667\n132174.43\n209613.2550\n227397.180\n238614.4925\n423019.37\n\n\nSunday\n14.0\n222040.763571\n51689.800794\n127310.71\n211008.1775\n245217.160\n251046.1250\n284520.15\n\n\nThursday\n14.0\n110808.970000\n23883.366451\n63247.57\n94773.4125\n112346.540\n128805.5325\n147562.14\n\n\nTuesday\n14.0\n110803.409286\n28858.768417\n64017.91\n95816.2750\n106621.260\n132957.8850\n166042.42\n\n\nWednesday\n14.0\n111780.540000\n24591.350227\n75302.24\n92629.4175\n111242.325\n127296.6300\n165467.64\n\n\n\n\n\n\n\nLooking at the table, it’s pretty noticeable that weekends bring in much higher revenue than weekdays. Saturday averages around $233,000 and Sunday about $222,000, while most weekdays hover between $110,000 and $125,000. This makes sense since more people are likely to visit the park on their days off. The bigger variation in weekend revenues probably comes from special events or particularly busy days. Overall, the day of the week has a noticeable effect, with weekends clearly driving the biggest crowds and sales.\n\n\nLobster Land management has decided that they won’t be using the Zone_ID variable. Remove it, and demonstrate that it’s no longer part of the dataset going forward.\n\nlobster25 = lobster25.drop(columns=['Zone_ID'], errors='ignore')\n# Double-check\nprint(lobster25.columns)\n\nIndex(['Date', 'Day_of_Week', 'season_week_rel', 'Is_Holiday',\n       'Total_Visitors', 'Passholder_Percentage', 'Day_Tickets_Sold',\n       'Avg_Ticket_Price', 'Gate_Revenue', 'Revenue_FoodBev', 'Revenue_Merch',\n       'Revenue_Arcade', 'Total_Revenue', 'Total_Labor_Hours',\n       'International_Visitors', 'High_Temperature', 'Precipitation',\n       'Weather_Type', 'Weather_Code', 'Promo_Flag', 'Promo_Type',\n       'Cruise_Docked', 'Is_Special_Event', 'Special_Events',\n       'Attraction_Tier', 'Per_Capita_Spend'],\n      dtype='object')"
  },
  {
    "objectID": "python_notebooks/data_exploration_visualization.html#part-ii-visualization",
    "href": "python_notebooks/data_exploration_visualization.html#part-ii-visualization",
    "title": "Data Exploration & Visualization",
    "section": "Part II: Visualization",
    "text": "Part II: Visualization\n\nUsing any plotting tool in Python, generate a bar plot that shows Promo_Type on one axis and average gate revenue on the other. Be sure to give your plot a clear, descriptive title.\n\nsns.barplot(x = \"Promo_Type\", y = \"Gate_Revenue\", data = lobster25, hue=\"Promo_Type\", palette=\"Set2\", dodge=False, edgecolor='black');\nplt.xlabel(\"Promotion Type\")\nplt.ylabel(\"Avg. Gate Revenue ($)\")\nplt.title(\"Lobsterland's Average Gate Revenue during Promotion Types\")\nplt.show()\n\n\n\n\n\n\n\n\nThe bar plot shows that days with no promotion, email, or influencer campaigns have similar average gate revenue, while bundle promotions have noticeably lower revenue. Surprisingly, bundles seem less effective than doing nothing or running other promotions, possibly because they attract fewer people or are offered on slower days. Overall, not all promotions necessarily increase gate revenue.\n\n\nLobster Land management wants to better understand how visitor counts vary across the days of the week. Generate a barplot that shows average total visitors by day of Week.\n\n# Version 1: y-axis starting at 0\nsns.barplot(x = \"Day_of_Week\", y = \"Gate_Revenue\", data = lobster25, color='skyblue', edgecolor='black');\nplt.xlabel(\"Day of the Week\")\nplt.ylabel(\"Avg Gate Revenue ($)\")\nplt.title(\"Lobsterland's Average Gate Revenue on Days of the Week\")\nplt.ylim(0)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Version 2: y-axis zoomed to highlight differences\n\navg_values = lobster25.groupby(\"Day_of_Week\")[\"Gate_Revenue\"].mean()\n\nsns.barplot(x=\"Day_of_Week\", y=\"Gate_Revenue\", data=lobster25, color='skyblue', edgecolor='black')\nplt.xlabel(\"Day of the Week\")\nplt.ylabel(\"Avg Gate Revenue ($)\")\nplt.title(\"Lobsterland's Average Gate Revenue on Days of the Week\")\n\n# Set y-axis limits slightly beyond min and max average values\nplt.ylim(avg_values.min() * 0.95, avg_values.max() * 1.05)\nplt.show()\n\n\n\n\n\n\n\n\nBoth plots display the same visitor data, but they feel very different. The first plot, with the y-axis starting at 0, shows the weekend boost in visitors clearly but keeps the differences in perspective. The second plot, with a zoomed-in y-axis, makes those differences look much larger and more dramatic than they really are. This kind of change can easily influence how people interpret the importance of the trend. That’s why, for bar charts, it’s usually best to start the axis at 0 — it helps present the data honestly and avoids misleading your audience.\n\n\nNext, generate a histogram of per capita spend across all visitors in the 2025 season. Be sure to give your plot a clear, descriptive title.\n\n#Version 1: Non-adjusted bins\nsns.histplot(data=lobster25, x=\"Per_Capita_Spend\", color='skyblue', edgecolor='black')\nplt.xlabel(\"Per Capita Spend ($)\")\nplt.title(\"Lobsterland Per Capita Spend in 2025\")\nplt.show()\n\n\n\n\n\n\n\n\nThe histogram shows that most visitors spend between about $70 and $85, with spending peaking around $80. The distribution is roughly bell-shaped, suggesting that per capita spending is fairly consistent across visitors, with only a few spending much lower or higher than the average.\n\n#Version 2: 50 bins\nsns.histplot(data=lobster25, x=\"Per_Capita_Spend\", bins=50, color='skyblue', edgecolor='black')\nplt.xlabel(\"Per Capita Spend ($)\")\nplt.title(\"Lobsterland Per Capita Spend in 2025\")\nplt.show()\n\n\n\n\n\n\n\n\nThe second histogram, with 50 bins, shows a lot more detail about how people spend. Instead of the smooth, bell-like shape in the first plot, you can see smaller peaks and dips, almost like there are a few “favorite” spending amounts that visitors cluster around. Someone looking at this version might notice that spending habits are a bit more varied and not perfectly smooth, which isn’t as clear in the first histogram.\n\n\nOne member of the Lobster Land Board of Directors thinks that people tend to buy more merch on hot days. Let’s see whether the data supports this theory! Generate a scatterplot with the day’s high temperature on the x-axis, and with total merch revenue on the y-axis. Be sure to give your plot a clear, descriptive title.\n\nsns.scatterplot(x = \"High_Temperature\", y = \"Revenue_Merch\", data = lobster25, color='skyblue');\nplt.xlabel(\"High Temperature\")\nplt.ylabel(\"Avg. Merch Revenue ($)\")\nplt.title(\"Lobsterland's Merch Revenue on Hot Temperature Day\")\nplt.show()\n\n\n\n\n\n\n\n\nThe scatterplot shows no clear relationship between high temperature and merch revenue — the points are spread fairly randomly across the temperature range. This suggests that hotter days do not consistently lead to higher merch sales. It’s possible that factors like crowd size, special events, or promotions play a bigger role in driving merch revenue than temperature alone.\n\n\nCreate a boxplot that shows day of week on one axis and passholder percentage on the other. Be sure to give your plot a clear, descriptive title.\n\nsns.boxplot(x=\"Day_of_Week\", y=\"Passholder_Percentage\", data=lobster25, color='skyblue')\nplt.xlabel(\"Day of the Week\")\nplt.ylabel(\"Passholder Percentage (%)\")\nplt.title(\"Lobsterland's Passholder Percentage during Day of the Week\")\nplt.show()\n\n\n\n\n\n\n\n\nThe boxplot shows that weekdays, especially Wednesday and Thursday, tend to have higher passholder percentages compared to weekends. Saturday has the lowest median and a narrower range, meaning fewer passholders visit that day. This might be because weekends attract more casual or one-time visitors, while passholders prefer visiting during less crowded weekdays.\n\n\nCreate a barplot that shows average daily total visitors by month.\n\n#create a new 'Month' column extracted from 'Date'\nlobster25['Month'] = lobster25['Date'].dt.month_name()\n\nmonthly_avg = lobster25.groupby('Month', as_index=False)['Total_Visitors'].mean()\n\nsns.barplot(x=\"Month\", \n            y=\"Total_Visitors\", \n            data=monthly_avg, \n            hue=\"Month\", \n            palette=\"Set2\", \n            dodge=False, \n            edgecolor='black')\nplt.xlabel(\"Month\")\nplt.ylabel(\"Avg. Daily Total Visitors\")\nplt.title(\"Average Daily Total Visitors at Lobsterland by Month\")\nplt.show()\n\n\n\n\n\n\n\n\nThe bar plot shows that average daily visitors are highest in September and lowest in May and July. There’s a clear upward trend from August into September, suggesting strong late-season attendance. I would tell management that September appears to be a key month for drawing visitors, which could be a good time to focus marketing campaigns or launch special fall events to maximize engagement and revenue.\n\n\nGenerate a barplot that shows special events on one axis and average international visitors on the other.\n\navg_visitor = lobster25.groupby(\"Special_Events\")[\"International_Visitors\"].mean().sort_values(ascending=False)\nprint(avg_visitor)\n\nSpecial_Events\nTaylor Swift Lookalike    236.666667\nMusicFest                 201.333333\nBeatles Tribute           194.000000\nVintage Days              178.666667\nNight Glow                172.000000\nNo Event                  167.738095\nName: International_Visitors, dtype: float64\n\n\n\norder_event = avg_visitor.index\nprint(order_event)\n\nIndex(['Taylor Swift Lookalike', 'MusicFest', 'Beatles Tribute',\n       'Vintage Days', 'Night Glow', 'No Event'],\n      dtype='object', name='Special_Events')\n\n\n\nsns.barplot(x=\"Special_Events\", \n            y=\"International_Visitors\", \n            data=lobster25, \n            order=order_event, \n            errorbar=None, \n            hue=\"Special_Events\", \n            palette=\"Set3\", \n            dodge=False, \n            edgecolor='black')\nplt.xlabel(\"Special Events\")\nplt.ylabel(\"Avg. International Visitors\")\nplt.xticks(rotation=45)\nplt.title(\"Lobsterland's Average International Visitors on Special Events\")\nplt.show()\n\n\n\n\n\n\n\n\nThe barplot shows that the Taylor Swift Lookalike event attracts the highest average number of international visitors, followed by MusicFest and Beatles Tribute. Days with no events have the lowest international attendance. This suggests that special events, especially music-related ones, may encourage more international guests to visit. However, we cannot assume the events directly cause the increase, other factors like peak tourist season or holidays could also explain the higher numbers. This is why the relationship should be viewed as a correlation, not proof of causation.\n\n\nCreate a faceted bar plot of food & beverage revenue, with facets for weather type, and bars representing day of the week.\n\nsns.catplot(x=\"Day_of_Week\", \n            y=\"Revenue_FoodBev\", \n            col=\"Weather_Type\", \n            data=lobster25, \n            kind=\"bar\",\n            color='skyblue', \n            edgecolor='black',\n            errorbar=None).fig.suptitle(\"Food & Beverage Revenue by Day of the Week, Faceted by Weather Type\", fontsize=16, y=1.05)\nplt.xlabel(\"Day of the Week\")\nplt.ylabel(\"Food & Beverage Revenue\")\nplt.show()\n\n\n\n\n\n\n\n\nThis plot shows that food and beverage sales really take off on weekends, especially when the weather is nice or just a bit cloudy. Thunderstorms seem to keep people away no matter the day, while sunny or partly cloudy Saturdays and Sundays bring in the biggest revenue. It’s interesting that even rainy weekends still perform better than most weekdays. However, this visualization does not show the number of observations (sample size) for each weather-day combination, making it hard to know whether some patterns are based on only a few data points.\n\n\nCreate a lineplot that shows season_week_rel on the x-axis, and aggregate (total) Gold Zone spending for that week on the y-axis.\n\nlobster25[['Attraction_Tier','Per_Capita_Spend']]\n\n\n\n\n\n\n\n\nAttraction_Tier\nPer_Capita_Spend\n\n\n\n\n0\n2\n73.43\n\n\n1\n2\n59.30\n\n\n2\n3\n82.33\n\n\n3\n2\n91.19\n\n\n4\n1\n58.67\n\n\n...\n...\n...\n\n\n94\n2\n81.26\n\n\n95\n1\n79.43\n\n\n96\n3\n82.08\n\n\n97\n2\n68.70\n\n\n98\n1\n84.26\n\n\n\n\n99 rows × 2 columns\n\n\n\n\n# Filter attraction tier to gold zone which is 1, copy() from lobster25() dataframe\ngold_zone = lobster25.query('Attraction_Tier==1').copy() \n\n# Double-check to see if there's any zone other than 1\nprint(gold_zone['Attraction_Tier'].unique())\n\n[1]\n\n\n\n# Add a 'Total_Spend' column to record the total spending of gold zone by multiplying total visitors by per visitor spend\ngold_zone['Total_Spend'] = gold_zone['Total_Visitors']*gold_zone['Per_Capita_Spend']\n\n# Double-check to see if the tier and the calculation is correct\ngold_zone[['Attraction_Tier','Total_Spend','season_week_rel']].tail()\n\n\n\n\n\n\n\n\nAttraction_Tier\nTotal_Spend\nseason_week_rel\n\n\n\n\n88\n1\n131917.20\n13\n\n\n89\n1\n209116.95\n13\n\n\n91\n1\n110994.19\n14\n\n\n95\n1\n138287.63\n14\n\n\n98\n1\n202561.04\n15\n\n\n\n\n\n\n\n\n# Aggregrate the total spending of gol\ngold_spend_weekly = gold_zone.groupby('season_week_rel')['Total_Spend'].sum().reset_index()\n\n# Plot the line chart\nsns.lineplot(data=gold_spend_weekly, x='season_week_rel',y='Total_Spend', color='skyblue')\nplt.xlabel(\"Week of the Season\")\nplt.ylabel(\"Total Spending\")\nplt.title(\"Lobsterland's Gold Zone Weekly Total Spending of the Season\")\nplt.show()\n\n\n\n\n\n\n\n\nThis line plot shows that Gold Zone spending starts low during opening week and rises quickly, peaking around Weeks 4–5. After that, there’s a big drop, though Week 7 shows a nice little rebound before things taper off again toward the end of the season. This tells us guests spend the most when excitement is fresh and maybe when special events happen. I’d suggest management double down on promotions early in the season and figure out what drove that Week 7 spike so we can recreate it."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nhat Tran",
    "section": "",
    "text": "I’m a Business Analytics graduate student at Boston University, with hands-on experience in macro research, financial analytics, and data-driven decision-making. My academic foundation in finance and economics from Drexel University helps me bridge quantitative analysis with practical business insight.\nI specialize in:\n\nDesigning automated workflows for efficient, reproducible analytics\nBuilding predictive and explanatory models\nTransforming complex datasets into clear, actionable insights\nSupporting business and investment decisions with evidence-based recommendations\n\nFeel free to explore the site to learn more about my work, projects, and professional experience."
  },
  {
    "objectID": "coursework.html",
    "href": "coursework.html",
    "title": "Relevant Coursework",
    "section": "",
    "text": "M.S. in Business Analytics, Boston University\n\n\n\n\n  \n  \n  Applied data manipulation, querying, and analytics workflows using Python and SQL.\n  \n    Python\n    SQL\n    Data Manipulation\n    ETL\n  \n\n  \n  \n  Core analytical methods, data-driven problem solving, and statistical reasoning.\n  \n    R\n    Machine Learning\n    Power BI\n    Statistical Reasoning\n  \n\n  \n  \n  Scalable data processing, cloud-based analytics architectures, and deployment concepts.\n  \n    Python\n    Machine Learning\n    SQL\n    plotly\n    AWS Cloud Architecture\n  \n\n  \n  \n  Customer segmentation, predictive modeling, and performance measurement using data.\n  \n    Python\n    Tableau\n    Regression\n    Clustering\n    Classification\n    Random Forest\n  \n\n\n\nTechniques for extracting insights from structured and unstructured data using Python, R, SQL, and Power BI.\n\n  Python\n  R\n  SQL\n  NLP\n  Clustering\n  Classification\n\n\n\n\nLearn to leverage risk analytics to identify opportunities, mitigate threats, and enhance enterprise value using advanced analytics tools.\n\n  Python\n  R\n  SQL\n  Optimization\n\n\n  \n  \n  Decision analysis using quantitative models complemented by qualitative frameworks.\n  \n    R\n    Optimization\n    Scenario Modeling\n  \n\n  \n  \n  Financial statement analysis, valuation fundamentals, and analytics applications in finance.\n  \n    Financial Modeling"
  },
  {
    "objectID": "coursework.html#graduate-coursework",
    "href": "coursework.html#graduate-coursework",
    "title": "Relevant Coursework",
    "section": "",
    "text": "M.S. in Business Analytics, Boston University\n\n\n\n\n  \n  \n  Applied data manipulation, querying, and analytics workflows using Python and SQL.\n  \n    Python\n    SQL\n    Data Manipulation\n    ETL\n  \n\n  \n  \n  Core analytical methods, data-driven problem solving, and statistical reasoning.\n  \n    R\n    Machine Learning\n    Power BI\n    Statistical Reasoning\n  \n\n  \n  \n  Scalable data processing, cloud-based analytics architectures, and deployment concepts.\n  \n    Python\n    Machine Learning\n    SQL\n    plotly\n    AWS Cloud Architecture\n  \n\n  \n  \n  Customer segmentation, predictive modeling, and performance measurement using data.\n  \n    Python\n    Tableau\n    Regression\n    Clustering\n    Classification\n    Random Forest\n  \n\n\n\nTechniques for extracting insights from structured and unstructured data using Python, R, SQL, and Power BI.\n\n  Python\n  R\n  SQL\n  NLP\n  Clustering\n  Classification\n\n\n\n\nLearn to leverage risk analytics to identify opportunities, mitigate threats, and enhance enterprise value using advanced analytics tools.\n\n  Python\n  R\n  SQL\n  Optimization\n\n\n  \n  \n  Decision analysis using quantitative models complemented by qualitative frameworks.\n  \n    R\n    Optimization\n    Scenario Modeling\n  \n\n  \n  \n  Financial statement analysis, valuation fundamentals, and analytics applications in finance.\n  \n    Financial Modeling"
  },
  {
    "objectID": "coursework.html#undergraduate-coursework",
    "href": "coursework.html#undergraduate-coursework",
    "title": "Relevant Coursework",
    "section": "Undergraduate Coursework",
    "text": "Undergraduate Coursework\n\n\n  B.S. in Economics, Drexel University\n\n\n\n\n\nMicroeconomics & Macroeconomics\nExplored market behavior, incentives, and macroeconomic dynamics underlying business and policy decisions.\n\n  Economic Theory\n  Market Analysis\n\n\nApplied Econometrics\nUsed regression, hypothesis testing, and time-series techniques to analyze economic and business data.\n\n  Regression\n  Time Series\n  Statistical Analysis\n\n\nDatabase Design & Implementation\nDesigned relational databases and wrote SQL queries to manage, retrieve, and analyze data efficiently.\n\n  SQL\n  Database Management\n  Relational Databases\n\n\nManagement Information Systems\nApplied information systems and database concepts to support business operations and decision-making.\n\n  SQL\n  Business Systems\n\n\nGame Theory Applications\nModeled strategic decision-making and competitive behavior using game-theoretic frameworks.\n\n  Strategic Analysis\n  Decision Theory\n  Economic Modeling\n\n\nIntermediate Corporate Finance\nAnalyzed capital structure, valuation, and investment decisions using financial theory and quantitative methods.\n\n  Valuation\n  Financial Analysis\n\n\nInvestment Securities & Markets\nStudied financial markets, asset pricing, and portfolio construction across major asset classes.\n\n  Financial Markets\n  Asset Pricing\n  Portfolio Analysis"
  },
  {
    "objectID": "pynotes.html",
    "href": "pynotes.html",
    "title": "Python Notebooks",
    "section": "",
    "text": "Classification\n\n\n\n\n\n\n\n\n\n\nData Exploration & Visualization\n\n\n\n\n\n\n\n\n\n\nHandling Time Series Data & Modeling with an Interaction Term\n\n\n\n\n\n\n\n\n\n\nMarket Segmentation & Conjoint Analysis\n\n\n\n\n\n\n\n\n\n\nStatistical Testing\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "python_notebooks/classification.html",
    "href": "python_notebooks/classification.html",
    "title": "Classification",
    "section": "",
    "text": "Will this Passholder Renew for Next Season?  Built and evaluated logistic regression and random forest models in Python to predict passholder renewal behavior using demographic, behavioral, and engagement data. Applied data cleaning, feature engineering, model interpretation, and performance evaluation to generate actionable insights for targeted marketing and retention strategy, complemented by an interactive Tableau KPI dashboard."
  },
  {
    "objectID": "python_notebooks/classification.html#part-i-logistic-regression-model",
    "href": "python_notebooks/classification.html#part-i-logistic-regression-model",
    "title": "Classification",
    "section": "Part I: Logistic Regression Model",
    "text": "Part I: Logistic Regression Model\n\n# Import necessary functions to use\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n\npassholder = pd.read_csv(\"data/lobsterland_passholders_dataset_25.csv\")\n\n\npassholder.head()\n\n\n\n\n\n\n\n\nAge\nPrevious_Visits\nTotal_Spend_2024\nFeedback_Score\nGold_Zone_Visits\nEmail_Engagement_Score\nDistance_From_Park_Miles\nHome_State\nPreferred_Attraction\nReferral_Source\nDining_Plan\nRenewed_Pass\n\n\n\n\n0\n59.000000\n3.000000\n394.780000\n4.606232\n4.000000\n44.200000\n1.900000\nVT\nThrill\nSocial Media\nUpgraded\n0\n\n\n1\n27.846318\n1.064843\n172.541334\n2.102264\n2.000819\n37.103840\n21.709631\nNH\nOther\nAd/Other\nNaN\n1\n\n\n2\n18.000000\n5.926144\n151.731274\n3.365074\n1.019084\n61.827722\n21.892411\nVT\nEntertainment\nSocial Media\nUpgraded\n1\n\n\n3\n25.000000\n12.000000\n251.300000\n3.458663\n1.000000\n41.800000\n6.600000\nNY\nOther\nFriend\nUpgraded\n0\n\n\n4\n66.206178\n4.038141\n197.686695\n2.985864\n3.955755\n61.174332\n29.981959\nNJ\nEntertainment\nAd/Other\nUpgraded\n1\n\n\n\n\n\n\n\n\npassholder.info()\n\n&lt;class 'pandas.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 12 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   Age                       1000 non-null   float64\n 1   Previous_Visits           1000 non-null   float64\n 2   Total_Spend_2024          1000 non-null   float64\n 3   Feedback_Score            1000 non-null   float64\n 4   Gold_Zone_Visits          1000 non-null   float64\n 5   Email_Engagement_Score    1000 non-null   float64\n 6   Distance_From_Park_Miles  1000 non-null   float64\n 7   Home_State                1000 non-null   str    \n 8   Preferred_Attraction      1000 non-null   str    \n 9   Referral_Source           1000 non-null   str    \n 10  Dining_Plan               667 non-null    str    \n 11  Renewed_Pass              1000 non-null   int64  \ndtypes: float64(7), int64(1), str(4)\nmemory usage: 93.9 KB\n\n\n\nNumerical Variables: Age, Previous_Visits, Total_Spend_2024, Feedback_Score, Gold_Zone_Visits, Email_Engagement_Score, Distance_From_Park_Miles\nCategorical Variables: Home_State, Preferred_Attraction, Referral_Source, Dining_Plan\nRenewed_Pass: binary target (0/1 or bool); treat as outcome, not a predictor.\n\n\n# Convert the variables to the categorical data type\npassholder[['Home_State', 'Preferred_Attraction', 'Referral_Source', 'Dining_Plan']] = passholder[['Home_State', 'Preferred_Attraction', 'Referral_Source', 'Dining_Plan']].astype('category')\n\n# Convert the Renewed_Pass to bool\npassholder[['Renewed_Pass']] = passholder[['Renewed_Pass']].astype('bool')\n\n# Double-check to see if all the variables return to the correct type\npassholder.info()\n\n&lt;class 'pandas.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 12 columns):\n #   Column                    Non-Null Count  Dtype   \n---  ------                    --------------  -----   \n 0   Age                       1000 non-null   float64 \n 1   Previous_Visits           1000 non-null   float64 \n 2   Total_Spend_2024          1000 non-null   float64 \n 3   Feedback_Score            1000 non-null   float64 \n 4   Gold_Zone_Visits          1000 non-null   float64 \n 5   Email_Engagement_Score    1000 non-null   float64 \n 6   Distance_From_Park_Miles  1000 non-null   float64 \n 7   Home_State                1000 non-null   category\n 8   Preferred_Attraction      1000 non-null   category\n 9   Referral_Source           1000 non-null   category\n 10  Dining_Plan               667 non-null    category\n 11  Renewed_Pass              1000 non-null   bool    \ndtypes: bool(1), category(4), float64(7)\nmemory usage: 59.9 KB\n\n\n\npassholder['Renewed_Pass'].value_counts()\n\nRenewed_Pass\nTrue     580\nFalse    420\nName: count, dtype: int64\n\n\nBy using the value_counts() function, I found that out of 1,000 passholders, 580 renewed their passes (True) and 420 did not (False). This means that about 58% of customers chose to renew, while 42% did not. Understanding this distribution helps LobsterLand’s management identify the current renewal rate and focus on strategies to increase it—for example, by attracting non-renewers with targeted promotions or loyalty programs.\n\nMissing values\n\npassholder.isnull().sum()\n\nAge                           0\nPrevious_Visits               0\nTotal_Spend_2024              0\nFeedback_Score                0\nGold_Zone_Visits              0\nEmail_Engagement_Score        0\nDistance_From_Park_Miles      0\nHome_State                    0\nPreferred_Attraction          0\nReferral_Source               0\nDining_Plan                 333\nRenewed_Pass                  0\ndtype: int64\n\n\n\npassholder['Dining_Plan'].head()\n\n0    Upgraded\n1         NaN\n2    Upgraded\n3    Upgraded\n4    Upgraded\nName: Dining_Plan, dtype: category\nCategories (1, str): ['Upgraded']\n\n\n\n# Add category 'None' to 'Dining_Plan' variable then fill any NaN values with 'None'\npassholder['Dining_Plan'] = passholder['Dining_Plan'].cat.add_categories(['None']).fillna('None')\n\n\n# Double-check to see if 'Dining_Plan' variable still has any missing values\npassholder['Dining_Plan'].isnull().sum()\npassholder['Dining_Plan'].unique\n\n&lt;bound method Series.unique of 0      Upgraded\n1          None\n2      Upgraded\n3      Upgraded\n4      Upgraded\n         ...   \n995    Upgraded\n996    Upgraded\n997    Upgraded\n998    Upgraded\n999    Upgraded\nName: Dining_Plan, Length: 1000, dtype: category\nCategories (2, str): ['Upgraded', 'None']&gt;\n\n\nAccording to the dataset description, the Dining_Plan variable indicates whether a passholder purchased a Lobster Land food plan, with two possible options: ‘None’ and ‘Upgraded’, where Upgraded includes both the Standard and Premium tiers. There are 333 missing values in this variable, which likely represent passholders who did not purchase any dining plan. For consistency, I replaced all missing values with ‘None’ to accurately reflect this behavior and ensure the data remains complete for analysis.\n\n\nImpossible values\n\npassholder.describe()\n\n\n\n\n\n\n\n\nAge\nPrevious_Visits\nTotal_Spend_2024\nFeedback_Score\nGold_Zone_Visits\nEmail_Engagement_Score\nDistance_From_Park_Miles\n\n\n\n\ncount\n1000.000000\n1000.000000\n1000.000000\n1000.000000\n1000.000000\n1000.000000\n1000.000000\n\n\nmean\n45.843471\n4.739685\n211.420490\n3.231483\n2.045040\n50.055373\n30.708474\n\n\nstd\n16.278317\n2.291030\n105.815989\n0.992790\n1.411507\n19.941021\n17.108942\n\n\nmin\n18.000000\n0.000000\n-39.950000\n1.000000\n0.000000\n-8.100000\n0.100000\n\n\n25%\n33.000000\n3.000000\n142.775364\n2.585852\n1.000000\n35.975548\n18.300000\n\n\n50%\n46.278713\n5.000000\n199.942199\n3.306673\n2.000000\n50.450000\n29.837614\n\n\n75%\n61.000000\n6.005339\n274.070000\n3.826473\n3.000000\n62.783218\n43.797075\n\n\nmax\n74.000000\n12.000000\n559.830000\n5.000000\n8.000000\n108.300000\n87.100000\n\n\n\n\n\n\n\n\n\nAre there any values in this dataset that appear to be impossible? If so, why? If not, why not?\n\n# View rows with negative Total_Spend_2024 and Email_Engagement_Score outside of range 0-100\npassholder[(passholder['Total_Spend_2024'] &lt; 0)]\n\n\n\n\n\n\n\n\nAge\nPrevious_Visits\nTotal_Spend_2024\nFeedback_Score\nGold_Zone_Visits\nEmail_Engagement_Score\nDistance_From_Park_Miles\nHome_State\nPreferred_Attraction\nReferral_Source\nDining_Plan\nRenewed_Pass\n\n\n\n\n228\n34.632594\n2.948977\n-1.000297\n3.072944\n0.965401\n53.158994\n53.764673\nME\nOther\nAd/Other\nUpgraded\nTrue\n\n\n280\n34.092198\n2.986994\n-0.508581\n3.033482\n0.977585\n52.641486\n53.059526\nME\nOther\nAd/Other\nUpgraded\nFalse\n\n\n465\n54.000000\n6.000000\n-39.950000\n3.142010\n4.000000\n58.900000\n10.300000\nNY\nOther\nAd/Other\nUpgraded\nTrue\n\n\n477\n34.430205\n2.951587\n-0.751035\n3.027981\n0.996431\n51.767003\n52.981610\nME\nOther\nAd/Other\nUpgraded\nTrue\n\n\n485\n33.764976\n2.934015\n-1.852003\n3.043365\n1.004973\n53.342196\n53.162699\nME\nOther\nAd/Other\nUpgraded\nTrue\n\n\n493\n26.000000\n5.000000\n-2.230000\n3.555003\n2.000000\n55.600000\n27.800000\nVT\nThrill\nAd/Other\nUpgraded\nTrue\n\n\n499\n33.327308\n2.984666\n-1.439116\n3.037064\n0.967710\n53.574152\n53.156368\nME\nOther\nAd/Other\nUpgraded\nFalse\n\n\n573\n42.000000\n4.000000\n-23.910000\n3.239964\n6.000000\n66.800000\n32.500000\nMA\nOther\nSocial Media\nUpgraded\nTrue\n\n\n750\n33.715105\n3.037335\n-3.514091\n3.059007\n1.022880\n53.124907\n53.273690\nME\nOther\nAd/Other\nUpgraded\nTrue\n\n\n761\n52.000000\n4.000000\n-3.180000\n3.991413\n4.000000\n66.700000\n51.200000\nNJ\nEntertainment\nAd/Other\nUpgraded\nFalse\n\n\n884\n34.367413\n3.052398\n-1.531736\n3.076422\n0.990859\n52.913776\n54.164073\nME\nOther\nAd/Other\nUpgraded\nTrue\n\n\n\n\n\n\n\n\n# View rows with Email_Engagement_Score outside of range 0-100\npassholder[(passholder['Email_Engagement_Score'] &lt; 0) | (passholder['Email_Engagement_Score'] &gt; 100)]\n\n\n\n\n\n\n\n\nAge\nPrevious_Visits\nTotal_Spend_2024\nFeedback_Score\nGold_Zone_Visits\nEmail_Engagement_Score\nDistance_From_Park_Miles\nHome_State\nPreferred_Attraction\nReferral_Source\nDining_Plan\nRenewed_Pass\n\n\n\n\n308\n42.0\n2.0\n385.92\n2.830563\n1.0\n102.9\n9.4\nMA\nOther\nAd/Other\nNone\nFalse\n\n\n379\n21.0\n4.0\n345.79\n2.868348\n1.0\n-8.1\n39.3\nME\nThrill\nAd/Other\nNone\nFalse\n\n\n418\n37.0\n4.0\n339.49\n3.264726\n1.0\n105.3\n34.9\nVT\nEntertainment\nAd/Other\nNone\nTrue\n\n\n902\n55.0\n4.0\n150.17\n4.445888\n0.0\n108.3\n41.9\nNY\nThrill\nAd/Other\nUpgraded\nFalse\n\n\n\n\n\n\n\n\n# Drop the negative Total_Spend_2024 value by filtering it to be greater than 0\npassholder = passholder[passholder['Total_Spend_2024'] &gt;= 0]\n\nThe negative values in Total_Spend_2024 range from –39 to –0.50, which isn’t much and could possibly represent small refunds from Lobster Land to customers. However, negative spending contradicts the variable’s definition, which means to capture the total amount spent by a passholder during the season. Even if these small negatives reflect refunds, they should be tracked separately to avoid misrepresenting overall spending behavior. Since there are only 11 negative values, accounting for less than 1.5% of the dataset, I decided to drop them to maintain a clean and accurate dataset for further analysis.\n\n# Cap Email_Engagement_Score at 100 for those exceeding the 100 range\npassholder['Email_Engagement_Score'] = passholder['Email_Engagement_Score'].clip(upper=100)\n\nI capped the Email_Engagement_Score at 100 because a few users had scores slightly above that limit (102.9; 105.3; 108.3), likely due to very high engagement. All three users visited Lobster Land at least twice, showing a 100% return rate after their first visit, which is possibly influenced by follow-up emails. Clipping these minor outliers keeps the metric consistent with its defined scale while preserving meaningful behavioral signals in the data.\n\n# Drop the negative Email_Engagement_Score value by filtering it to be greater than 0\npassholder = passholder[passholder['Email_Engagement_Score'] &gt;= 0]\n\nI decided to drop the row with a negative Email_Engagement_Score because it’s supposed to stay within the 0–100 range, and negative values don’t have any meaningful interpretation. Since only one row is affected, I felt removing it would keep the dataset clean without introducing bias and ensure consistent analysis.\n\n# Re-check to see if the dataset contains any nan value\npassholder.isnull().sum()\n\nAge                         0\nPrevious_Visits             0\nTotal_Spend_2024            0\nFeedback_Score              0\nGold_Zone_Visits            0\nEmail_Engagement_Score      0\nDistance_From_Park_Miles    0\nHome_State                  0\nPreferred_Attraction        0\nReferral_Source             0\nDining_Plan                 0\nRenewed_Pass                0\ndtype: int64\n\n\n\n\nExamining correlations\n\n\nBuild a correlation table to examine the correlations among your numeric independent variables.\n\n# Filter out numeric variables to examine the correlations\nnumeric_vars = passholder.select_dtypes(include=['float64', 'int64'])\n\n# Create a correlation table with numeric variables from the dataset\ncorr_table = numeric_vars.corr()\nprint(corr_table.round(3).to_string())\n\n                            Age  Previous_Visits  Total_Spend_2024  Feedback_Score  Gold_Zone_Visits  Email_Engagement_Score  Distance_From_Park_Miles\nAge                       1.000            0.072             0.135           0.009             0.020                  -0.037                     0.100\nPrevious_Visits           0.072            1.000             0.054          -0.070            -0.084                  -0.071                    -0.094\nTotal_Spend_2024          0.135            0.054             1.000           0.046            -0.054                  -0.001                     0.065\nFeedback_Score            0.009           -0.070             0.046           1.000             0.027                  -0.092                    -0.033\nGold_Zone_Visits          0.020           -0.084            -0.054           0.027             1.000                   0.023                    -0.092\nEmail_Engagement_Score   -0.037           -0.071            -0.001          -0.092             0.023                   1.000                    -0.093\nDistance_From_Park_Miles  0.100           -0.094             0.065          -0.033            -0.092                  -0.093                     1.000\n\n\n\n# Display the correlation table in heatmap\nplt.figure(figsize=(8,6))\nsns.heatmap(corr_table, annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title(\"Correlation Heatmap of Numeric Variables\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nAre there any correlations here that are so high as to present a likely problem with multicollinearity?\nThe correlation table indicates that none of the variables are strongly related to one another, as all coefficients are well below the typical threshold for multicollinearity (r ≥ 0.8). The highest correlation, 0.135 between Age and Total_Spend_2024 indicating older renewers tend to spend more than the younger audience, is too weak to raise any concerns. Therefore, I can keep all numeric predictors for the modeling stage.\n\n\nFor any variables that need to be dummified, dummify them, being sure to drop one level as you do.\n\n# Dummify all categorical variables and dropping one category per variable to avoid multicollinearity\npassholder = pd.get_dummies(passholder, columns=[\n    'Home_State',\n    'Preferred_Attraction',\n    'Referral_Source',\n    'Dining_Plan'\n], drop_first=True)\n\n\npassholder.head(5)\n\n\n\n\n\n\n\n\nAge\nPrevious_Visits\nTotal_Spend_2024\nFeedback_Score\nGold_Zone_Visits\nEmail_Engagement_Score\nDistance_From_Park_Miles\nRenewed_Pass\nHome_State_ME\nHome_State_NH\nHome_State_NJ\nHome_State_NY\nHome_State_VT\nPreferred_Attraction_Other\nPreferred_Attraction_Thrill\nReferral_Source_Friend\nReferral_Source_Social Media\nDining_Plan_None\n\n\n\n\n0\n59.000000\n3.000000\n394.780000\n4.606232\n4.000000\n44.200000\n1.900000\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n1\n27.846318\n1.064843\n172.541334\n2.102264\n2.000819\n37.103840\n21.709631\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\n\n\n2\n18.000000\n5.926144\n151.731274\n3.365074\n1.019084\n61.827722\n21.892411\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n3\n25.000000\n12.000000\n251.300000\n3.458663\n1.000000\n41.800000\n6.600000\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\n\n\n4\n66.206178\n4.038141\n197.686695\n2.985864\n3.955755\n61.174332\n29.981959\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n\n\n\nAfter dummifying the categorical variables, I set the following reference levels: Home_State – MA, Preferred_Attraction – Entertainment, Referral_Source – Ad/Other, and Dining_Plan – Upgraded. These serve as the baseline categories against which other levels are compared in the model. The Renewed_Pass variable is already numeric and binary, so it does not require dummification.\n\n\nCreate a data partition. Assign 40% of your rows to your test set, and 60% to your training set.\n\nfrom sklearn.model_selection import train_test_split\ntrain_df, test_df = train_test_split(passholder, test_size=0.4, random_state=156)\nprint(f\"Training set size: {len(train_df)} rows\")\nprint(f\"Test set size: {len(test_df)} rows\")\n\nTraining set size: 592 rows\nTest set size: 396 rows\n\n\nFor the random seed, I chose 156 because it matches the last three digits of my BUID. Using a personal but simple reference like this helps ensure consistency while keeping the seed easy to remember. It also guarantees that my results are reproducible if I need to rerun the analysis.\n\n\nCompare the mean values of the variables in the dataset after grouping by Renewed_Pass.\n\n# Change Renewed_Pass to Class 0/1\npassholder['Renewed_Pass'] = passholder['Renewed_Pass'].astype(int)\npassholder.groupby('Renewed_Pass').mean(numeric_only=True)\n\n\n\n\n\n\n\n\nAge\nPrevious_Visits\nTotal_Spend_2024\nFeedback_Score\nGold_Zone_Visits\nEmail_Engagement_Score\nDistance_From_Park_Miles\nHome_State_ME\nHome_State_NH\nHome_State_NJ\nHome_State_NY\nHome_State_VT\nPreferred_Attraction_Other\nPreferred_Attraction_Thrill\nReferral_Source_Friend\nReferral_Source_Social Media\nDining_Plan_None\n\n\nRenewed_Pass\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n47.381249\n4.672605\n217.665995\n3.176391\n2.014964\n51.457782\n30.060697\n0.110577\n0.127404\n0.165865\n0.201923\n0.290865\n0.454327\n0.348558\n0.194712\n0.211538\n0.358173\n\n\n1\n44.929268\n4.811163\n210.848800\n3.272654\n2.067988\n48.989534\n30.889094\n0.159091\n0.173077\n0.125874\n0.178322\n0.208042\n0.477273\n0.325175\n0.145105\n0.225524\n0.319930\n\n\n\n\n\n\n\n\n\nFrom the results you see here, choose any THREE independent variables from the dataset, and speculate about their likely impact on the result – do you think this variable will be strongly impactful?\nI chose these three variables: Email_Engagement_Score, Total_Spend_2024, and Dining_Plan_None because they each capture different aspects of customer behavior that could influence renewal decisions. Email_Engagement_Score reflects digital interaction and communication responsiveness, Total_Spend_2024 shows spending patterns for each visitors, and Dining_Plan_None indicates spending habits and overall commitment to the park experience. Together, these variables provide a balanced mix of behavioral, experiential, and transactional factors that could help explain differences between renewers and non-renewers.\n\nEmail_Engagement_Score: Non-renewers have a slightly higher average score (51.45) than renewers (48.98). This suggests that email engagement may not directly drive renewal, or that more engaged users are more critical.\nTotal_Spend_2024: Non-renewers spent slightly more on average in 2024 (\\$217.67) than renewers (\\$210.85), which is unexpected if we assume higher spend reflects stronger loyalty. This could suggest that one-time high spenders don’t necessarily return, or that renewers are more consistent but spend less per season.\nDining_Plan_None: Non-renewers are more likely to have no dining plan (35.81%) compared to renewers (31.99%), suggesting that opting out of the food plan may correlate with lower overall commitment or value perception.\n\n\n\nIteration #1\n\nBuild a logistic regression model using statsmodels, with the outcome variable ‘Renewed_Pass’.\n\npassholder.head()\n\n\n\n\n\n\n\n\nAge\nPrevious_Visits\nTotal_Spend_2024\nFeedback_Score\nGold_Zone_Visits\nEmail_Engagement_Score\nDistance_From_Park_Miles\nRenewed_Pass\nHome_State_ME\nHome_State_NH\nHome_State_NJ\nHome_State_NY\nHome_State_VT\nPreferred_Attraction_Other\nPreferred_Attraction_Thrill\nReferral_Source_Friend\nReferral_Source_Social Media\nDining_Plan_None\n\n\n\n\n0\n59.000000\n3.000000\n394.780000\n4.606232\n4.000000\n44.200000\n1.900000\n0\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n1\n27.846318\n1.064843\n172.541334\n2.102264\n2.000819\n37.103840\n21.709631\n1\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\n\n\n2\n18.000000\n5.926144\n151.731274\n3.365074\n1.019084\n61.827722\n21.892411\n1\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n3\n25.000000\n12.000000\n251.300000\n3.458663\n1.000000\n41.800000\n6.600000\n0\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\n\n\n4\n66.206178\n4.038141\n197.686695\n2.985864\n3.955755\n61.174332\n29.981959\n1\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n\n\n\n\nprint(train_df.dtypes)\n\nAge                             float64\nPrevious_Visits                 float64\nTotal_Spend_2024                float64\nFeedback_Score                  float64\nGold_Zone_Visits                float64\nEmail_Engagement_Score          float64\nDistance_From_Park_Miles        float64\nRenewed_Pass                       bool\nHome_State_ME                      bool\nHome_State_NH                      bool\nHome_State_NJ                      bool\nHome_State_NY                      bool\nHome_State_VT                      bool\nPreferred_Attraction_Other         bool\nPreferred_Attraction_Thrill        bool\nReferral_Source_Friend             bool\nReferral_Source_Social Media       bool\nDining_Plan_None                   bool\ndtype: object\n\n\n\n# Convert all bool variables to numeric form\ntrain_df = train_df.astype({col: int for col in train_df.select_dtypes('bool').columns})\n\n\nimport statsmodels.api as sm\n\nX_train = train_df.drop(columns=['Renewed_Pass'])\ny_train = train_df['Renewed_Pass']\n\nX_train = sm.add_constant(X_train)\n\nlogit_model = sm.Logit(y_train, X_train)\npassholder_model = logit_model.fit()\n\nOptimization terminated successfully.\n         Current function value: 0.647964\n         Iterations 5\n\n\n\n\nShow the summary of your model with log_reg.summary().\n\npassholder_model.summary()\n\n\nLogit Regression Results\n\n\nDep. Variable:\nRenewed_Pass\nNo. Observations:\n592\n\n\nModel:\nLogit\nDf Residuals:\n574\n\n\nMethod:\nMLE\nDf Model:\n17\n\n\nDate:\nWed, 04 Feb 2026\nPseudo R-squ.:\n0.04379\n\n\nTime:\n20:03:21\nLog-Likelihood:\n-383.59\n\n\nconverged:\nTrue\nLL-Null:\n-401.16\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n0.005978\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n0.7841\n0.694\n1.130\n0.258\n-0.576\n2.144\n\n\nAge\n-0.0079\n0.005\n-1.462\n0.144\n-0.019\n0.003\n\n\nPrevious_Visits\n0.0647\n0.041\n1.568\n0.117\n-0.016\n0.146\n\n\nTotal_Spend_2024\n-0.0004\n0.001\n-0.411\n0.681\n-0.002\n0.001\n\n\nFeedback_Score\n0.1498\n0.093\n1.618\n0.106\n-0.032\n0.331\n\n\nGold_Zone_Visits\n0.0494\n0.064\n0.769\n0.442\n-0.076\n0.175\n\n\nEmail_Engagement_Score\n4.514e-05\n0.005\n0.010\n0.992\n-0.009\n0.009\n\n\nDistance_From_Park_Miles\n0.0031\n0.005\n0.559\n0.576\n-0.008\n0.014\n\n\nHome_State_ME\n-0.2888\n0.357\n-0.810\n0.418\n-0.988\n0.410\n\n\nHome_State_NH\n-0.1766\n0.364\n-0.486\n0.627\n-0.889\n0.536\n\n\nHome_State_NJ\n-1.1599\n0.349\n-3.322\n0.001\n-1.844\n-0.476\n\n\nHome_State_NY\n-0.6981\n0.329\n-2.122\n0.034\n-1.343\n-0.053\n\n\nHome_State_VT\n-1.0633\n0.322\n-3.299\n0.001\n-1.695\n-0.432\n\n\nPreferred_Attraction_Other\n-0.2398\n0.239\n-1.004\n0.315\n-0.708\n0.228\n\n\nPreferred_Attraction_Thrill\n-0.5000\n0.251\n-1.988\n0.047\n-0.993\n-0.007\n\n\nReferral_Source_Friend\n-0.3234\n0.246\n-1.313\n0.189\n-0.806\n0.159\n\n\nReferral_Source_Social Media\n-0.0085\n0.221\n-0.039\n0.969\n-0.442\n0.425\n\n\nDining_Plan_None\n0.0198\n0.186\n0.106\n0.916\n-0.346\n0.385\n\n\n\n\n\n\n\nWhich of your numeric variables here are showing high p-values?\nA p-value greater than 0.05 means the variable is not statistically significant, suggesting that any observed effect could easily be due to random chance rather than a real relationship with renewal behavior. All the numeric variables display relatively high p-values, suggesting that none are statistically significant predictors of renewal at the 0.05 level. Age (p = 0.144), Previous_Visits (p = 0.117), Feedback_Score (p = 0.106), and Distance_From_Park_Miles (p = 0.576) show weak associations with renewal status, while Total_Spend_2024 (p = 0.681) and Email_Engagement_Score (p = 0.992) are particularly uninformative. This indicates that these factors do not have a meaningful impact on predicting renewal behavior.\n\n\nFor your categorical variables, which ones are showing high p-values for ALL of the levels in the model summary?\nFor the categorical variables, both Referral_Source and Dining_Plan have high p-values across all levels: Referral_Source_Friend (p = 0.189), Referral_Source_Social Media (p = 0.969), and Dining_Plan_None (p = 0.916) suggesting they are not statistically significant overall.\nOn the other hand, several Home_State variables are significant, including NJ (p = 0.001), NY (p = 0.034), and VT (p = 0.001), indicating that passholder’s location may impact one’s renewal behavior. Similarly, Preferred_Attraction_Thrill is significant (p = 0.047), implying that customers who prefer thrill rides are less likely to renew compared to those who prefer entertainment attractions. Overall, these results suggest that geographic and experiential factors have stronger predictive power than engagement or spending-related measures.\n\n\n\nIteration #2\n\nBuild yet another model, use training set only.\nFor the new model, I’m keeping only the variables that showed a meaningful relationship with renewal in the previous analysis. This includes Home_State (specifically NJ, NY, and VT) and Preferred_Attraction_Thrill, since these had p-values below 0.05 and appear to influence whether a passholder renews.\nAll other variables are being dropped because they were not statistically significant. This includes all the numeric variables, such as Age, Previous_Visits, Total_Spend_2024, Feedback_Score, Gold_Zone_Visits and Email_Engagement_Score, as well as categorical variables: Referral_Source and Dining_Plan. Though Home_State and Preferred_Attraction have some insignificant variables, these sections still contain meaningful variables, so I will keep all of them for consistent result.\n\n# Drop all insignificant variables for the 2nd model including all numeric variables and some categorical variables\ndrop_vars = [\n    'Age', 'Previous_Visits', 'Total_Spend_2024', 'Feedback_Score',\n    'Gold_Zone_Visits', 'Email_Engagement_Score', 'Distance_From_Park_Miles',\n    'Referral_Source_Friend', 'Referral_Source_Social Media', 'Dining_Plan_None'\n]\n\nX_train = train_df.drop(columns=['Renewed_Pass'] + drop_vars)\ny_train = train_df['Renewed_Pass']\n\nX_train = sm.add_constant(X_train)\n\nlogit_model = sm.Logit(y_train, X_train)\npassholder_model2 = logit_model.fit()\npassholder_model2.summary()\n\nOptimization terminated successfully.\n         Current function value: 0.655800\n         Iterations 5\n\n\n\nLogit Regression Results\n\n\nDep. Variable:\nRenewed_Pass\nNo. Observations:\n592\n\n\nModel:\nLogit\nDf Residuals:\n584\n\n\nMethod:\nMLE\nDf Model:\n7\n\n\nDate:\nWed, 04 Feb 2026\nPseudo R-squ.:\n0.03222\n\n\nTime:\n20:03:21\nLog-Likelihood:\n-388.23\n\n\nconverged:\nTrue\nLL-Null:\n-401.16\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n0.0005348\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n1.2775\n0.318\n4.020\n0.000\n0.655\n1.900\n\n\nHome_State_ME\n-0.3568\n0.347\n-1.028\n0.304\n-1.037\n0.323\n\n\nHome_State_NH\n-0.1293\n0.346\n-0.373\n0.709\n-0.808\n0.550\n\n\nHome_State_NJ\n-1.1510\n0.344\n-3.343\n0.001\n-1.826\n-0.476\n\n\nHome_State_NY\n-0.7526\n0.322\n-2.340\n0.019\n-1.383\n-0.122\n\n\nHome_State_VT\n-1.0290\n0.315\n-3.267\n0.001\n-1.646\n-0.412\n\n\nPreferred_Attraction_Other\n-0.2879\n0.232\n-1.238\n0.216\n-0.744\n0.168\n\n\nPreferred_Attraction_Thrill\n-0.4526\n0.245\n-1.846\n0.065\n-0.933\n0.028\n\n\n\n\n\n\n\nUsing scikit-learn, build another version of your model, using your remaining variables.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\n\nX = passholder[['Home_State_ME','Home_State_NH','Home_State_NJ', 'Home_State_NY', 'Home_State_VT', 'Preferred_Attraction_Other', 'Preferred_Attraction_Thrill']]\ny = passholder['Renewed_Pass']\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.4, random_state=156\n)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nlogmodel = LogisticRegression(max_iter=1000)\nlogmodel.fit(X_train_scaled, y_train)\nprint(f\"Intercept: {logmodel.intercept_[0]:.3f}\")\nprint(\"\\nCoefficient:\")\nfor feature, coef in zip(X_train.columns, logmodel.coef_[0]):\n    print(f\"{feature}: {coef:.3f}\")\n\nIntercept: 0.372\n\nCoefficient:\nHome_State_ME: -0.115\nHome_State_NH: -0.035\nHome_State_NJ: -0.384\nHome_State_NY: -0.286\nHome_State_VT: -0.417\nPreferred_Attraction_Other: -0.139\nPreferred_Attraction_Thrill: -0.211\n\n\n\n\nAssess the performance of your model against the test set. Build a confusion matrix, and answer the following questions about your model.\n\n# Import metric functions from scikit-learn\nfrom sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, balanced_accuracy_score\n\ny_pred = logmodel.predict(X_test_scaled)\n\npassholder_cm = confusion_matrix(y_test, y_pred)\ntn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\nprint(\"The model's confusion matrix:\\n\", passholder_cm)\n\nThe model's confusion matrix:\n [[ 59 113]\n [ 69 155]]\n\n\n\nplt.figure(figsize=(5, 4))\nsns.heatmap(passholder_cm, annot=True, fmt='d', cmap='coolwarm',\n            xticklabels=['Actual 1', 'Actual 0'],\n            yticklabels=['Predict 1', 'Predict 0'])\nplt.xlabel(\"Actual Label\")\nplt.ylabel(\"Predicted Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n\n\n\n\n\n\n\n\nThe model correctly predicted 59 as non‑renewal (TN) and incorrectly predicted as 113 renewal (FP). While it correctly predicted 155 as renewal (TP) and incorrectly predicted 69 as non‑renewal (FN).\n\n\nWhat is your model’s accuracy rate?\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy Rate: {accuracy:.3f}\")\n\nAccuracy Rate: 0.540\n\n\n\n\nWhat is your model’s sensitivity rate?\n\nsensitivity = recall_score(y_test, y_pred)\nprint(f\"Sensitivity Rate: {sensitivity:.3f}\")\n\nSensitivity Rate: 0.692\n\n\n\n\nWhat is your model’s specificity rate?\n\nspecificity = tn / (tn + fp)\nprint(f\"Specificity Rate: {specificity:.3f}\")\n\nSpecificity Rate: 0.343\n\n\n\n\nWhat is your model’s precision?\n\nprecision = precision_score(y_test, y_pred)\nprint(f\"Precision: {precision:.3f}\")\n\nPrecision: 0.578\n\n\n\n\nWhat is your model’s balanced accuracy?\n\nbalanced_accuracy = balanced_accuracy_score(y_test, y_pred)\nprint(f\"Balanced Accuracy: {balanced_accuracy:.3f}\")\n\nBalanced Accuracy: 0.517\n\n\nThe model shows good sensitivity (0.692), meaning it captures most renewals, but low specificity (0.343), missing many non-renewals. With overall accuracy at 51.7%, which is slightly above baseline, suggesting that it leans toward predicting renewals and struggles to distinguish non-renewals effectively.\n\n\nWhat is the naive accuracy rate (baseline rate) for this dataset?\n\nbaseline_rate = max(y_test.mean(), 1 - y_test.mean())\nprint(f\"Naive Accuracy Rate: {baseline_rate:.3f}\")\n\nNaive Accuracy Rate: 0.566\n\n\n\n\nHow does this compare with your model’s accuracy rate? If a classification model’s accuracy is not much better than the naive rate, can the model still be useful? If so, how?\nMy model achieved an accuracy of 51.7%, which is slightly lower than the naive baseline rate of 56.6%, representing the proportion of the majority class. Although it didn’t outperform a simple majority prediction, accuracy alone doesn’t capture the model’s full value. The model can still be useful if it improves metrics like sensitivity or precision, especially when correctly identifying renewals would help with targeted marketing and better allocation.\n\n\nCompare your model’s accuracy against the training set vs. accuracy against the test set.\n\n# Accuracy score for trained dataset\ntrain_pred = logmodel.predict(X_train_scaled)\ntrain_acc = accuracy_score(y_train, train_pred)\n\n# Accuracy score for tested dataset\ntest_pred = logmodel.predict(X_test_scaled)\ntest_acc = accuracy_score(y_test, test_pred)\nprint(f\"Accuracy score for training set: {train_acc:.3f}\")\nprint(f\"Accuracy score for test set: {test_acc:.3f}\")\n\nAccuracy score for training set: 0.620\nAccuracy score for test set: 0.540\n\n\n\n\nWhat is the purpose of comparing those two values?\nComparing the training and test accuracy helps determine how well the model performs on new data. When there is a large difference between the two values, it often means the model is overfitting, whereas the model is doing well on the training data but struggling to generalize. A smaller difference usually means the model is more consistent and better at handling unseen data.\n\n\nIn this case, what does the comparison of those values suggest about the model that you have built?\nIn this case, the training accuracy (62.0%) is only slightly higher than the test accuracy (54.0%), so overfitting is not the main concern. However, both accuracy rates are relatively low and close to the naive baseline, which could indicate that the model is underfitting or that the chosen variables don’t provide enough predictive evidence. Additional data or more informative features might be needed to increase the model’s efficiency.\n\n\nMake up a passholder. Assign this customer a value for each predictor variable in this model, and store the results in a new dataframe.\n\n# Sample passholder A who's from NY and preferred thrill attraction\npassholder_A = pd.DataFrame([{\n    'Home_State_ME': 0,\n    'Home_State_NH': 0,\n    'Home_State_NJ': 0,\n    'Home_State_NY': 1,\n    'Home_State_VT': 0,\n    'Preferred_Attraction_Other': 0,\n    'Preferred_Attraction_Thrill': 1\n}])\n\n# Scale using previously fitted scaler\npassholder_A_scaled = scaler.transform(passholder_A)\n\n# Predict renewal probability\npassholder_A_prob = logmodel.predict_proba(passholder_A_scaled)[0][1]\npassholder_A_class = logmodel.predict(passholder_A_scaled)[0]\n\nprint(f\"Predicted Class for Passholder A: {passholder_A_class}\")\nprint(f\"Predicted Renewal Probability for Passholder A: {passholder_A_prob:.3f}\")\n\nPredicted Class for Passholder A: 1\nPredicted Renewal Probability for Passholder A: 0.519\n\n\n\n\nWhat did your model predict – will this passholder renew?\nTo test the model, I created Passholder A, who is from New York (NY) and prefers Thrill Attractions. Based on the model’s prediction, Passholder A falls into class 1, meaning they are expected to renew their pass at Lobster Land.\n\n\nAccording to your model, what is the probability that the passholder will renew?\nAccording to the model, the predicted probability of renewal for Passholder A is 51.9%, indicating that while there is some likelihood of non-renewal, it is above the 50% threshold used for classifying renewers.\n\n\nWhen using a logistic regression model to make predictions, why is it important to only use values within the range of the dataset used to build the model?\n\n\nMake a new dataframe, but this time, for the numeric predictor variables, select some numbers that are outside the range of the dataset. Use your model to make a prediction for this new dataframe.\n\n# Create unrealistic but structurally valid input for passholder Z\npassholder_Z = pd.DataFrame([{\n    'Home_State_ME': 0,\n    'Home_State_NH': 0,\n    'Home_State_NJ': 0,\n    'Home_State_NY': 1,\n    'Home_State_VT': 0,\n    'Preferred_Attraction_Other': 0,\n    'Preferred_Attraction_Thrill': 999\n}])\n\npassholder_Z_scaled = scaler.transform(passholder_Z)\npassholder_Z_prob = logmodel.predict_proba(passholder_Z_scaled)[0][1]\npassholder_Z_class = logmodel.predict(passholder_Z_scaled)[0]\nprint(f\"Predicted Class for Passholder Z: {passholder_Z_class}\")\nprint(f\"Predicted Probability for Passholder Z: {passholder_Z_prob:.3f}\")\n\nPredicted Class for Passholder Z: 0\nPredicted Probability for Passholder Z: 0.000\n\n\n\n\nWhat do you notice about the result?\nTo test the model, I created Passholder Z from New York (NY) who prefers Thrill Attractions, but assigned an unrealistic value of 999 for Thrill preference, far beyond the training range. Logistic regression handles extreme values by pushing the log-odds sharply, which makes the predicted probability collapse to 0 or 1. As a result, the model predicted a renewal probability of 0.000 and classified this passholder as class 0, demonstrating why input values need to remain within realistic ranges for reliable predictions."
  },
  {
    "objectID": "python_notebooks/classification.html#part-ii-random-forest-model",
    "href": "python_notebooks/classification.html#part-ii-random-forest-model",
    "title": "Classification",
    "section": "Part II: Random Forest Model",
    "text": "Part II: Random Forest Model\n\nRead the dataset back into Python\n\npassholder_rf = pd.read_csv(\"data/lobsterland_passholders_dataset_25.csv\")\n\n\n# Convert the variables to the categorical data type\npassholder_rf[['Home_State', 'Preferred_Attraction', 'Referral_Source', 'Dining_Plan']] = passholder_rf[['Home_State', 'Preferred_Attraction', 'Referral_Source', 'Dining_Plan']].astype('category')\n# Add category 'None' to 'Dining_Plan' variable then fill any NaN values with 'None'\npassholder_rf['Dining_Plan'] = passholder_rf['Dining_Plan'].cat.add_categories(['None']).fillna('None')\n# Drop the negative Total_Spend_2024 value by filtering it to be greater than 0\npassholder_rf = passholder_rf[passholder_rf['Total_Spend_2024'] &gt;= 0]\n# Cap Email_Engagement_Score at 100 for those exceeding the 100 range\npassholder_rf['Email_Engagement_Score'] = passholder_rf['Email_Engagement_Score'].clip(upper=100)\n# Drop the negative Email_Engagement_Score value by filtering it to be greater than 0\npassholder_rf = passholder_rf[passholder_rf['Email_Engagement_Score'] &gt;= 0]\n\n\n\nDummify the categorical inputs again, but this time, don’t drop any levels.\n\n# Dummify all categorical variables and no drop\npassholder_rf = pd.get_dummies(passholder_rf, columns=[\n    'Home_State',\n    'Preferred_Attraction',\n    'Referral_Source',\n    'Dining_Plan'\n], drop_first=False)\n\npassholder_rf.head()\n\n\n\n\n\n\n\n\nAge\nPrevious_Visits\nTotal_Spend_2024\nFeedback_Score\nGold_Zone_Visits\nEmail_Engagement_Score\nDistance_From_Park_Miles\nRenewed_Pass\nHome_State_MA\nHome_State_ME\n...\nHome_State_NY\nHome_State_VT\nPreferred_Attraction_Entertainment\nPreferred_Attraction_Other\nPreferred_Attraction_Thrill\nReferral_Source_Ad/Other\nReferral_Source_Friend\nReferral_Source_Social Media\nDining_Plan_Upgraded\nDining_Plan_None\n\n\n\n\n0\n59.000000\n3.000000\n394.780000\n4.606232\n4.000000\n44.200000\n1.900000\n0\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nTrue\nFalse\n\n\n1\n27.846318\n1.064843\n172.541334\n2.102264\n2.000819\n37.103840\n21.709631\n1\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\n\n\n2\n18.000000\n5.926144\n151.731274\n3.365074\n1.019084\n61.827722\n21.892411\n1\nFalse\nFalse\n...\nFalse\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nTrue\nFalse\n\n\n3\n25.000000\n12.000000\n251.300000\n3.458663\n1.000000\n41.800000\n6.600000\n0\nFalse\nFalse\n...\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n4\n66.206178\n4.038141\n197.686695\n2.985864\n3.955755\n61.174332\n29.981959\n1\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n\n\n5 rows × 22 columns\n\n\n\n\n\nRe-partition the data, using the same seed value that you used in the previous part of this assignment.\n\ntrain_df, test_df = train_test_split(passholder_rf, test_size=0.4, random_state=156)\nprint(f\"Training set size: {len(train_df)} rows\")\nprint(f\"Test set size: {len(test_df)} rows\")\n\nTraining set size: 592 rows\nTest set size: 396 rows\n\n\n\n\nBuild a random forest model in Python with your training set (the only difference here is that the categories should not have any levels dropped). Use GridSearchCV to help you determine the best hyperparameter settings for your model.\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\nX_rf = passholder_rf.drop('Renewed_Pass', axis=1)\ny_rf = passholder_rf['Renewed_Pass']\n\nX_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(\n    X_rf, y_rf, test_size=0.4, random_state=156\n)\n\n\n# Set up a parameter grid for hyperparameter tuning.\nparam_grid = {\n    'n_estimators': [50, 100, 150],\n    'max_depth': [None, 5, 10],\n    'min_samples_split': [2, 5],\n    'min_samples_leaf': [1, 2]}\n\n# Instantiate the Random Forest classifier with a fixed random state.\nrf = RandomForestClassifier(random_state=156)\n\n# Use GridSearchCV for hyperparameter tuning with 5-fold cross-validation.\ngridsearch_rf = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1)\ngridsearch_rf.fit(X_train_rf, y_train_rf)\nprint(\"Best parameters found:\", gridsearch_rf.best_params_)\nbestparam_rf = gridsearch_rf.best_estimator_\n\nFitting 5 folds for each of 36 candidates, totalling 180 fits\nBest parameters found: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 50}\n\n\n\n\nHow did your random forest model rank the variables in order of importance, from highest to lowest?\n\n# Extract feature importances from the best random forest model\nimportances_rf = bestparam_rf.feature_importances_\nfeatures_rf = X_train_rf.columns\n\nimportance_df = pd.DataFrame({'Feature': features_rf, 'Importance': importances_rf})\nimportance_df = importance_df.sort_values(by='Importance', ascending=False)\nprint(\"Random Forest feature importances ranked from highest to lowest:\")\nimportance_df\n\nRandom Forest feature importances ranked from highest to lowest:\n\n\n\n\n\n\n\n\n\nFeature\nImportance\n\n\n\n\n6\nDistance_From_Park_Miles\n0.134086\n\n\n3\nFeedback_Score\n0.126785\n\n\n0\nAge\n0.123248\n\n\n5\nEmail_Engagement_Score\n0.122290\n\n\n2\nTotal_Spend_2024\n0.116996\n\n\n1\nPrevious_Visits\n0.099605\n\n\n4\nGold_Zone_Visits\n0.096552\n\n\n10\nHome_State_NJ\n0.016589\n\n\n14\nPreferred_Attraction_Other\n0.015116\n\n\n12\nHome_State_VT\n0.015014\n\n\n7\nHome_State_MA\n0.014658\n\n\n16\nReferral_Source_Ad/Other\n0.014174\n\n\n13\nPreferred_Attraction_Entertainment\n0.013437\n\n\n15\nPreferred_Attraction_Thrill\n0.013159\n\n\n9\nHome_State_NH\n0.013115\n\n\n18\nReferral_Source_Social Media\n0.012260\n\n\n8\nHome_State_ME\n0.011718\n\n\n19\nDining_Plan_Upgraded\n0.010869\n\n\n17\nReferral_Source_Friend\n0.010784\n\n\n20\nDining_Plan_None\n0.009866\n\n\n11\nHome_State_NY\n0.009680\n\n\n\n\n\n\n\n\n\nFor a random forest model, how can you interpret feature importance?\nThe random forest model ranked variables by how much they reduced classification error across all trees. The top predictors were Distance_From_Park_Miles (0.134), Feedback_Score (0.127), Age (0.123), Email_Engagement_Score (0.122), and Total_Spend_2024 (0.117), while categorical factors like Home_State and Preferred_Attraction had much lower importance (around 0.01–0.02).\nUnlike logistic regression, which shows the direction of relationships, random forest feature importance only measures how strongly each variable contributes to predictions. For Lobster Land, this means focusing on key behavioral drivers such as engagement and spending will likely have the greatest impact on improving renewal rates, while factors such as Home_State or Preferred_Attraction appear to play a much smaller role.\n\n\nAssess the performance of your model against the test set using confusion matrix to do this.\n\n# Predict outcomes for the Random Forest test set using the best Random Forest model\ny_pred_rf = bestparam_rf.predict(X_test_rf)\n\npassholder_cm_rf = confusion_matrix(y_test_rf, y_pred_rf)\ntn, fp, fn, tp = confusion_matrix(y_test_rf, y_pred_rf).ravel()\nprint(\"The model's confusion matrix:\\n\", passholder_cm_rf)\n\nThe model's confusion matrix:\n [[ 55 117]\n [ 62 162]]\n\n\n\nplt.figure(figsize=(5, 4))\nsns.heatmap(passholder_cm_rf, annot=True, fmt='d', cmap='BuGn',\n            xticklabels=['Actual 1', 'Actual 0'],\n            yticklabels=['Predict 1', 'Predict 0'])\nplt.xlabel(\"Actual Label\")\nplt.ylabel(\"Predicted Label\")\nplt.title(\"Random Forest Confusion Matrix\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nWhat is your model’ s accuracy rate?\n\naccuracy_rf = accuracy_score(y_test_rf, y_pred_rf)\nprint(f\"Random Forest Accuracy Rate: {accuracy_rf:.3f}\")\n\nRandom Forest Accuracy Rate: 0.548\n\n\n\n\nWhat is your model’s sensitivity rate?\n\nsensitivity_rf = recall_score(y_test_rf, y_pred_rf)\nprint(f\"Random Forest Sensitivity Rate: {sensitivity_rf:.3f}\")\n\nRandom Forest Sensitivity Rate: 0.723\n\n\n\n\nWhat is your model’s specificity rate?\n\nspecificity_rf = tn / (tn + fp)\nprint(f\"Random Forest Specificity Rate: {specificity_rf:.3f}\")\n\nRandom Forest Specificity Rate: 0.320\n\n\n\n\nWhat is your model’s precision?\n\nprecision_rf = precision_score(y_test_rf, y_pred_rf)\nprint(f\"Random Forest Precision: {precision_rf:.3f}\")\n\nRandom Forest Precision: 0.581\n\n\n\n\nWhat is your model’s balanced accuracy?\n\nbalanced_accuracy_rf = balanced_accuracy_score(y_test_rf, y_pred_rf)\nprint(f\"Random Forest Balanced Accuracy: {balanced_accuracy_rf:.3f}\")\n\nRandom Forest Balanced Accuracy: 0.521\n\n\n\n\nCompare your model’s accuracy against the training set vs. your model’s accuracy against the test set.\n\n# Accuracy score for RF trained dataset\ntrain_pred_rf = bestparam_rf.predict(X_train_rf)\ntrain_acc_rf = accuracy_score(y_train_rf, train_pred)\n\n# Accuracy score for RF tested dataset\ntest_pred_rf = bestparam_rf.predict(X_test_rf)\ntest_acc_rf = accuracy_score(y_test_rf, test_pred)\n\nprint(f\"Accuracy score for Random Forest training set: {train_acc_rf:.3f}\")\nprint(f\"Accuracy score for Random Forest test set: {test_acc_rf:.3f}\")\n\nAccuracy score for Random Forest training set: 0.610\nAccuracy score for Random Forest test set: 0.543\n\n\n\n\nHow different were these results?\nThe random forest model performed slightly better on the training data (61.0%) than on the test data (54.3%), showing a modest gap of about 6.7 percentage points. The small gap suggests the Random Forest isn’t overfitting but may have limited generalization, likely because the features don’t fully explain renewal behavior or key factors are missing from the data.\n\n\nUse the predict() function with your model to classify the person who you invented in the previous section.\n\n# Start with a template row of zeros for then update with selected features\npassholder_A = pd.DataFrame([dict.fromkeys(X_train_rf.columns, 0)])\npassholder_A['Home_State_NY'] = 1\npassholder_A['Preferred_Attraction_Thrill'] = 1\n\npred_class = bestparam_rf.predict(passholder_A)[0]\npred_prob = bestparam_rf.predict_proba(passholder_A)[0][1]\nprint(f\"Predicted Class for Passholder A (Random Forest): {pred_class}\")\nprint(f\"Predicted Renewal Probability (Random Forest): {pred_prob:.3f}\")\n\nPredicted Class for Passholder A (Random Forest): 1\nPredicted Renewal Probability (Random Forest): 0.504\n\n\n\n\nDoes the model think that the passholder will renew?\nThe Random Forest model predicts that Passholder A will renew (class 1) with a renewal probability of 50.4%, though this suggests a moderate likelihood of renewing their pass.\nLobster Land can use these models as practical tools to identify which passholders are more or less likely to renew, focusing on the variables that stood out in both analyses. The logistic regression pointed to factors like Home_State and Preferred_Attraction types, suggesting opportunities for local/geographical or interest-based marketing. The random forest, on the other hand, added further insights by focusing on behavioral and engagement factors: Distance_From_Park_Miles, Feedback_Score, Total_Spending_2024, and Previous_Visits as key drivers of renewal.\nBy combining these insights, Lobster Land can target passholders who live farther away, show lower satisfaction, or engage less often, while rewarding loyal visitors. For instance, the park could offer transportation discounts, personalized follow-ups, or loyalty perks to strengthen renewal intent and use its marketing resources more effectively."
  },
  {
    "objectID": "python_notebooks/classification.html#part-iii-tableau-dashboard",
    "href": "python_notebooks/classification.html#part-iii-tableau-dashboard",
    "title": "Classification",
    "section": "Part III: Tableau Dashboard",
    "text": "Part III: Tableau Dashboard\nHere’s the public link for my dashboard: https://public.tableau.com/app/profile/nhat.tran5503/viz/LobsterLandKPIDashBoard2025/Dashboard1?publish=yes\n\n\n\nimage.png\n\n\nThe Lobster Land KPI Dashboard reveals patterns and trends that help management understand park performance and make informed decisions. It shows which days see the most visitors, how weather impacts ticket sales, and which attractions and seasons generate the most revenue. By tracking these metrics, management can also see whether the park is meeting its KPIs, such as visitor targets, revenue goals, and event performance, and use the insights to adjust staffing, marketing, and resources as needed.\n\nAverage Visitors by Day of the Week (Line Chart): This chart shows how attendance varies across the week, with weekends (Friday–Saturday) drawing the highest visitor counts. It helps identify peak days for staffing and promotions.\nDay Tickets Sold by Weather Type (Box Plot): The box plots compare ticket sales under different weather conditions, revealing how sunny days show the highest ticket sales, while rainy or stormy conditions lead to fewer visitors, highlighting the strong influence of weather on attendance.\nTotal Revenue by Attraction Tiers (Pie Chart): The pie chart highlights how Tier 1 generate the largest share of revenue compared to Tier 2 and Tier 3. This emphasizes the importance of maintaining and marketing top-tier experiences.\nRevenue Breakdown by Season (Bar Chart): Seasonal bars show how revenue streams (food, merchandise, events) shift across seasons. Summer produces the strongest revenue across categories, especially in food and merchandise, reflecting peak tourism and vacation travel.This helps Lobster Land plan seasonal offerings and optimize resource allocation.\nInternational Visitors and Merch Revenue During Special Events (Combination of Bar + Line Chart): This chart compares per‑capita spending and merchandise revenue across major events, showing which events drive the most international engagement and sales. It helps prioritize which events to expand or replicate."
  },
  {
    "objectID": "python_notebooks/statistical_testing.html",
    "href": "python_notebooks/statistical_testing.html",
    "title": "Statistical Testing",
    "section": "",
    "text": "Conducted A/B testing and statistical analysis to evaluate how different app versions impact user spending and retention using Python. Applied hypothesis testing (ANOVA, binomial z-tests, chi-square) to identify significant differences in long-term engagement and validate data-driven product recommendations."
  },
  {
    "objectID": "python_notebooks/statistical_testing.html#part-i-ab-testing-for-user-spending",
    "href": "python_notebooks/statistical_testing.html#part-i-ab-testing-for-user-spending",
    "title": "Statistical Testing",
    "section": "Part I: A/B Testing for User Spending",
    "text": "Part I: A/B Testing for User Spending\n\n# Import necessary functions to use\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom scipy.stats import chisquare\n\n\nlobster_run = pd.read_csv(\"data/lobster_run.csv\")\n\n\nlobster_run.head()\n\n\n\n\n\n\n\n\nuserid\nversion\nsum_gamerounds\nretention_1\nretention_7\nuser_spend\n\n\n\n\n0\n1\nGulf of Maine\n3\nFalse\nFalse\n17.09\n\n\n1\n2\nGulf of Maine\n38\nTrue\nFalse\n15.30\n\n\n2\n7\nGulf of Maine\n0\nFalse\nFalse\n13.67\n\n\n3\n12\nGulf of Maine\n0\nFalse\nFalse\n16.92\n\n\n4\n14\nGulf of Maine\n39\nTrue\nFalse\n8.42\n\n\n\n\n\n\n\n\nlobster_run.isnull().sum()\n\nuserid            0\nversion           0\nsum_gamerounds    0\nretention_1       0\nretention_7       0\nuser_spend        0\ndtype: int64\n\n\n\nlobster_run.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 90189 entries, 0 to 90188\nData columns (total 6 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   userid          90189 non-null  int64  \n 1   version         90189 non-null  object \n 2   sum_gamerounds  90189 non-null  int64  \n 3   retention_1     90189 non-null  bool   \n 4   retention_7     90189 non-null  bool   \n 5   user_spend      90189 non-null  float64\ndtypes: bool(2), float64(1), int64(2), object(1)\nmemory usage: 2.9+ MB\n\n\n\n# Change datatype of version in the dataset to categorical variable\nlobster_run[['version']] = lobster_run[['version']].astype('category')\nlobster_run.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 90189 entries, 0 to 90188\nData columns (total 6 columns):\n #   Column          Non-Null Count  Dtype   \n---  ------          --------------  -----   \n 0   userid          90189 non-null  int64   \n 1   version         90189 non-null  category\n 2   sum_gamerounds  90189 non-null  int64   \n 3   retention_1     90189 non-null  bool    \n 4   retention_7     90189 non-null  bool    \n 5   user_spend      90189 non-null  float64 \ndtypes: bool(2), category(1), float64(1), int64(2)\nmemory usage: 2.3 MB\n\n\n\nlobster_run.describe()\n\n\n\n\n\n\n\n\nuserid\nsum_gamerounds\nuser_spend\n\n\n\n\ncount\n90189.000000\n90189.000000\n90189.000000\n\n\nmean\n45095.000000\n51.872457\n15.015899\n\n\nstd\n26035.466051\n195.050858\n2.747268\n\n\nmin\n1.000000\n0.000000\n4.050000\n\n\n25%\n22548.000000\n5.000000\n13.170000\n\n\n50%\n45095.000000\n16.000000\n15.010000\n\n\n75%\n67642.000000\n51.000000\n16.870000\n\n\nmax\n90189.000000\n49854.000000\n28.000000\n\n\n\n\n\n\n\n\n# Show the row with the maximum sum_gamerounds\nmax_rounds = lobster_run.loc[lobster_run['sum_gamerounds'].idxmax()]\nprint(max_rounds)\n\nuserid                    57703\nversion           Gulf of Maine\nsum_gamerounds            49854\nretention_1               False\nretention_7               False\nuser_spend                13.36\nName: 28650, dtype: object\n\n\n\n# Filter rows where sum_gamerounds &gt; 2000\nhigh_rounds = lobster_run[lobster_run['sum_gamerounds'] &gt; 2000]\nprint(high_rounds)\n\n       userid         version  sum_gamerounds  retention_1  retention_7  \\\n3927     7913   Gulf of Maine            2961         True        False   \n21685   43672   Gulf of Maine            2438         True        False   \n23037   46345   Gulf of Maine            2251         True        False   \n28650   57703   Gulf of Maine           49854        False        False   \n43118   87008   Gulf of Maine            2156         True        False   \n47967    6537  North Atlantic            2015         True         True   \n59444   29418  North Atlantic            2640         True        False   \n63193   36934  North Atlantic            2124         True         True   \n68948   48189  North Atlantic            2294         True         True   \n89253   88329  North Atlantic            2063         True         True   \n\n       user_spend  \n3927        11.28  \n21685       12.57  \n23037       18.00  \n28650       13.36  \n43118       17.68  \n47967       10.41  \n59444       15.47  \n63193       15.37  \n68948       16.28  \n89253       18.10  \n\n\n\n# Filter rows where sum_gamerounds &gt; 3000\nhigh_rounds = lobster_run[lobster_run['sum_gamerounds'] &gt; 3000]\nprint(high_rounds)\n\n       userid        version  sum_gamerounds  retention_1  retention_7  \\\n28650   57703  Gulf of Maine           49854        False        False   \n\n       user_spend  \n28650       13.36  \n\n\n\nmax_spend = lobster_run.loc[lobster_run['user_spend'].idxmax()]\nprint(max_spend)\n\nuserid                     35202\nversion           North Atlantic\nsum_gamerounds                42\nretention_1                False\nretention_7                False\nuser_spend                  28.0\nName: 62331, dtype: object\n\n\nAfter running some basic checks on the dataset, I noticed an extreme outlier in sum_gamerounds - a value of 49,854 for userid = 57703, which is the only record exceeding 3,000 rounds. What’s interesting is that this user’s user_spend is actually lower than the average, and they didn’t return to the app after one day (retention_1) or seven days (retention_7) following the download. This would mean the user supposedly completed nearly 50,000 game rounds in a single day, which is highly unrealistic. It’s likely due to a data error or possibly a case of hacking. Because this outlier is so extreme compared to other high-ranking users and this is a large dataset (90189 entries), I decided to remove it to avoid distorting the analysis.\n\nlobster_run = lobster_run[lobster_run['sum_gamerounds'] &lt;= 3000]\n\n\nGenerate a histogram to view the user_spend variable.\n\nplt.figure(figsize=(10, 6))\nplt.hist(lobster_run['user_spend'], bins=50, color='skyblue', edgecolor='black')\nplt.title('Distribution of User Spend')\nplt.xlabel('User Spend (USD)')\nplt.ylabel('Number of Users')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe Distribution of User Spend histogram shows a clear bell-shaped pattern centered around $15, indicating that most users spend roughly this amount in the app. The distribution is fairly symmetric, with fewer users spending much less or more - ranging from less than \\$5 to more than \\$25, suggesting that spending behavior is consistent across the player base. Understanding this distribution can help the Lobster Land team design pricing strategies and in-app offers that align with typical user spending habits.\n\n\nGenerate another histogram that depicts user_spend, but this time, use version as a color or hue variable.\n\n# Plot histogram with hue by version\nplt.figure(figsize=(10, 6))\nsns.histplot(data=lobster_run, x='user_spend', hue='version', bins=50, palette='Set2', multiple='stack', stat='count', edgecolor='black')\nplt.title('User Spend Distribution by Version')\nplt.xlabel('User Spend (USD)')\nplt.ylabel('Number of Users')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe User Spend Distribution by Version histogram shows the distribution of user spend by Gulf of Maine vs. North Atlantic version, with both versions following a similar bell-shaped pattern centered around $15. However, users of the Gulf of Maine version appear to spend slightly more on average than those using the North Atlantic version. This suggests that the Gulf of Maine version may encourage higher in-app spending or greater engagement among players. The overall similarity shape between the two distributions indicates that spending behavior is consistent across versions, with only modest differences in average spend.\n\n\nConduct a statistical test to check whether the user_spend variable is normally distributed.\n\n# Run statistical test to check if user_spend is normally distributed\nuspend_result = stats.normaltest(lobster_run['user_spend'])\n\nprint(f\"Statistic = {uspend_result.statistic:.4f}\")\nprint(f\"p-value   = {uspend_result.pvalue:.4f}\")\n\nStatistic = 0.2353\np-value   = 0.8890\n\n\nThe test’s null hypothesis:  H0: The user_spend variable is normally distributed.  H1: The user_spend variable is not normally distributed. \nThis hypothesis tests whether the observed distribution of user_spend significantly deviates from a normal distribution, which is important for determining if standard statistical methods can be appropriately applied. Under H0, any deviation from normality is due to random variation, implying the data follow a normal pattern. Under H1, the deviations are systematic, indicating that user spend is not normally distributed.\n\nalpha = 0.05\nif uspend_result.pvalue &gt; alpha:\n    print(\"Fail to reject H0: user_spend looks normally distributed.\")\nelse:\n    print(\"Reject H0: user_spend is not normally distributed.\")\n\nFail to reject H0: user_spend looks normally distributed.\n\n\nSince the p-value = 0.8890, much greater than the common significance level of 0.05 - the test fail to reject the null hypothesis. This indicates that there is no strong statistical evidence suggesting the Lobster Land’s user spending deviates from normality.\n\n\nNext, conduct an appropriate statistical test to check whether version has a significant impact on user spending. You can use any alpha value that you prefer to use for the significance threshold.\n\n# Extract places from version where player begins\ngulf_of_maine_spend = lobster_run[lobster_run['version'] == 'Gulf of Maine']['user_spend']\nnorth_atlantic_spend = lobster_run[lobster_run['version'] == 'North Atlantic']['user_spend']\n\n# Perform one-way Analysis of Variance (ANOVA) to compare user_spend across 2 versions\nversion_anova = stats.f_oneway(gulf_of_maine_spend, north_atlantic_spend)\nprint(f\"Statistic = {version_anova.statistic:.4f}\")\nprint(f\"p-value   = {version_anova.pvalue:.4f}\")\n\nStatistic = 2.5264\np-value   = 0.1120\n\n\nThe test’s null hypothesis:  H0: The mean user_spend is equal between Gulf of Maine and North Atlantic versions.  H1: The mean user_spend is different between the two versions.  In this analysis, I tested whether the product version (Gulf of Maine vs. North Atlantic) had a statistically significant effect on user spending. The null hypothesis (H0) assumed that the average user_spend was the same for both versions.\n\nalpha = 0.05\nif version_anova.pvalue &gt; 0.05:\n    print(\"Fail to reject H0: No significant difference in user_spend between versions.\")\nelse:\n    print(\"Reject H0: Version has a significant impact on user_spend.\")\n\nFail to reject H0: No significant difference in user_spend between versions.\n\n\nThe ANOVA is chosen in this case because it allows me to compare the means of the user_spend variable across two product versions (Gulf of Maine and North Atlantic) to see if any observed differences are statistically significant. The test produced an F-statistic of 2.5264 and a p-value of 0.1120. Since the p-value is greater than the 0.05 significance level, I failed to reject the null hypothesis. This suggests that there isn’t a meaningful difference in average user spending between the two product versions, and any variation observed is likely due to random chance rather than an actual version effect."
  },
  {
    "objectID": "python_notebooks/statistical_testing.html#part-ii-ab-testing-two-app-versions",
    "href": "python_notebooks/statistical_testing.html#part-ii-ab-testing-two-app-versions",
    "title": "Statistical Testing",
    "section": "Part II: A/B Testing Two App Versions",
    "text": "Part II: A/B Testing Two App Versions\n\nGenerate some summary stats here – group the observations by ’version’and then compare the retention_1 and the retention_7 outcomes.\n\nretention_summary = lobster_run.groupby('version',observed=True)[['retention_1', 'retention_7']].mean().round(6)\nprint(retention_summary)\n\n                retention_1  retention_7\nversion                                 \nGulf of Maine      0.448198      0.19562\nNorth Atlantic     0.442283      0.18200\n\n\nFrom the results, the average retention rates for both Day 1 and Day 7 are slightly higher in the Gulf of Maine version compared to the North Atlantic version. - Day 1 retention: Gulf of Maine (44.82%) vs. North Atlantic (44.23%) - Day 7 retention: Gulf of Maine (19.56%) vs. North Atlantic (18.20%) \nAlthough the differences are minimal, the fact that they appear at both time points hints at a subtle but consistent pattern. Users of the Gulf of Maine version might be finding something slightly more engaging or user-friendly, which helps keep them around a bit longer. While this isn’t enough to draw firm conclusions without a statistical test, the trend suggests there could be small design or content differences that make the Gulf of Maine version more appealing over time.\n\n\nUse a binomial z-test to compare retention_1 between the two versions of the game\n\nfrom statsmodels.stats.proportion import proportions_ztest\n# Count retained users per version\nretained_count_rt1 = lobster_run.groupby('version', observed=True)['retention_1'].sum()\nprint(\"Retained users (retention_1):\")\nprint(retained_count_rt1)\n\n# Count total users per version\nuser_total_rt1 = lobster_run.groupby('version', observed=True)['retention_1'].count()\nprint(\"\\nTotal users per version (retention_1):\")\nprint(user_total_rt1)\n\nRetained users (retention_1):\nversion\nGulf of Maine     20034\nNorth Atlantic    20119\nName: retention_1, dtype: int64\n\nTotal users per version (retention_1):\nversion\nGulf of Maine     44699\nNorth Atlantic    45489\nName: retention_1, dtype: int64\n\n\nFor retention day one, Gulf of Maine retained 20,034 out of 44,699 users, while North Atlantic retained 20,119 out of 45,489 users, showing nearly identical retention performance across both versions. This suggests that initial engagement is consistent regardless of which version users started with.\n\n# Run binomial z-test to compare the 2 versions\nz_stat1, p_value1 = proportions_ztest(count=retained_count_rt1, nobs=user_total_rt1)\nprint(\"Binomial Z-Test Results (retention_1):\")\nprint(f\"Z-statistic = {z_stat1:.4f}\")\nprint(f\"p-value     = {p_value1:.4f}\")\n\nBinomial Z-Test Results (retention_1):\nZ-statistic = 1.7871\np-value     = 0.0739\n\n\n\nalpha = 0.05\nif p_value1 &gt; 0.05:\n    print(\"Fail to reject H0: No significant difference in day-1 retention between versions.\")\nelse:\n    print(\"Reject H0: Version has a significant impact on day-1 retention.\")\n\nFail to reject H0: No significant difference in day-1 retention between versions.\n\n\nThe test’s null hypothesis:  H0: Day-1 retention rates are equal between Gulf of Maine and North Atlantic versions.  H1: Day-1 retention rates are different between versions. \nThe results show a Z-statistic of 1.7871 and a p-value of 0.0739. Since this p-value is slightly above the 0.05 threshold, there isn’t enough evidence to say that the two versions differ in a statistically significant way. Though p-value is close enough to suggest a possible pattern. In other words, while the difference is not significant, it hints that the Gulf of Maine version might be performing a bit better, and Lobster Land should keep an eye on it for future marketing strategy.\n\n\nIn this case, I chose a slighly better performer which is ‘Gulf of Maine’.\n\ngom_version = 'Gulf of Maine'\n\n\n\nFind the number of users of your chosen version who stayed with the game for one day.\n\n# The total users in Gulf of Maine version\ngom_total = lobster_run[lobster_run['version'] == gom_version].shape[0]\nprint(f\"The total users in Gulf of Maine is: {gom_total}\")\n\n# Filter out the retained users for Gulf of Maine after day 1\ngom_retained1 = lobster_run[(lobster_run['version'] == gom_version) & \n    (lobster_run['retention_1'] == True)].shape[0]\n\nprint(f\"The actual retained users after Day 1 in Gulf of Maine is: {gom_retained1}\")\n\nThe total users in Gulf of Maine is: 44699\nThe actual retained users after Day 1 in Gulf of Maine is: 20034\n\n\n\n\nFind the expected number of users who would have stayed with the game for one day, if your null hypothesis were true.\n\n# Find the average retention_1 rate across all users of Lobster Land\navg_retention1 = lobster_run['retention_1'].mean()\n\n# The expected number of users from Gulf of Maine that stayed after one day if the hypothesis were true\nexpect_retained1 = gom_total * avg_retention1\n\n# Round number of users to the nearest integer\nexpect_retained1_round = round(expect_retained1)\nprint(f\"The expected retained users in Gulf of Maine after Day 1 under H0: {expect_retained1_round}\")\n\nThe expected retained users in Gulf of Maine after Day 1 under H0: 19901\n\n\nThe expected value shows how many users from the Gulf of Maine version I would expect to stay after Day 1 if the version had no effect on retention. To calculate this, I multiplied the total number of Gulf of Maine users by the overall day 1 retention rate across all users. I used the overall rate because, under the null hypothesis, I assume retention is independent of version. This gives me a baseline to compare against the actual retention in Gulf of Maine and helps me assess whether the observed difference is meaningful.\nAfter the calculation, the expected retained users in Gulf of Maine after day 1 is 19901 users. This version retained 20,034 users, compared to the expected 19,901 under the null hypothesis, meaning it held on to about 133 more users than anticipated. While this difference is small relative to the total user base, it suggests a slight positive deviation in retention performance for Gulf of Maine.\n\n\nConduct a binomial z-test to determine whether the retention_1 for the version you chose is meaningfully different from the expected number, given your null hypothesis.\n\n# Run binomial z-test\nz_stat, p_value = proportions_ztest(count=gom_retained1, nobs=gom_total, value=avg_retention1)\n\n# Step 6: Print results\nprint(\"Binomial Z-Test for Gulf of Maine retention_1 vs expected under H0:\")\nprint(f\"Z-statistic = {z_stat:.4f}\")\nprint(f\"p-value     = {p_value:.4f}\")\n\nBinomial Z-Test for Gulf of Maine retention_1 vs expected under H0:\nZ-statistic = 1.2684\np-value     = 0.2047\n\n\nBased on the binomial z-test, the day 1 retention rate for the Gulf of Maine version does not differ significantly from what would be expected under the null hypothesis. The p-value of 0.2047 is greater than the conventional 0.05 threshold, so I fail to reject the null hypothesis. This result indicates that, given the available data, there isn’t sufficient evidence to conclude that the Gulf of Maine version has an effect on day 1 retention relative to the overall average. Lobster Land could improve this analysis by gathering more data or testing targeted design changes to better understand what drives early user retention.\n\n\nUsing a binomial z-test, compare the retention_7 outcomes for the two app versions.\n\n# Count retained users per version by retention_7\nretained_count_rt7 = lobster_run.groupby('version', observed=True)['retention_7'].sum()\nprint(\"Retained users (retention_7):\")\nprint(retained_count_rt7)\n\n# Count total users per version by retention 7\nuser_total_rt7 = lobster_run.groupby('version', observed=True)['retention_7'].count()\nprint(\"\\nTotal users per version (retention_7):\")\nprint(user_total_rt7)\n\nRetained users (retention_7):\nversion\nGulf of Maine     8744\nNorth Atlantic    8279\nName: retention_7, dtype: int64\n\nTotal users per version (retention_7):\nversion\nGulf of Maine     44699\nNorth Atlantic    45489\nName: retention_7, dtype: int64\n\n\n\n# Run binomial z-test to compare the 2 versions\nz_stat7, p_value7 = proportions_ztest(count=retained_count_rt7, nobs=user_total_rt7)\nprint(\"Binomial Z-Test Results (retention_7):\")\nprint(f\"Z-statistic = {z_stat7:.4f}\")\nprint(f\"p-value     = {p_value7:.4f}\")\n\nBinomial Z-Test Results (retention_7):\nZ-statistic = 5.2260\np-value     = 0.0000\n\n\n\nalpha = 0.05\nif p_value7 &gt; 0.05:\n    print(\"Fail to reject H0: No significant difference in day-7 retention between versions.\")\nelse:\n    print(\"Reject H0: Version has a significant impact on day-7 retention.\")\n\nReject H0: Version has a significant impact on day-7 retention.\n\n\nThe test’s null hypothesis:  H0: Day-7 retention rates are equal between Gulf of Maine and North Atlantic versions.  H1: Day-7 retention rates are different between versions. \nI conducted a binomial z-test to compare day-7 retention between the Gulf of Maine and North Atlantic versions. The p-value was approximately 0.000, which is well below the standard significance level of 0.05. Therefore, I reject the null hypothesis and conclude that there is a statistically significant difference in day-7 retention between the two versions. This result suggests that users playing the Gulf of Maine version are more likely to remain active after 7 days compared to those playing the North Atlantic version.\n\n\nI still chose a slighly better performer which is ‘Gulf of Maine’.\n\ngom_version = 'Gulf of Maine'\n\n\n\nFind the number of users of your chosen version who stayed with the game for one day.\n\n# Filter out the retained users for Gulf of Maine after day 1\ngom_retained7 = lobster_run[(lobster_run['version'] == gom_version) & \n    (lobster_run['retention_7'] == True)].shape[0]\n\nprint(f\"The actual retained users after Day 7 in Gulf of Maine is: {gom_retained7}\")\n\nThe actual retained users after Day 7 in Gulf of Maine is: 8744\n\n\n\n\nFind the expected number of users who would have stayed with the game for one day, if your null hypothesis were true.\n\n# Find the average retention 7 rate across all users of Lobster Land\navg_retention7 = lobster_run['retention_7'].mean()\n\n# The expected number of users from Gulf of Maine that stayed after one day if the hypothesis were true\nexpect_retained7 = gom_total * avg_retention7\n# Round number of users to the nearest integer\nexpect_retained7_round = round(expect_retained7)\nprint(f\"The expected retained users in Gulf of Maine after Day 7 under H0: {expect_retained7_round}\")\n\nThe expected retained users in Gulf of Maine after Day 7 under H0: 8437\n\n\nThe expected value here refers to the number of Gulf of Maine users I would anticipate being retained under the null hypothesis (H0), which assumes that day‑7 retention in Gulf of Maine is the same as the overall average retention rate across all users.\nTo calculate this, I take the total number of Gulf of Maine users and multiply by the overall day‑7 retention rate (the pooled proportion across both versions). This gives an expected retained count of 8,437 users. In other words, if Gulf of Maine behaved exactly like the overall user base, Lobster Land would expect about 8,437 users to remain by day 7.\n\n\nConduct a binomial z-test to determine whether the retention_1 for the version you chose is meaningfully different from the expected number, given your null hypothesis.\n\n# Run binomial z-test\nz_stat, p_value = proportions_ztest(count=gom_retained7, nobs=gom_total, value=avg_retention7)\n\n# Step 6: Print results\nprint(\"Binomial Z-Test for Gulf of Maine retention_7 vs expected under H0:\")\nprint(f\"Z-statistic = {z_stat:.4f}\")\nprint(f\"p-value     = {p_value:.4f}\")\n\nBinomial Z-Test for Gulf of Maine retention_7 vs expected under H0:\nZ-statistic = 3.6613\np-value     = 0.0003\n\n\nThe binomial z-test for Gulf of Maine’s day‑7 retention produced a Z‑statistic of 3.6613 with a p‑value of 0.0003, well below the 0.05 threshold, suggesting retention is higher than expected under the null hypothesis. Gulf of Maine retained 20,034 users by day 1 and continued to outperform expectations by day 7, with 8,437 users retained compared to the expected count.\nThis suggests that while early retention looked similar across versions, Gulf of Maine is delivering a stronger long‑term engagement advantage. For Lobster Land, this is a clear signal that the Gulf of Maine version could be leveraged more aggressively in marketing and product strategy.\n\n\nRecommendations for Lobster Land based on the overall results that you have found from the Super Lobster Run data and the statistical tests.\nThe Super Lobster Run experiment shows that the starting level didn’t have a major impact on day‑1 retention or spending, with both Gulf of Maine and North Atlantic versions retaining roughly 20,034 users initially. Retention differences on day 1 were not statistically significant (Z = 1.79, p = 0.074), but by day‑7, the Gulf of Maine version retained 8,437 users, showing a clear and statistically significant advantage (Z = 5.23, p &lt; 0.05). This suggests that while early engagement was similar, Gulf of Maine may help maintain stronger long‑term retention.\nBased on these findings, Lobster Land could consider emphasizing the Gulf of Maine level marketing strategies to leverage its retention advantage. They might also experiment with transferring successful design elements from Gulf of Maine - such as level design, difficulty pacing, or visual - into North Atlantic or future levels. Additionally, offering limited-time promotion booster packages for $0.99 or applying seasonal/holiday theme could encourage spending and interaction, potentially increasing engagement across all versions. To better understand the results, it would be useful to monitor retention beyond day‑7, track whether higher retention leads to increased long-term spending, and gather qualitative feedback to learn what aspects of Gulf of Maine are driving engagement.\n\n\nPart III: Chi-Square Goodness of Fit\n\n# The data collected from the survey  Roller Coasters, Water Rides, Lobsterama Restaurant, Marine Life Exhibit, Lobsterland Merchandise Shop\nobserved = [150, 120, 90, 80, 60]\ntotal = sum(observed)\nprint(f\"The total of observed visitors from their favorite attraction at Lobster Land is: {total}.\")\n\nexpected = [0.30 * total, 0.25 * total, 0.15 * total, 0.15 * total, 0.15 * total]\n\nprint(\"\\nThe expected visitors at Lobster Land is:\")\nprint(f\"Roller Coasters: {expected[0]}\")\nprint(f\"Water Rides: {expected[1]}\")\nprint(f\"Lobsterama Restaurant: {expected[2]}\")\nprint(f\"Marine Life Exhibit: {expected[3]}\")\nprint(f\"Lobsterland Merchandise Shop: {expected[4]}\")\n\nThe total of observed visitors from their favorite attraction at Lobster Land is: 500.\n\nThe expected visitors at Lobster Land is:\nRoller Coasters: 150.0\nWater Rides: 125.0\nLobsterama Restaurant: 75.0\nMarine Life Exhibit: 75.0\nLobsterland Merchandise Shop: 75.0\n\n\n\nThe null hypothesis and the alternative hypothesis of this test:\nH0 - Null Hypothesis: The distribution of visitor preferences across the five attractions at Lobster Land matches the expected proportions provided by management: - 30% Roller Coasters - 25% Water Rides - 15% Lobsterama Restaurant - 15% Marine Life Exhibit - 15% Merchandise Shop\nH1 - Alternative hypothesis: The distribution of visitor preferences does not match the expected proportions.\nThe null hypothesis (H0) assumes that the distribution of visitor preferences across Lobster Land’s attractions: roller coasters, water rides, the seafood-themed food court, marine life exhibit, and lobster-themed merchandise shop which matches the proportions expected by the park’s management. Whereas, the alternative hypothesis (H1) suggests that visitor preferences differ enough from these expected proportions that the differences are unlikely to occur by chance, indicating that certain attractions may be more or less popular than anticipated, which could inform resource allocation and marketing strategies.\n\n\nUnder the null hypothesis, what is our expected number of respondents for roller coasters, water rides, Lobsterama Restaurant, Marine Life Exhibit, and the Lobsterland Merchandise Shop?\nSince a total of 500 survey responses were collected, I calculated the expected number of respondents for each attraction by applying the expected percentages to this total. Lobster Land expects 30% of visitors to prefer roller coasters, which equals 150 respondents. For water rides, the expected proportion is 25%, resulting in 125 expected respondents. The remaining three attractions, Lobsterama Restaurant, the Marine Life Exhibit, and the Lobsterland Merchandise Shop, are each expected to receive 15% of the votes, corresponding to 75 respondents per attraction.\n\n\nUsing both the expected values and the actual numbers that came from the survey respondents, run a chi-square goodness of fit test to assess your null hypothesis.\n\n# Conduct a chi-square goodness of fit\nchisq, pvalue = chisquare(f_obs=observed, f_exp=expected)\n\nprint(\"\\nThe chi-square value is: \", round(chisq, 4))\nprint(\"The p-value is: \", round(pvalue, 4))\n\n\nThe chi-square value is:  6.5333\nThe p-value is:  0.1627\n\n\n\nalpha = 0.05\nif pvalue &gt; alpha:\n    print(\"Fail to reject H0: The observed distribution does not differ significantly from the expected distribution.\")\nelse:\n    print(\"Reject H0: The observed distribution differ significantly from the expected distribution.\")\n\nFail to reject H0: The observed distribution does not differ significantly from the expected distribution.\n\n\nThe p-value of this chi-square goodness of fit test is 0.1627.\nSince this p-value = 0.1627 is greater than the common significance level of 0.05, I fail to reject the null hypothesis. This means there is no statistically significant difference between the observed distribution of visitor preferences and the expected distribution provided by Lobster Land management.\nIn other words, the survey results do not provide enough evidence to conclude that visitor preferences differ from what management anticipated. The observed variation is likely due to random chance rather than a meaningful shift in attraction popularity. To improve the reliability of this test, Lobster Land could collect a larger sample of visitor responses to reduce sampling error and increase statistical power. Additionally, the management team could refine the expected distribution by providing more recent attendance data or seasonal trends, ensuring that the comparison reflects current visitor behavior more accurately.\n\n\nDemonstrate where the chi-square number from your test came from. Use Jupyter Notebook or Colab to do this, but do not use any Python libraries or modules.\nBased on the results and datas that I retrieve from part A and B, once I have both the observed (O) and expected (E) counts, I am able to calculate the contribution of each cell to the overall chi-square statistic using this formula:\n\nThe calculations for each attraction at Lobster Land are as follows: - Roller Coasters: (150 − 150)^2/150 = 0.00 - Water Rides: (120 − 125)^2/125 = 0.20 - Lobsterama Restaurant: (90 − 75)^2/75 = 3.00 - Marine Life Exhibit: (80 − 75)^2/75 = 0.33 - Merchandise Shop: (60 − 75)^2/75 = 3.00\nFinally, I sum these five contributions to obtain the overall chi-square statistic:  X^2 = 0.00 + 0.20 + 3.00 + 0.33 + 3.00 = 6.53\nThe degrees of freedom in a chi‑square goodness‑of‑fit test are calculated as the number of categories minus one. Since there are 5 attraction categories at Lobster Land, the degrees of freedom are 5 − 1 = 4. With 4 degrees of freedom, the corresponding p‑value is approximately 0.1627. Since this p‑value is greater than the 0.05 threshold, I fail to reject the null hypothesis. In practical terms, this means the observed visitor preferences are not significantly different from the expected proportions, and the distribution of favorite attractions at Lobster Land is broadly consistent with the assumed model.\nThe results show that visitors’ attraction preferences are largely in line with what Lobster Land’s management expected, with only small differences in how often people chose the Lobsterama Restaurant and Merchandise Shop. The chi-square value of 6.53 and p-value of 0.1627 suggest that these variations aren’t statistically significant, or the overall pattern of visitor preferences are pretty consistent with the original expectations. Though, as Lobster Land expands its engagement through the “Super Lobster Run” game, it could benefit from collecting data from a larger and more diverse group of users throughout the year to identify any seasonal shifts in interest. Updating expected values to reflect new attractions or marketing efforts could also help the park fine-tune its offerings and digital strategy."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "I’m always happy to connect! You can reach me through any of the platforms below:\n\n💼 LinkedIn 💻 GitHub 📧 Email\n\n\nYou can also return to the About tab to learn more about me."
  },
  {
    "objectID": "contact.html#get-in-touch",
    "href": "contact.html#get-in-touch",
    "title": "Contact",
    "section": "",
    "text": "I’m always happy to connect! You can reach me through any of the platforms below:\n\n💼 LinkedIn 💻 GitHub 📧 Email\n\n\nYou can also return to the About tab to learn more about me."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m currently pursuing my Master’s in Business Analytics (MSBA) at Boston University, where I’m focused on applying data to understand markets and improve decision-making. Prior to this, I studied finance and economics at Drexel University, which sparked my interest in how numbers tell the story behind business trends. I’ve worked on projects involving predictive modeling, marketing analytics, NLP, and data visualization, which has strengthened my ability to connect technical analysis with practical, real-world applications."
  },
  {
    "objectID": "about.html#hi-im-nhat",
    "href": "about.html#hi-im-nhat",
    "title": "About",
    "section": "",
    "text": "I’m currently pursuing my Master’s in Business Analytics (MSBA) at Boston University, where I’m focused on applying data to understand markets and improve decision-making. Prior to this, I studied finance and economics at Drexel University, which sparked my interest in how numbers tell the story behind business trends. I’ve worked on projects involving predictive modeling, marketing analytics, NLP, and data visualization, which has strengthened my ability to connect technical analysis with practical, real-world applications."
  },
  {
    "objectID": "about.html#skills",
    "href": "about.html#skills",
    "title": "About",
    "section": "Skills",
    "text": "Skills\n\nProgramming Languages: Python, SQL, Eviews, R\nData Analysis Tools: Bloomberg, Power BI, Tableau, Macrobond, Haver, Microsoft Office (Word, Excel, PowerPoint)\n\nTechnical Skills: Data Visualizations, Data Manipulation, Database Management, Macro Research, ML, GenAI, Regression, XGBoost, Random Forest"
  },
  {
    "objectID": "about.html#what-im-interested-in",
    "href": "about.html#what-im-interested-in",
    "title": "About",
    "section": "What I’m Interested In",
    "text": "What I’m Interested In\nI’m passionate about turning raw data into actionable insights that solve real-world business problems, uncover market trends, and create tools that make complex information easy to understand. Outside of work, I love exploring the world through travel, photography, and trying new cuisines, often visiting local restaurants to discover hidden gems.\nA few moments I’ve captured along the way — places and details that shape how I see the world.\n\n  \n    Through My Lens\n  \n\n  \n\n    \n\n  \n    \n\n    \n      A small corner of Hanoi, where I grew up.\n    \n  \n\n\n    \n    \n      \n\n        \n        1\n\n        \n        2\n\n        \n        3\n\n        \n        4\n\n        \n        5"
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "About",
    "section": "Contact",
    "text": "Contact\nFor inquiries or collaboration, please visit Contact — happy to connect anytime :)"
  }
]