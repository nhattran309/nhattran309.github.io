[
  {
    "objectID": "python_notebooks/AD654_Nhat_Tran_Assigment_2.html",
    "href": "python_notebooks/AD654_Nhat_Tran_Assigment_2.html",
    "title": "Market Segmentation & Conjoint Analysis",
    "section": "",
    "text": "Analyzed Lobster Land’s operations using k-means clustering to segment park days based on attendance, pricing, weather, and spending behavior, uncovering distinct day types to inform staffing, pricing, and marketing decisions. Conducted a conjoint analysis with a linear regression model to quantify how entertainment features, crowding, and experience design impact guest satisfaction, translating insights into actionable recommendations for revenue and experience optimization.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans"
  },
  {
    "objectID": "python_notebooks/AD654_Nhat_Tran_Assigment_2.html#part-i-segmentation",
    "href": "python_notebooks/AD654_Nhat_Tran_Assigment_2.html#part-i-segmentation",
    "title": "Market Segmentation & Conjoint Analysis",
    "section": "Part I: Segmentation",
    "text": "Part I: Segmentation\n\n# Import day_25.csv files into python for use\nday_25 = pd.read_csv(\"data/days25.csv\")\n\n\n# Then use info() and head() function to show how the data is stored\nday_25.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 99 entries, 0 to 98\nData columns (total 19 columns):\n #   Column                  Non-Null Count  Dtype  \n---  ------                  --------------  -----  \n 0   Day_ID                  99 non-null     int64  \n 1   Day_of_Week             99 non-null     object \n 2   season_week_rel         99 non-null     int64  \n 3   Is_Holiday              99 non-null     int64  \n 4   Total_Visitors          99 non-null     int64  \n 5   Passholder_Percentage   99 non-null     float64\n 6   Day_Tickets_Sold        99 non-null     int64  \n 7   Avg_Ticket_Price        99 non-null     float64\n 8   Gate_Revenue            99 non-null     float64\n 9   Revenue_Food            99 non-null     float64\n 10  Revenue_Merch           99 non-null     float64\n 11  Revenue_Arcade          99 non-null     float64\n 12  Total_Revenue           99 non-null     float64\n 13  Total_Labor_Hours       99 non-null     float64\n 14  International_Visitors  99 non-null     float64\n 15  High_Temperature        99 non-null     float64\n 16  Weather_Type            99 non-null     object \n 17  Is_Special_Event        99 non-null     int64  \n 18  Per_Capita_Spend        99 non-null     float64\ndtypes: float64(11), int64(6), object(2)\nmemory usage: 14.8+ KB\n\n\n\nday_25.head()\n\n\n\n\n\n\n\n\nDay_ID\nDay_of_Week\nseason_week_rel\nIs_Holiday\nTotal_Visitors\nPassholder_Percentage\nDay_Tickets_Sold\nAvg_Ticket_Price\nGate_Revenue\nRevenue_Food\nRevenue_Merch\nRevenue_Arcade\nTotal_Revenue\nTotal_Labor_Hours\nInternational_Visitors\nHigh_Temperature\nWeather_Type\nIs_Special_Event\nPer_Capita_Spend\n\n\n\n\n0\n1\nMonday\n1\n1\n1769\n48.65\n908\n77.42\n70297.36\n40521.91\n15881.290\n3195.46\n129896.02\n238.1\n114.0\n74.6\nThunderstorms\n0\n73.43\n\n\n1\n2\nTuesday\n1\n0\n1717\n60.58\n677\n69.80\n47254.60\n35298.92\n15882.665\n2977.63\n101812.36\n253.4\n255.0\n74.0\nPartly Cloudy\n0\n59.30\n\n\n2\n3\nWednesday\n1\n0\n1600\n30.84\n1107\n70.09\n77589.63\n36495.95\n14954.040\n2683.58\n131723.20\n213.3\n160.0\n76.8\nPartly Cloudy\n0\n82.33\n\n\n3\n4\nThursday\n1\n0\n1037\n19.02\n840\n72.30\n60732.00\n22486.65\n9517.060\n1831.98\n94567.69\n192.7\n83.0\n75.4\nShowers\n0\n91.19\n\n\n4\n5\nFriday\n1\n0\n1671\n66.18\n565\n68.11\n38482.15\n40705.15\n15727.620\n3115.94\n98030.86\n235.9\n158.0\n80.7\nPartly Cloudy\n0\n58.67\n\n\n\n\n\n\n\n\n# Convert binary flags (0,1) including: Is_Holiday and Is_Special_Event to categorical\nday_25[['Is_Holiday','Is_Special_Event']] = day_25[['Is_Holiday','Is_Special_Event']].astype('category')\n\n# Convert season_week_rel, Day_of_Week and Weather_Type from object to categorical variables\nday_25[['Day_of_Week','season_week_rel','Weather_Type']] = day_25[['Day_of_Week','season_week_rel','Weather_Type']].astype('category')\n\n\n# Drop the Day_ID variable\nday_25 = day_25.drop(columns=[\"Day_ID\"])\n\nIn this case, I decided to drop Day_ID because it’s just a sequential label and doesn’t seem to offer meaningful insight for analysis or prediction. Since it might unintentionally influence models by introducing artificial patterns, it felt more appropriate to leave it out of the feature set.\n\n# Double check to see if all the variables are stored as the appropriate data type\nday_25.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 99 entries, 0 to 98\nData columns (total 18 columns):\n #   Column                  Non-Null Count  Dtype   \n---  ------                  --------------  -----   \n 0   Day_of_Week             99 non-null     category\n 1   season_week_rel         99 non-null     category\n 2   Is_Holiday              99 non-null     category\n 3   Total_Visitors          99 non-null     int64   \n 4   Passholder_Percentage   99 non-null     float64 \n 5   Day_Tickets_Sold        99 non-null     int64   \n 6   Avg_Ticket_Price        99 non-null     float64 \n 7   Gate_Revenue            99 non-null     float64 \n 8   Revenue_Food            99 non-null     float64 \n 9   Revenue_Merch           99 non-null     float64 \n 10  Revenue_Arcade          99 non-null     float64 \n 11  Total_Revenue           99 non-null     float64 \n 12  Total_Labor_Hours       99 non-null     float64 \n 13  International_Visitors  99 non-null     float64 \n 14  High_Temperature        99 non-null     float64 \n 15  Weather_Type            99 non-null     category\n 16  Is_Special_Event        99 non-null     category\n 17  Per_Capita_Spend        99 non-null     float64 \ndtypes: category(5), float64(11), int64(2)\nmemory usage: 12.1 KB\n\n\n\nNumeric Variables: Total_Visitors, Passholder_Percentage, Day_Tickets_Sold, Avg_Ticket_Price, Gate_Revenue, Revenue_Food, Revenue_Merch, Revenue_Arcade, Total_Revenue, Total_Labor_Hours, International_Visitors, High_Temperature, Per_Capita_Spend\nCategorical Variables: Day_ID, Day_of_Week, season_week_rel, Is_Holiday, Weather_Type, Is_Special_Event\n\nIn my opinion, it’s not a great idea to use categorical inputs in a k-means model because the algorithm calculates distances between data points, and turning categories into numbers can be misleading. Take season_week_rel for example, it uses values from 1 to 15 to represent different weeks, but those numbers don’t really capture how similar or different the weeks are in terms of things like visitor patterns or revenue. The model might end up grouping week 1 and week 2 together just because their numbers are close, even if they behave very differently. Since k-means tries to minimize distance within clusters, it could create groups that look related mathematically but don’t actually make much sense in the real-world context of the data.\n\nCall the describe() function on your dataset.\n\nday_25.describe()\n\n\n\n\n\n\n\n\nTotal_Visitors\nPassholder_Percentage\nDay_Tickets_Sold\nAvg_Ticket_Price\nGate_Revenue\nRevenue_Food\nRevenue_Merch\nRevenue_Arcade\nTotal_Revenue\nTotal_Labor_Hours\nInternational_Visitors\nHigh_Temperature\nPer_Capita_Spend\n\n\n\n\ncount\n99.000000\n99.000000\n99.000000\n99.000000\n99.000000\n99.000000\n99.000000\n99.000000\n99.000000\n99.000000\n99.000000\n99.000000\n99.000000\n\n\nmean\n1870.707071\n41.973232\n1098.292929\n70.439697\n79484.778182\n54715.882626\n18691.392273\n4450.617879\n147911.785253\n256.535354\n178.464646\n77.126263\n76.930808\n\n\nstd\n641.078735\n15.581994\n466.750518\n10.680450\n37053.465251\n97737.730733\n8077.026667\n2992.738130\n62854.729274\n47.232843\n85.522800\n13.975537\n12.582334\n\n\nmin\n-50.000000\n11.720000\n322.000000\n-25.000000\n22350.020000\n22486.650000\n9517.060000\n1831.980000\n63247.570000\n192.700000\n-10.000000\n63.800000\n51.630000\n\n\n25%\n1482.000000\n28.895000\n771.000000\n68.250000\n53853.740000\n33291.365000\n13492.635000\n2693.595000\n103186.105000\n222.400000\n117.500000\n70.850000\n68.575000\n\n\n50%\n1704.000000\n40.900000\n995.000000\n70.480000\n70272.360000\n38054.660000\n15882.665000\n3026.520000\n129433.130000\n243.500000\n158.000000\n74.800000\n77.500000\n\n\n75%\n2080.000000\n54.590000\n1411.000000\n73.735000\n103147.720000\n55612.790000\n22567.790000\n5419.410000\n184173.685000\n272.000000\n232.500000\n81.350000\n84.385000\n\n\nmax\n3938.000000\n73.650000\n3029.000000\n82.280000\n242229.130000\n999999.990000\n51601.440000\n15755.610000\n423019.370000\n435.400000\n560.000000\n200.000000\n110.860000\n\n\n\n\n\n\n\nWhen I run the describe() function on this dataset, it provides a concise statistical summary of each numeric column, including the count, mean, standard deviation, and range values. This snapshot helps me quickly understand the distribution and behavior of each variable, which is essential for assessing data quality and preparing for modeling.\nIn this case, the summary reveals several problematic entries that need attention. For example, Total_Visitors, International_Visitors, and Avg_Ticket_Price contain negative values, which are not realistic in the context of theme park operations. Another noticeable issue in the summary table is that Revenue_Food shows a maximum value of 999,999.99, which exceeds the Total_Revenue maximum of 423,019.37, even though Total_Revenue should represent the combined daily revenue across all categories (gate, food, merchandise, and arcade). Identifying these issues early allows me to flag potential data entry errors or outliers and decide whether to clean, transform, or exclude certain records. This step is crucial for ensuring that the model is built on reliable and interpretable data.\n\n\nMissing values / Impossible values\n\nday_25.isnull().sum()\n\nDay_of_Week               0\nseason_week_rel           0\nIs_Holiday                0\nTotal_Visitors            0\nPassholder_Percentage     0\nDay_Tickets_Sold          0\nAvg_Ticket_Price          0\nGate_Revenue              0\nRevenue_Food              0\nRevenue_Merch             0\nRevenue_Arcade            0\nTotal_Revenue             0\nTotal_Labor_Hours         0\nInternational_Visitors    0\nHigh_Temperature          0\nWeather_Type              0\nIs_Special_Event          0\nPer_Capita_Spend          0\ndtype: int64\n\n\nAfter running the code and checking the results for missing values, the output indicates that all columns contain zero missing values, suggesting that the dataset is complete and no data cleaning through imputation or row removal is necessary.\n\nday_25.select_dtypes(include=\"number\").lt(0).sum()\n\nTotal_Visitors            1\nPassholder_Percentage     0\nDay_Tickets_Sold          0\nAvg_Ticket_Price          1\nGate_Revenue              0\nRevenue_Food              0\nRevenue_Merch             0\nRevenue_Arcade            0\nTotal_Revenue             0\nTotal_Labor_Hours         0\nInternational_Visitors    1\nHigh_Temperature          0\nPer_Capita_Spend          0\ndtype: int64\n\n\n\n# Look for rows with negative values in Total_Visitors, Avg_Ticket_Price and International_Visitors \nday_25[(day_25[\"Total_Visitors\"] &lt; 0) | \n       (day_25[\"Avg_Ticket_Price\"] &lt; 0) | \n       (day_25[\"International_Visitors\"] &lt; 0)]\n\n\n\n\n\n\n\n\nDay_of_Week\nseason_week_rel\nIs_Holiday\nTotal_Visitors\nPassholder_Percentage\nDay_Tickets_Sold\nAvg_Ticket_Price\nGate_Revenue\nRevenue_Food\nRevenue_Merch\nRevenue_Arcade\nTotal_Revenue\nTotal_Labor_Hours\nInternational_Visitors\nHigh_Temperature\nWeather_Type\nIs_Special_Event\nPer_Capita_Spend\n\n\n\n\n10\nThursday\n2\n0\n-50\n72.26\n444\n69.49\n30853.56\n40019.25\n15179.56\n2722.04\n88774.41\n228.6\n75.0\n82.4\nPartly Cloudy\n0\n55.52\n\n\n20\nSunday\n3\n0\n2691\n27.24\n1958\n-25.00\n146771.68\n67432.83\n26147.56\n7571.99\n247924.06\n305.3\n178.0\n79.5\nCloudy\n0\n92.13\n\n\n50\nTuesday\n8\n0\n1204\n72.60\n330\n68.39\n22568.70\n27999.37\n11246.96\n2202.88\n64017.91\n205.4\n-10.0\n78.0\nRain\n0\n53.17\n\n\n\n\n\n\n\n\n# Find rows with suspiciously high food revenue\nday_25[day_25[\"Revenue_Food\"] &gt; day_25[\"Total_Revenue\"]]\n\n\n\n\n\n\n\n\nDay_of_Week\nseason_week_rel\nIs_Holiday\nTotal_Visitors\nPassholder_Percentage\nDay_Tickets_Sold\nAvg_Ticket_Price\nGate_Revenue\nRevenue_Food\nRevenue_Merch\nRevenue_Arcade\nTotal_Revenue\nTotal_Labor_Hours\nInternational_Visitors\nHigh_Temperature\nWeather_Type\nIs_Special_Event\nPer_Capita_Spend\n\n\n\n\n30\nWednesday\n5\n0\n1772\n18.77\n1439\n69.82\n100470.98\n999999.99\n16262.36\n2974.24\n165467.64\n238.3\n137.0\n86.4\nCloudy\n0\n93.38\n\n\n\n\n\n\n\n\n# Find rows with suspiciosly high temperature \nday_25[day_25[\"High_Temperature\"] &gt; 120]\n\n\n\n\n\n\n\n\nDay_of_Week\nseason_week_rel\nIs_Holiday\nTotal_Visitors\nPassholder_Percentage\nDay_Tickets_Sold\nAvg_Ticket_Price\nGate_Revenue\nRevenue_Food\nRevenue_Merch\nRevenue_Arcade\nTotal_Revenue\nTotal_Labor_Hours\nInternational_Visitors\nHigh_Temperature\nWeather_Type\nIs_Special_Event\nPer_Capita_Spend\n\n\n\n\n40\nSaturday\n6\n0\n1933\n25.14\n1447\n74.99\n108510.53\n54861.67\n19528.12\n5038.63\n187938.95\n251.4\n194.0\n200.0\nRain\n0\n97.23\n\n\n\n\n\n\n\nTo identify impossible values, I focused on numeric variables and used the command day_25.select_dtypes(include=“number”).lt(0).sum(). This function isolates numeric columns and checks for any values less than zero, providing a quick count of negative entries per column. This approach is useful because variables like visitor counts, ticket sales, and revenue figures cannot logically be negative in a theme park setting. At the same time, Passholder_Percentage falls within a reasonable 0–100% range, so those values appear consistent.\nThe results showed three impossible values in Total_Visitors, International_Visitors, and Avg_Ticket_Price. These values are considered impossible because visitor counts cannot be negative - they must be zero or greater. Similarly, ticket prices are typically non-negative, unless there’s a rare case involving refunds or discounts. In this context, negative values likely indicate data entry errors or corrupted records.\nThe summary statistics also showed that Revenue_Food has a maximum value of 999,999.99, whereas Total_Revenue only has a maximum of 423,019.37. Because Total_Revenue should include all revenue categories, this should be corrected or reviewed before any further analysis.\nAnother entry that appears suspicious is High_Temperature, which shows a maximum value of 200°F. While temperature ranges can vary by region, typical readings in the United States generally fall between 0°F and a little over 100°F. Even Boston’s recent heatwave only reached a record high of 102°F, so a value of 200°F is unrealistic. To verify, I filtered the High_Temperature variable for values greater than 120°F and found only one record, suggesting this is likely a data entry or recording error.\n\n\nIf so, handle them in any way that you see fit.\n\n# Drop rows where Food_Revenue exceeds Total_Revenue\nday_25 = day_25[day_25[\"Revenue_Food\"] &lt;= day_25[\"Total_Revenue\"]]\n\nIn this case, the simplest and most reasonable solution is to remove the entire row, since even if I replaced Revenue_Food with a typical value (like the median), Total_Revenue would still end up smaller than the sum of its parts, which doesn’t make sense for this dataset.\n\n# Drop rows where  High_Temperature is abnormally high\nday_25 = day_25[day_25[\"High_Temperature\"] &lt;= 120]\n\nI decided to remove the row with the unusually high temperature value instead of imputing it with the median, since it appeared to be an outlier that could distort the results. By removing it, I ensure the dataset remains accurate and free from unrealistic values.\n\n# Filter so that all variables is non-negative\nday25 = day_25[\n    (day_25[\"Total_Visitors\"] &gt;= 0) &\n    (day_25[\"Avg_Ticket_Price\"] &gt;= 0) &\n    (day_25[\"International_Visitors\"] &gt;= 0)\n]\n\nday25.select_dtypes(include=\"number\").lt(0).sum()\n\nTotal_Visitors            0\nPassholder_Percentage     0\nDay_Tickets_Sold          0\nAvg_Ticket_Price          0\nGate_Revenue              0\nRevenue_Food              0\nRevenue_Merch             0\nRevenue_Arcade            0\nTotal_Revenue             0\nTotal_Labor_Hours         0\nInternational_Visitors    0\nHigh_Temperature          0\nPer_Capita_Spend          0\ndtype: int64\n\n\n\n# Now, day25 dataframe has 94 entries for each variable after dropping the 5 impossible rows.\nday25.count()\n\nDay_of_Week               94\nseason_week_rel           94\nIs_Holiday                94\nTotal_Visitors            94\nPassholder_Percentage     94\nDay_Tickets_Sold          94\nAvg_Ticket_Price          94\nGate_Revenue              94\nRevenue_Food              94\nRevenue_Merch             94\nRevenue_Arcade            94\nTotal_Revenue             94\nTotal_Labor_Hours         94\nInternational_Visitors    94\nHigh_Temperature          94\nWeather_Type              94\nIs_Special_Event          94\nPer_Capita_Spend          94\ndtype: int64\n\n\nI chose to drop the rows with impossible values rather than replace them with the median or mode because the errors are rare and clearly invalid. According to data_25.info() results above, I know that there are 99 entries for each variables. With only 5 affected entries out of 99, removing them avoids introducing assumptions that could distort relationships between variables. Replacing with typical values like the median might mask underlying issues or flatten meaningful variation, especially when the context behind the error is unknown. Dropping ensures the dataset remains clean and reasonable.\n\n\nVariable selection. Select any 5 variables from the potential set of inputs in order to build your k-means clustering model.\n\nvar_selected = day25[[\"Total_Visitors\",\"Avg_Ticket_Price\",\n                       \"High_Temperature\",\"Passholder_Percentage\",\"Per_Capita_Spend\"]]\n\nTo build a meaningful k-means clustering model, I selected the following five variables from the dataset: Total_Visitors, Avg_Ticket_Price, High_Temperature, Passholder_Percentage, and Per_Capita_Spend. These five variables were chosen to capture key aspects of daily park dynamics. Total_Visitors reflects overall attendance, giving a sense of how busy the park is. Avg_Ticket_Price ties into pricing strategy and its potential influence on visitor volume. High_Temperature displays a weather factor that can affect both attendance and spending. Passholder_Percentage helps distinguish between loyal visitors and one-time guests, which may impact behavior and spending habits. Per_Capita_Spend offers a normalized view of how much each visitor contributes financially, combining attendance and revenue into a single meaningful metric.\n\n\nData Scaling\n\n\nDo your variables need to be standardized? Why or why not?\nYes, I do think my variables should be standardized before applying k-means clustering. This is because they are measured on very different scales. For instance, Avg_Ticket_Price is no more than about $83, while Total_Visitors can reach nearly 4,000, which is a much larger numerical range. Similarly, Passholder_Percentage is expressed as a proportion (0–100%), and High_Temperature and Per_Capita_Spend also exist on their own distinct scales. Since k-means uses Euclidean distance to group observations, variables with larger numeric ranges would otherwise dominate the clustering process. Standardizing the data ensures that each variable contributes equally to distance calculations, leading to more balanced and meaningful clusters.\n\n\nIf your data requires standardization, use Python to convert your values into z-scores, and store the normalized data in a new dataframe. If not, proceed to the next step without changing the variables.\n\n# Import scaler function to standardize dataframe\nscaler = StandardScaler()\nvar_scaled = scaler.fit_transform(var_selected)\n# Create a new dataframe with standardized values\nvar_scaled_df = pd.DataFrame(var_scaled, columns = var_selected.columns)\n\nprint(var_scaled_df.head())\n\n   Total_Visitors  Avg_Ticket_Price  High_Temperature  Passholder_Percentage  \\\n0       -0.196940          1.312127         -0.163867               0.452158   \n1       -0.281650         -0.361277         -0.259344               1.252239   \n2       -0.472247         -0.297591          0.186213              -0.742262   \n3       -1.389393          0.187740         -0.036565              -1.534966   \n4       -0.356585         -0.732412          0.806810               1.627801   \n\n   Per_Capita_Spend  \n0         -0.285287  \n1         -1.460976  \n2          0.455238  \n3          1.192436  \n4         -1.513395  \n\n\n\n\nElbow Chart\n\n# Define X with 5 selected columns from the standardized data\nX = var_scaled_df[[\"Total_Visitors\",\n                   \"Avg_Ticket_Price\",\n                   \"High_Temperature\",\n                   \"Passholder_Percentage\",\n                   \"Per_Capita_Spend\"\n                  ]]\n\n# Build the elbow chart\nsse = []\ncluster_range = range(1, 15)\nfor k in cluster_range:\n    kmeans = KMeans(n_clusters=k, random_state=616)\n    kmeans.fit(X)\n    sse.append(kmeans.inertia_)\n\n# Plot the Elbow chart\nplt.figure(figsize=(8, 5))\nplt.plot(cluster_range, sse, marker='o')\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"SSE (Sum of Squared Errors)\")\nplt.title(\"Elbow Chart for K-means Clustering\")\nplt.xticks(cluster_range)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nHow many clusters will you use for your k-means model?\nThe elbow chart above shows the SSE decreasing sharply from around 480 to approximately 175 as the number of clusters increases. The most significant drop occurs between 1 and 4 clusters, after which the curve begins to flatten, suggesting diminishing returns from adding more clusters. Based on this pattern, I selected k = 4 for the k-means model, as it captures most of the variance in the data while maintaining a manageable number of segments.\nA smaller number of clusters (e.g., 2 or 3) would likely combine distinct park-day types, such as slower weekdays and peak weekends - into the same group, masking meaningful operational differences. By using four clusters, the park can better understand how key factors like visitor volume, pricing, weather, and spending behavior interact across different day types.\n\n\nBuild a k-means model with your desired number of clusters.\n\n# Build and fit the K-Means model on the standardized DataFrame 'X'\nkmeans_model = KMeans(n_clusters = 4, random_state = 616)\nkmeans_model.fit(X)\n\n# Add cluster labels to the original standardized df\nvar_scaled_df['Cluster'] = kmeans_model.labels_\n\nprint(\"K-Means Cluster Centers (for 4 clusters):\")\nprint(var_scaled_df)\n\nK-Means Cluster Centers (for 4 clusters):\n    Total_Visitors  Avg_Ticket_Price  High_Temperature  Passholder_Percentage  \\\n0        -0.196940          1.312127         -0.163867               0.452158   \n1        -0.281650         -0.361277         -0.259344               1.252239   \n2        -0.472247         -0.297591          0.186213              -0.742262   \n3        -1.389393          0.187740         -0.036565              -1.534966   \n4        -0.356585         -0.732412          0.806810               1.627801   \n..             ...               ...               ...                    ...   \n89       -0.742667         -0.567707         -0.084304              -0.679222   \n90       -0.242553         -0.211944         -1.325497              -0.760370   \n91        1.531466          1.031030         -0.609424              -0.212452   \n92       -0.009601         -0.225120         -0.800377               0.871312   \n93        0.837497          0.752130         -0.338907              -0.750310   \n\n    Per_Capita_Spend  Cluster  \n0          -0.285287        0  \n1          -1.460976        0  \n2           0.455238        3  \n3           1.192436        3  \n4          -1.513395        1  \n..               ...      ...  \n89          0.366209        3  \n90          0.213943        3  \n91          0.434437        2  \n92         -0.678848        0  \n93          0.615824        3  \n\n[94 rows x 6 columns]\n\n\nI built the k-means model using a data frame containing only the 5 selected variables. By setting random_state = 616, I ensured that the clustering results remain consistent across runs. I selected 3 clusters to achieve a more detailed and meaningful segmentation of the park.\n\n\nGenerate Centroid Values\n\n# Calculate centroid values for each cluster\ncentroids = var_scaled_df.groupby('Cluster').mean()\n\n# Display the centroids\nprint(centroids)\n\n         Total_Visitors  Avg_Ticket_Price  High_Temperature  \\\nCluster                                                       \n0             -0.592234         -0.358172         -0.670880   \n1             -0.295578         -0.381371          1.168029   \n2              1.977070          1.522274          0.319635   \n3             -0.081737         -0.055474         -0.251885   \n\n         Passholder_Percentage  Per_Capita_Spend  \nCluster                                           \n0                     0.643200         -0.830625  \n1                     0.596481         -0.552168  \n2                     0.303739          0.530955  \n3                    -1.079094          0.882159  \n\n\n\ncluster_counts = var_scaled_df['Cluster'].value_counts().sort_index()\nprint(\"\\nCluster Counts:\")\nprint(cluster_counts)\n\n\nCluster Counts:\nCluster\n0    29\n1    20\n2    13\n3    32\nName: count, dtype: int64\n\n\n\n\nBuild four simple visualizations to help management better understand your clusters\n\n\nVisualization 1: Histogram of Total Visitors by Cluster\n\nplt.figure(figsize=(8, 5))\nsns.histplot(data=var_scaled_df, x=\"Total_Visitors\", hue=\"Cluster\", multiple=\"stack\", palette=\"Set2\")\nplt.title(\"Distribution of Total Visitors by Cluster\")\nplt.xlabel(\"Total Visitors (Standardized)\")\nplt.ylabel(\"Frequency\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis histogram shows the distribution of Total Visitors(standardized) across the four clusters groupped by the model, with the x-axis representing Total Visitors to Lobster Land and the y-axis showing their frequency. Cluster 0 (green) is concentrated around lower standardized visitor counts, suggesting it represents quieter, low-traffic days. Cluster 1 (orange) sits in the middle, reflecting moderate visitor days, not too busy nor too quiet. Cluster 2 (blue) dominates the higher end of the scale, capturing peak attendance periods, while Cluster 3 (pink) spans wider range, suggesting it includes a more diverse mix of visitors who vary from occasional guests to highly engaged ones.\nAlthough this is a simple chart, it’s crucial for Lobster Land, as it clearly shows how each cluster reacts differently in terms of attendance and helps the team plan targeted promotions and engagement strategies tailored to each group’s behavior.\n\n\nVisualization 2: Barplot Average Ticket Price per Cluster\n\nplt.figure(figsize=(8, 5))\nsns.barplot(data=var_scaled_df, x=\"Cluster\", y=\"Avg_Ticket_Price\", hue=\"Cluster\", palette=\"Set2\", dodge=False, legend=False, edgecolor=\"black\")\nplt.title(\"Average Ticket Price per Cluster\")\nplt.xlabel(\"Cluster\")\nplt.ylabel(\"Average Ticket Price (Standardized)\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis bar chart illustrates the mean standardized average ticket price across the four clusters identified by the model. Cluster 2 (blue) shows the highest average ticket price, representing premium or high-demand days. Clusters 0 (green) and 1 (orange) display below-average ticket prices, with Cluster 1 slightly lower than Cluster 0, indicating more budget-friendly or passholder-heavy days. Cluster 3 (pink) remains close to the standardized mean, reflecting more typical pricing conditions across Lobster Land’s visitor segments. This chart helps Lobster Land better understand how ticket prices vary across different visitor groups, allowing the team to align pricing strategies and promotions with each segment’s spending behavior.\n\n\nVisualization 3: Scatterplot of Passholder Percentage vs Per Capita Spend colored by Cluster\n\nplt.figure(figsize=(8, 5))\nsns.scatterplot(data=var_scaled_df, x=\"Passholder_Percentage\", y=\"Per_Capita_Spend\", hue=\"Cluster\", palette=\"Set2\")\nplt.title(\"Passholder Percentage vs Per Capita Spend by Cluster\")\nplt.xlabel(\"Passholder Percentage\")\nplt.ylabel(\"Per Capita Spend\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis scatter plot shows how Passholder Percentage relates to Per Capita Spend across the four clusters identified by the model. I noticed a clear pattern where clusters with a higher share of passholders tend to have lower spending per person, while those with fewer passholders show higher spending levels. Cluster 0 (green) has a wide range of passholder percentages but generally lower spending, whereas Cluster 3 (pink) stands out with a smaller share of passholders and higher spending, likely representing one-time or premium visitors. Whereas Cluster 2 (blue) shows a distinct profile, with moderate passholder percentages and positive per capita spending, reflecting a balanced mix of loyal and casual visitors at Lobster Land. From this chart, we can see how different visitor types behave at Lobster Land, helping identify ways to balance loyalty programs with opportunities to attract higher-value guests.\n\n\nVisualization 4: Boxplot High_Temperature per Cluster\n\nplt.figure(figsize=(8, 5))\nsns.boxplot(data=var_scaled_df, x=\"Cluster\", y=\"High_Temperature\", hue=\"Cluster\", palette=\"Set2\", dodge=False)\nplt.title(\"High Temperature by Cluster\")\nplt.xlabel(\"Cluster\")\nplt.ylabel(\"High Temperature (Standardized)\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis boxplot shows how standardized high temperatures vary across the four clusters identified by the model. Cluster 1 (orange) has the highest median temperature, representing hotter days, while Clusters 0 (green) and 3 (pink) are generally cooler. Cluster 2 (blue) shows the widest temperature range, including both hot and cool days, which may reflect mixed or transitional seasonal conditions at Lobster Land.\n\n\nCluster Descriptions\n\n\nCluster 0: “Chill & Sparse”\nThese are the cool, calm days with lower visitor counts, below-average ticket prices, and a higher share of passholders. Spending per person is lighter, creating an easygoing park atmosphere that appeals to loyal guests and locals who enjoy a quieter visit. - Targeting: LobsterLand can focus on community connections and rewarding frequent visitors on these days. Promotions highlighting relaxed park experiences, quiet attractions, or weekday benefits can draw local families and passholders seeking a slower, more comfortable visit.\n\n\nCluster 1: “Scorch & Stroll”\nThis cluster displays the warmest temperatures, moderate attendance, and lower ticket prices. Spending per capita is modest, and many visitors hold passes, creating a steady but easygoing flow of activity across the park. - Targeting: The marketing team can emphasize refreshment and comfort through shaded seating, cool treat bundles or special water rides. Bright and refreshing summer campaigns that highlight leisurely fun and family-friendly attractions can reach guests looking for simple, enjoyable park days.\n\n\nCluster 2: “Gold Rush Days”\nCluster 2 represents the park’s busiest, most profitable days with high visitor counts, the highest standardized ticket prices, and strong per capita spending. With a balanced mix of loyal and casual visitors, these are the moments when the park operates at full capacity and captures its greatest market potential. - Targeting: These days can spotlight premium experiences such as fast passes, dining packages, and exclusive access areas. Promotions on travel platforms and event partnerships can reach visitors planning full-day trips or group outings, aligning offerings with elevated park energy and spending activity.\n\n\nCluster 3: “Balanced Buzz”\nThis cluster shows moderate temperatures, mixed visitor counts, and steady spending levels. Ticket prices remain close to the average, and the lower passholder percentage suggests a healthy mix of new and returning guests. - Targeting: With a blend of loyal customers and newcomers, LobsterLand can use this opportunity to run limited-time offers, themed events, or new experiences can help identify emerging interests.\n\n\nHow can Lobster Land use this model?\nLobster Land can use this day-type clustering model as a practical tool to help the team make better decisions about staffing, pricing, and creating great experiences for guests. By figuring out which cluster an upcoming day fits into, like a busy “Gold Rush Day” or a quieter “Chill & Sparse” day, the team can get ahead of operations instead of just reacting. On high-traffic days, the park can bring in extra staff, stay open later, and offer special upgrades that guests will actually want. On slower days, there’s room to focus on rewarding regulars, running leaner operations, or taking care of behind-the-scenes maintenance that’s hard to do when the park is packed.\nThe marketing team can also use these insights to shape promotions that actually fit the day. Take “Scorch & Stroll” days, for instance. These hot, leisurely days might be perfect for promoting cold drink bundles or access to shaded seating areas. “Balannced Buzz” days, with their unexpected energy, could be a testing ground for new attractions or digital campaigns to see what resonates. As Lobster Land builds out this approach, weaving in weather forecasts and calendar events could help predict cluster assignments days or even weeks ahead. That shifts the park from reactive scrambling to a proactive, rhythm-based planning style that aligns operations with real guest patterns and business priorities.\nIf the team could predict which cluster a day might fall into, it would make planning feel a lot more purposeful and efficient. For instance, if an upcoming weekend looks like a “Gold Rush Day,” the park could bring in extra staff, stock up on food and drinks, and roll out seasonal specials like fall-inspired treats or Halloween-themed beverages to attract both new visitors and returning fans. On the other hand, if a slower Tuesday seems to fall into the “Chill & Sparse” cluster, Lobster Land could use the opportunity to run local promotions, offer discounted afternoon passes, or host a cozy themed dining night for family and friends. This kind of planning helps the park stay balanced—keeping operations efficient, guests happy, and business decisions aligned with real demand."
  },
  {
    "objectID": "python_notebooks/AD654_Nhat_Tran_Assigment_2.html#part-ii-conjoint-analysis-with-a-linear-model",
    "href": "python_notebooks/AD654_Nhat_Tran_Assigment_2.html#part-ii-conjoint-analysis-with-a-linear-model",
    "title": "Market Segmentation & Conjoint Analysis",
    "section": "Part II: Conjoint Analysis with a Linear Model",
    "text": "Part II: Conjoint Analysis with a Linear Model\n\n# Import night_show.csv files into python for use\nnight_show = pd.read_csv(\"/Users/nhattran/Downloads/MSBA/METAD654 - Marketing Analytics/assignment 2/night_show.csv\",keep_default_na=False, na_values=[])\n\n\nnight_show.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 73728 entries, 0 to 73727\nData columns (total 12 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   bundleID          73728 non-null  int64  \n 1   headliner         73728 non-null  object \n 2   music_atmosphere  73728 non-null  object \n 3   show_length       73728 non-null  int64  \n 4   peak_dB_Level     73728 non-null  int64  \n 5   crowding_index    73728 non-null  int64  \n 6   viewing_zone      73728 non-null  object \n 7   dining_bundle     73728 non-null  object \n 8   re_entry          73728 non-null  object \n 9   live_host         73728 non-null  object \n 10  charity_tie_in    73728 non-null  object \n 11  ratings           73728 non-null  float64\ndtypes: float64(1), int64(4), object(7)\nmemory usage: 6.8+ MB\n\n\n\nnight_show.head()\n\n\n\n\n\n\n\n\nbundleID\nheadliner\nmusic_atmosphere\nshow_length\npeak_dB_Level\ncrowding_index\nviewing_zone\ndining_bundle\nre_entry\nlive_host\ncharity_tie_in\nratings\n\n\n\n\n0\n1\nFireworks\nPop Soundtrack\n30\n65\n40\nfront plaza\nNone\nNone\nYes\nYes\n4.249\n\n\n1\n2\nFireworks\nPop Soundtrack\n30\n65\n40\nfront plaza\nNone\nNone\nYes\nNo/Doesn't Matter\n4.689\n\n\n2\n3\nFireworks\nPop Soundtrack\n30\n65\n40\nfront plaza\nNone\nNone\nNo/Doesn't Matter\nYes\n7.999\n\n\n3\n4\nFireworks\nPop Soundtrack\n30\n65\n40\nfront plaza\nNone\nNone\nNo/Doesn't Matter\nNo/Doesn't Matter\n4.309\n\n\n4\n5\nFireworks\nPop Soundtrack\n30\n65\n40\nfront plaza\nNone\nOne\nYes\nYes\n6.239\n\n\n\n\n\n\n\nBased on the 12 variables provided in the dataset, here’s the list of numeric and categorical variables: - Numerical Variables: show_length (in minutes); peak_dB_Level (decibels level); crowding_index (crowd density); ratings (average ratings from 0 t0 10).\n\nCategorical Variables: bundleID, headliner, music_atmosphere, viewing_zone, dining_bundle, re_entry, live_host, and charity_tie_in, as they represent distinct categories or labels rather than quantities with mathematical meaning.\nI also dropped bundleID from the dataset because it functions solely as a sequential identifier without providing any analytical or predictive value. Including it could potentially create misleading patterns in the model, so excluding it prevents future problems.\n\n\nnight_show = night_show.drop(columns=[\"bundleID\"])\n\n\n# Define the columns to convert\ncategorical_cols = [\"headliner\", \"music_atmosphere\", \"viewing_zone\", \"dining_bundle\",\n                    \"re_entry\", \"live_host\", \"charity_tie_in\"]\n# Convert to category dtype\nnight_show[categorical_cols] = night_show[categorical_cols].astype('category')\n\n\nnight_show.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 73728 entries, 0 to 73727\nData columns (total 11 columns):\n #   Column            Non-Null Count  Dtype   \n---  ------            --------------  -----   \n 0   headliner         73728 non-null  category\n 1   music_atmosphere  73728 non-null  category\n 2   show_length       73728 non-null  int64   \n 3   peak_dB_Level     73728 non-null  int64   \n 4   crowding_index    73728 non-null  int64   \n 5   viewing_zone      73728 non-null  category\n 6   dining_bundle     73728 non-null  category\n 7   re_entry          73728 non-null  category\n 8   live_host         73728 non-null  category\n 9   charity_tie_in    73728 non-null  category\n 10  ratings           73728 non-null  float64 \ndtypes: category(7), float64(1), int64(3)\nmemory usage: 2.7 MB\n\n\n\nUse the pandas get_dummies() function in order to prepare these variables for use in a linear model. Inside this function, include this argument: drop_first = True. Doing this will save us from the multicollinearity problem that would make our model unreliable.\n\n# Before dummifying the variables, check to see if the dataset has any null input\nnight_show.isnull().values.any()\n\nnp.False_\n\n\n\nnight_show.columns\n\nIndex(['headliner', 'music_atmosphere', 'show_length', 'peak_dB_Level',\n       'crowding_index', 'viewing_zone', 'dining_bundle', 're_entry',\n       'live_host', 'charity_tie_in', 'ratings'],\n      dtype='object')\n\n\n\nnight_show_dm = pd.get_dummies(night_show, drop_first=True, columns=['headliner', 'music_atmosphere', 'show_length', 'peak_dB_Level',\n       'crowding_index', 'viewing_zone', 'dining_bundle', 're_entry','live_host', 'charity_tie_in'])\n\n\nnight_show_dm.head()\n\n\n\n\n\n\n\n\nratings\nheadliner_Drone Light Show\nheadliner_Fireworks\nheadliner_Live Musical\nmusic_atmosphere_Pop Soundtrack\nshow_length_70\nshow_length_110\nshow_length_150\npeak_dB_Level_85\npeak_dB_Level_105\n...\nviewing_zone_boardwalk\nviewing_zone_castle_courtyard\nviewing_zone_front plaza\ndining_bundle_dessert_party\ndining_bundle_meal_package\ndining_bundle_snack_voucher\nre_entry_One\nre_entry_Unlimited\nlive_host_Yes\ncharity_tie_in_Yes\n\n\n\n\n0\n4.249\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nTrue\n\n\n1\n4.689\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n2\n7.999\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n3\n4.309\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n4\n6.239\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\n\n\n\n\n5 rows × 23 columns\n\n\n\n\nnight_show_dm.columns\n\nIndex(['ratings', 'headliner_Drone Light Show', 'headliner_Fireworks',\n       'headliner_Live Musical', 'music_atmosphere_Pop Soundtrack',\n       'show_length_70', 'show_length_110', 'show_length_150',\n       'peak_dB_Level_85', 'peak_dB_Level_105', 'crowding_index_60',\n       'crowding_index_80', 'crowding_index_100', 'viewing_zone_boardwalk',\n       'viewing_zone_castle_courtyard', 'viewing_zone_front plaza',\n       'dining_bundle_dessert_party', 'dining_bundle_meal_package',\n       'dining_bundle_snack_voucher', 're_entry_One', 're_entry_Unlimited',\n       'live_host_Yes', 'charity_tie_in_Yes'],\n      dtype='object')\n\n\n\n\nWhy should the numeric input variables based on this survey data be dummified?\nNumeric input variables from survey data should be converted into dummy variables because even though they appear as numbers, they represent discrete categories rather than continuous measurements. Take show_length for example, it might include values such as 30, 70, 110, and 150 minutes. These values are not part of a linear scale but rather specific options provided by the survey design. Treating them as continuous would incorrectly assume that the difference between 30 and 70 minutes has the same proportional relationship as the difference between 110 and 150 minutes, which may not reflect reality.\nBy converting these numeric inputs into dummy variables, the model can assign independent effects to each category without imposing misleading linear assumptions. This approach improves both the accuracy and interpretability of the model while ensuring that the analysis respects the categorical structure inherent in survey-based numeric data.\n\n\nIt might seem tempting here to just skip the model-building step – instead of building a model, we could just rank the bundles by average rating, in descending order, and then just tell Lobster Land to implement all the features in the highest-rated bundle.\nRanking bundles by their average rating might seem ideal, but it hides the true drivers of guest satisfaction. Without building a model, it’s difficult to tell which specific features within each bundle actually influence ratings or how they interact with one another. For example, a highly rated bundle might include both a live musical and unlimited re-entry, but we would not know that the musical boosts satisfaction while the re-entry option reduces it. A conjoint analysis model solves this by estimating the independent effect of each feature across all bundles, allowing Lobster Land to predict how new combinations would perform and design offerings that maximize impact while keeping costs manageable. In short, the model provides a deeper, data-driven understanding of guest preferences that simple ranking cannot achieve.\n\n\nBuild a linear model with your data, using the average rating as the outcome variable, and with all of your other variables (other than the bundleID) as inputs.\nI build a linear regression model using all dummified input features to predict the average rating. This allows to estimate how each individual feature contributes to guest satisfaction.\n\n# Create Linear Regression model\nX = night_show_dm[['headliner_Drone Light Show', 'headliner_Fireworks',\n       'headliner_Live Musical', 'music_atmosphere_Pop Soundtrack',\n       'show_length_70', 'show_length_110', 'show_length_150',\n       'peak_dB_Level_85', 'peak_dB_Level_105', 'crowding_index_60',\n       'crowding_index_80', 'crowding_index_100', 'viewing_zone_boardwalk',\n       'viewing_zone_castle_courtyard', 'viewing_zone_front plaza',\n       'dining_bundle_dessert_party', 'dining_bundle_meal_package',\n       'dining_bundle_snack_voucher', 're_entry_One', 're_entry_Unlimited',\n       'live_host_Yes', 'charity_tie_in_Yes']]\ny = night_show_dm[\"ratings\"]\n\n\n# Import LinearRegression function from scikit‑learn\nfrom sklearn.linear_model import LinearRegression\n\nregressor = LinearRegression()\nregressor.fit(X, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\n\nregressor.intercept_\nprint(f\"Intercept: {regressor.intercept_:.3f}\")\n\nIntercept: 4.953\n\n\n\nimport statsmodels.api as sm\n\n# Define outcome and predictors\ny = night_show_dm['ratings']\nX_model = night_show_dm.drop(columns=['ratings'])\n\n# Add intercept term\nX_model = sm.add_constant(X_model)\n# Convert boolean columns to numeric\nbool_cols = X_model.select_dtypes(include=['bool']).columns\nX_model[bool_cols] = X_model[bool_cols].astype(int)\nX_model = X_model.astype(float)\n\n# Fit the model\nlinear_model = sm.OLS(y, X_model).fit()\nprint(linear_model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                ratings   R-squared:                       0.345\nModel:                            OLS   Adj. R-squared:                  0.345\nMethod:                 Least Squares   F-statistic:                     1766.\nDate:                Mon, 12 Jan 2026   Prob (F-statistic):               0.00\nTime:                        22:53:46   Log-Likelihood:            -1.4678e+05\nNo. Observations:               73728   AIC:                         2.936e+05\nDf Residuals:                   73705   BIC:                         2.938e+05\nDf Model:                          22                                         \nCovariance Type:            nonrobust                                         \n===================================================================================================\n                                      coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------------------------\nconst                               4.9533      0.031    158.283      0.000       4.892       5.015\nheadliner_Drone Light Show          0.0178      0.018      0.964      0.335      -0.018       0.054\nheadliner_Fireworks                 0.0009      0.018      0.046      0.963      -0.035       0.037\nheadliner_Live Musical              1.0299      0.018     55.801      0.000       0.994       1.066\nmusic_atmosphere_Pop Soundtrack    -0.7130      0.013    -54.634      0.000      -0.739      -0.687\nshow_length_70                      0.7179      0.018     38.898      0.000       0.682       0.754\nshow_length_110                     0.9709      0.018     52.608      0.000       0.935       1.007\nshow_length_150                     0.7038      0.018     38.136      0.000       0.668       0.740\npeak_dB_Level_85                    0.0165      0.016      1.034      0.301      -0.015       0.048\npeak_dB_Level_105                  -0.8872      0.016    -55.507      0.000      -0.919      -0.856\ncrowding_index_60                   0.2230      0.018     12.085      0.000       0.187       0.259\ncrowding_index_80                   0.2323      0.018     12.588      0.000       0.196       0.268\ncrowding_index_100                 -0.6046      0.018    -32.760      0.000      -0.641      -0.568\nviewing_zone_boardwalk              0.3889      0.018     21.070      0.000       0.353       0.425\nviewing_zone_castle_courtyard       0.3560      0.018     19.291      0.000       0.320       0.392\nviewing_zone_front plaza            0.3389      0.018     18.362      0.000       0.303       0.375\ndining_bundle_dessert_party        -0.0028      0.018     -0.154      0.877      -0.039       0.033\ndining_bundle_meal_package          0.0033      0.018      0.177      0.860      -0.033       0.039\ndining_bundle_snack_voucher        -0.0922      0.018     -4.993      0.000      -0.128      -0.056\nre_entry_One                        0.2968      0.016     18.569      0.000       0.265       0.328\nre_entry_Unlimited                 -1.5920      0.016    -99.604      0.000      -1.623      -1.561\nlive_host_Yes                       0.8425      0.013     64.555      0.000       0.817       0.868\ncharity_tie_in_Yes                  0.2418      0.013     18.529      0.000       0.216       0.267\n==============================================================================\nOmnibus:                      584.467   Durbin-Watson:                   2.006\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              394.485\nSkew:                          -0.030   Prob(JB):                     2.18e-86\nKurtosis:                       2.647   Cond. No.                         10.8\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\nDisplay the coefficient values of your model inputs.\nI extract and display the model’s coefficients to understand the direction and strength of each feature’s impact on ratings.\n\ncoef_df = pd.DataFrame(regressor.coef_, X.columns, columns=['Coefficient'])\ncoef_df\n\n\n\n\n\n\n\n\nCoefficient\n\n\n\n\nheadliner_Drone Light Show\n0.017789\n\n\nheadliner_Fireworks\n0.000853\n\n\nheadliner_Live Musical\n1.029869\n\n\nmusic_atmosphere_Pop Soundtrack\n-0.713001\n\n\nshow_length_70\n0.717907\n\n\nshow_length_110\n0.970937\n\n\nshow_length_150\n0.703843\n\n\npeak_dB_Level_85\n0.016527\n\n\npeak_dB_Level_105\n-0.887189\n\n\ncrowding_index_60\n0.223038\n\n\ncrowding_index_80\n0.232322\n\n\ncrowding_index_100\n-0.604624\n\n\nviewing_zone_boardwalk\n0.388865\n\n\nviewing_zone_castle_courtyard\n0.356038\n\n\nviewing_zone_front plaza\n0.338891\n\n\ndining_bundle_dessert_party\n-0.002845\n\n\ndining_bundle_meal_package\n0.003259\n\n\ndining_bundle_snack_voucher\n-0.092152\n\n\nre_entry_One\n0.296802\n\n\nre_entry_Unlimited\n-1.592003\n\n\nlive_host_Yes\n0.842467\n\n\ncharity_tie_in_Yes\n0.241810\n\n\n\n\n\n\n\n\n\nWrite two-three paragraphs for Lobster Land management about what your model is showing you.\nDear Lobster Land Management Team,\nI wanted to share some important findings from our conjoint analysis that can help guide decisions around evening entertainment programming. By examining guest survey responses across different features—headliner type, show length, sound levels, re-entry policies, and more—the analysis provides a clearer picture of what actually drives satisfaction and what might be holding the park back. The results show some strong patterns. Live Musical performances (coefficient of 1.03) and having a live host (0.84) make a significant positive impact on guest ratings, while unlimited re-entry (-1.59) and extremely loud sound at 105 dB (-0.89) actively hurt the experience. Shows running around 110 minutes (0.97) appear to be the ideal length, and high crowding levels (negative 0.60 at maximum capacity) clearly diminish satisfaction. These numbers help clarify not just what guests prefer, but how much each element matters when prioritizing investments and operational changes.\nFrom a short-term planning perspective, there are several actionable takeaways. The 110-minute show format seems to be the best option where guests feel they’re getting a full experience without it dragging on or feeling rushed. Although the unlimited re-entry sounds captivating, guests actually prefer having just one opportunity to leave and return, likely because unlimited access signals overcrowding or diminishes the sense of exclusivity. A simple re-entry policy could make guests feel they’re getting more value while also helping manage crowds better. Live musicals clearly offer the best experiences, but they also come with higher costs, tighter schedules, and more moving parts than drone shows or fireworks. The team will need to think carefully about when and how often to host live performances, especially during busy seasons when multiple shows a night could stretch both staff and resources.\nLooking at longer-term strategy, the data suggests some interesting opportunities for tiered experiences. The park should offer premium ticket packages which allow guests to book particular less crowded viewing spots within standard capacity areas. This allows guests who value personal space to pay for that comfort while offering more budget-conscious visitors access at lower price points. Live hosts and charity partnerships, though subtle in impact, show how genuine connection and purpose-driven storytelling can make experiences more memorable and unique. On the dining side, bundles don’t boost satisfaction much, and snack vouchers may even hurt it, as guests often see them as low-value add-ons rather than real enhancements.\nThe key take-away is: prioritize live performances with engaging hosts, keep 110-minute show lengths, keep the sound levels part of the experience without being too overpowering, and manage crowds so visitors have room to enjoy themselves. By focusing on what matters most to customers (authentic experiences, good pacing, and making the evening special), the park can differentiate and create the evenings guests will come back for."
  },
  {
    "objectID": "coursework.html",
    "href": "coursework.html",
    "title": "Relevant Coursework",
    "section": "",
    "text": "M.S. in Business Analytics, Boston University\n\n\n\n\n  \n  \n  Applied data manipulation, querying, and analytics workflows using Python and SQL.\n  \n    Python\n    SQL\n    Data Manipulation\n    ETL\n  \n\n  \n  \n  Core analytical methods, data-driven problem solving, and statistical reasoning.\n  \n    R\n    Machine Learning\n    Power BI\n    Statistical Reasoning\n  \n\n  \n  \n  Scalable data processing, cloud-based analytics architectures, and deployment concepts.\n  \n    Python\n    Machine Learning\n    SQL\n    plotly\n    AWS Cloud Architecture\n  \n\n  \n  \n  Customer segmentation, predictive modeling, and performance measurement using data.\n  \n    Python\n    Tableau\n    Regression\n    Clustering\n    Classification\n    Random Forest\n  \n\n\n\nTechniques for extracting insights from structured and unstructured data using Python, R, SQL, and Power BI.\n\n  Python\n  R\n  SQL\n  NLP\n  Clustering\n  Classification\n\n\n\n\nLearn to leverage risk analytics to identify opportunities, mitigate threats, and enhance enterprise value using advanced analytics tools.\n\n  Python\n  R\n  SQL\n  Optimization\n\n\n  \n  \n  Decision analysis using quantitative models complemented by qualitative frameworks.\n  \n    R\n    Optimization\n    Scenario Modeling\n  \n\n  \n  \n  Financial statement analysis, valuation fundamentals, and analytics applications in finance.\n  \n    Financial Modeling"
  },
  {
    "objectID": "coursework.html#graduate-coursework",
    "href": "coursework.html#graduate-coursework",
    "title": "Relevant Coursework",
    "section": "",
    "text": "M.S. in Business Analytics, Boston University\n\n\n\n\n  \n  \n  Applied data manipulation, querying, and analytics workflows using Python and SQL.\n  \n    Python\n    SQL\n    Data Manipulation\n    ETL\n  \n\n  \n  \n  Core analytical methods, data-driven problem solving, and statistical reasoning.\n  \n    R\n    Machine Learning\n    Power BI\n    Statistical Reasoning\n  \n\n  \n  \n  Scalable data processing, cloud-based analytics architectures, and deployment concepts.\n  \n    Python\n    Machine Learning\n    SQL\n    plotly\n    AWS Cloud Architecture\n  \n\n  \n  \n  Customer segmentation, predictive modeling, and performance measurement using data.\n  \n    Python\n    Tableau\n    Regression\n    Clustering\n    Classification\n    Random Forest\n  \n\n\n\nTechniques for extracting insights from structured and unstructured data using Python, R, SQL, and Power BI.\n\n  Python\n  R\n  SQL\n  NLP\n  Clustering\n  Classification\n\n\n\n\nLearn to leverage risk analytics to identify opportunities, mitigate threats, and enhance enterprise value using advanced analytics tools.\n\n  Python\n  R\n  SQL\n  Optimization\n\n\n  \n  \n  Decision analysis using quantitative models complemented by qualitative frameworks.\n  \n    R\n    Optimization\n    Scenario Modeling\n  \n\n  \n  \n  Financial statement analysis, valuation fundamentals, and analytics applications in finance.\n  \n    Financial Modeling"
  },
  {
    "objectID": "coursework.html#undergraduate-coursework",
    "href": "coursework.html#undergraduate-coursework",
    "title": "Relevant Coursework",
    "section": "Undergraduate Coursework",
    "text": "Undergraduate Coursework\n\n\n  B.S. in Economics, Drexel University\n\n\n\n\n\nMicroeconomics & Macroeconomics\nExplored market behavior, incentives, and macroeconomic dynamics underlying business and policy decisions.\n\n  Economic Theory\n  Market Analysis\n\n\nApplied Econometrics\nUsed regression, hypothesis testing, and time-series techniques to analyze economic and business data.\n\n  Regression\n  Time Series\n  Statistical Analysis\n\n\nDatabase Design & Implementation\nDesigned relational databases and wrote SQL queries to manage, retrieve, and analyze data efficiently.\n\n  SQL\n  Database Management\n  Relational Databases\n\n\nManagement Information Systems\nApplied information systems and database concepts to support business operations and decision-making.\n\n  SQL\n  Business Systems\n\n\nGame Theory Applications\nModeled strategic decision-making and competitive behavior using game-theoretic frameworks.\n\n  Strategic Analysis\n  Decision Theory\n  Economic Modeling\n\n\nIntermediate Corporate Finance\nAnalyzed capital structure, valuation, and investment decisions using financial theory and quantitative methods.\n\n  Valuation\n  Financial Analysis\n\n\nInvestment Securities & Markets\nStudied financial markets, asset pricing, and portfolio construction across major asset classes.\n\n  Financial Markets\n  Asset Pricing\n  Portfolio Analysis"
  },
  {
    "objectID": "pynotes.html",
    "href": "pynotes.html",
    "title": "Python Notebooks",
    "section": "",
    "text": "Assignment 3 - Statistical Testing\n\n\n\n\n\n\n\n\n\n\n\nData Exploration & Visualization\n\n\n\n\n\n\n\n\n\n\n\nMarket Segmentation & Conjoint Analysis\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "certifications.html",
    "href": "certifications.html",
    "title": "Certifications",
    "section": "",
    "text": "Verified certifications that demonstrate expertise in technology, data analytics, and financial markets.\n\n  \n    AWS Academy Cloud Foundations\n  \n\n  \n    Developed a strong understanding of cloud computing concepts, AWS services, architecture, security, and pricing, enhancing my ability to leverage cloud technologies for business innovation and digital transformation.\n\n    \n      AWS\n      Cloud Computing\n      Security\n      Digital Transformation\n    \n\n    \n      View Credential\n    \n  \n\n\n  \n    Bloomberg Market Concepts (BMC)\n  \n\n  \n    Completed the self-paced BMC certification, covering core pillars of the financial markets including economics, currencies, fixed income, and equities through the Bloomberg Terminal. Strengthened financial literacy, market analysis, and data-driven decision-making skills.\n\n    \n      Financial Markets\n      Equities\n      Fixed Income\n      Data Analysis\n    \n\n    \n      View Credential"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nhat Tran",
    "section": "",
    "text": "I’m a Business Analytics graduate student at Boston University, with hands-on experience in macro research, financial analytics, and data-driven decision-making. My academic foundation in finance and economics from Drexel University helps me bridge quantitative analysis with practical business insight.\nI specialize in:\n\nDesigning automated workflows for efficient, reproducible analytics\nBuilding predictive and explanatory models\nTransforming complex datasets into clear, actionable insights\nSupporting business and investment decisions with evidence-based recommendations\n\nFeel free to explore the site to learn more about my work, projects, and professional experience."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "I’m always happy to connect! You can reach me through any of the platforms below:\n\n💼 LinkedIn 💻 GitHub 📧 Email\n\n\nYou can also return to the About tab to learn more about me."
  },
  {
    "objectID": "contact.html#get-in-touch",
    "href": "contact.html#get-in-touch",
    "title": "Contact",
    "section": "",
    "text": "I’m always happy to connect! You can reach me through any of the platforms below:\n\n💼 LinkedIn 💻 GitHub 📧 Email\n\n\nYou can also return to the About tab to learn more about me."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Selected Projects",
    "section": "",
    "text": "1. Gender Disparities in Labor Force Analysis (2024)\nWebsite Research Paper PPT\n\nLabor Market Analytics Research Project | Sep 2025 – Dec 2025\n\nAnalyzed Lightcast and FRED data in Python, applying regression and ML models to forecast 2024 labor-market trends\nBuilt visualizations to assess skill demand, salary patterns, geographic variation, and gender-based differences across U.S. markets\nSynthesized model results with academic research to deliver actionable insights on hiring behavior and gender disparities\n\n\n\n\n2. Factors That Influence Airbnb Prices in Geneva\nResearch Paper PPT\n\nApplied Machine Learning for Airbnb Pricing in Geneva | Sep 2025 – Dec 2025\n\nApplied Python for data visualization, regression, k-NN, classification trees, NLP transformers, and clustering to analyze Airbnb pricing in Geneva\nEngineered features from structured and unstructured listing data to build predictive and market segmentation models\nGenerated actionable insights to inform Airbnb hosts’ pricing decisions and listing strategies, and to support platform-level market segmentation in the Geneva market\n\n\n\n\n3. Bangladesh Garment Industry: Health and Environmental Impacts\nResearch Paper\n\nEconomics Seminar Research Paper | Jan 2021 – Mar 2021\n\nConducted a literature review on short- and long-term effects of Bangladesh’s garment industry on health and environment\nExamined workers’ health, safety evaluations, and child labor issues using historical data\nProposed strategies to mitigate risks while supporting sustainable industry growth"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m currently pursuing my Master’s in Business Analytics (MSBA) at Boston University, where I’m focused on applying data to understand markets and improve decision-making. Prior to this, I studied finance and economics at Drexel University, which sparked my interest in how numbers tell the story behind business trends. I’ve worked on projects involving predictive modeling, marketing analytics, NLP, and data visualization, which has strengthened my ability to connect technical analysis with practical, real-world applications."
  },
  {
    "objectID": "about.html#hi-im-nhat",
    "href": "about.html#hi-im-nhat",
    "title": "About",
    "section": "",
    "text": "I’m currently pursuing my Master’s in Business Analytics (MSBA) at Boston University, where I’m focused on applying data to understand markets and improve decision-making. Prior to this, I studied finance and economics at Drexel University, which sparked my interest in how numbers tell the story behind business trends. I’ve worked on projects involving predictive modeling, marketing analytics, NLP, and data visualization, which has strengthened my ability to connect technical analysis with practical, real-world applications."
  },
  {
    "objectID": "about.html#skills",
    "href": "about.html#skills",
    "title": "About",
    "section": "Skills",
    "text": "Skills\n\nProgramming: Python; SQL; EViews; R\nData Analysis Tools: Bloomberg; Power BI; Tableau; Macrobond; Haver; Microsoft Office: Word, Excel, PowerPoint\nTechnical Skills: Data Visualizations; Data Manipulation; Database Management; Macro Research; ML Models"
  },
  {
    "objectID": "about.html#what-im-interested-in",
    "href": "about.html#what-im-interested-in",
    "title": "About",
    "section": "What I’m Interested In",
    "text": "What I’m Interested In\nI’m passionate about turning raw data into actionable insights that solve real-world business problems, uncover market trends, and create tools that make complex information easy to understand. Outside of work, I love exploring the world through travel, photography, and trying new cuisines, often visiting local restaurants to discover hidden gems."
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "About",
    "section": "Contact",
    "text": "Contact\nFor inquiries or collaboration, please visit Contact — happy to connect anytime :)"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "You can download my CV here: CV (PDF)"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Curriculum Vitae",
    "section": "Education",
    "text": "Education\nBoston University, Boston, MA\nM.S. in Business Analytics | Expected May 2026\nDrexel University, Philadelphia, PA\nB.S. in Economics | 2017 – 2021\nMinor: Finance | Concentration: Business Economics"
  },
  {
    "objectID": "cv.html#skills",
    "href": "cv.html#skills",
    "title": "Curriculum Vitae",
    "section": "Skills",
    "text": "Skills\n\nData Analysis Tools: Bloomberg, Power BI, Tableau, Macrobond, Haver, Microsoft Office (Word, Excel, PowerPoint)\n\nProgramming Languages: Python, SQL, Eviews, R\nTechnical Skills: Data Visualizations, Data Manipulation, Database Management, Macro Research, ML Models"
  },
  {
    "objectID": "cv.html#professional-experience",
    "href": "cv.html#professional-experience",
    "title": "Curriculum Vitae",
    "section": "Professional Experience",
    "text": "Professional Experience\n\nBrandywine Global Investment Management, Philadelphia, PA\nGlobal Macro Research Analyst | Feb 2022 – Aug 2024\n\nManaged and maintained large economic datasets and visualizations to support research and analysis.\n\nConducted macroeconomic studies and built models to guide investment strategies.\n\nDeveloped client presentations and marketing materials to communicate research insights.\n\n\n\n\nKPIM Joint Stock Company, Hanoi, Vietnam\nBusiness Intelligence Analyst | Mar 2021 – Sep 2021\n\nDeveloped dashboards and reports to monitor financial performance and support decision-making.\n\nAnalyzed business data to identify trends, opportunities, and potential risks.\n\nStreamlined reporting processes and coordinated with teams to ensure accuracy.\n\n\n\n\nErnst & Young LLP, Hanoi, Vietnam\nIntern – Assurance Services | May 2020 – Sep 2020\n\nPerformed financial analysis and evaluation of client performance.\nImplemented process improvements and extracted data-driven insights for advisory projects.\nCollaborated with cross-functional teams to complete engagements efficiently.\n\n\n\n\nCity of Philadelphia – Streets Department, Philadelphia, PA\nIntern – Office of the Deputy Commissioner of Streets | Apr 2019 – Sep 2019\n\nMonitored project budgets and maintained financial records.\n\nOrganized and processed data to enhance reporting accuracy.\nPrepared performance reports to inform departmental decision-making."
  },
  {
    "objectID": "python_notebooks/AD654_Nhat_Tran_Assignment_1.html",
    "href": "python_notebooks/AD654_Nhat_Tran_Assignment_1.html",
    "title": "Data Exploration & Visualization",
    "section": "",
    "text": "Analyzed Lobsterland’s 2025 visitor and revenue data using Python to uncover trends in attendance, spending, and event performance. Applied exploratory data analysis and data visualization techniques to generate actionable business insights."
  },
  {
    "objectID": "python_notebooks/AD654_Nhat_Tran_Assignment_1.html#part-i-exploratory-data-analysis-exploration-manipulation",
    "href": "python_notebooks/AD654_Nhat_Tran_Assignment_1.html#part-i-exploratory-data-analysis-exploration-manipulation",
    "title": "Data Exploration & Visualization",
    "section": "Part I: Exploratory Data Analysis: Exploration & Manipulation",
    "text": "Part I: Exploratory Data Analysis: Exploration & Manipulation\n\n# Importing necessary functions\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nlobster_25 = pd.read_csv(\"data/lobster_25.csv\")\n\n\nlobster_25.head()\n\n\n\n\n\n\n\n\nDate\nDay_of_Week\nseason_week_rel\nIs_Holiday\nTotal_Visitors\nPassholder_Percentage\nDay_Tickets_Sold\nAvg_Ticket_Price\nGate_Revenue\nRevenue_Food\n...\nWeather_Type\nWeather_Code\nPromo_Flag\nPromo_Type\nCruise_Docked\nIs_Special_Event\nSpecial_Events\nAttraction_Tier\nZone_ID\nPer_Capita_Spend\n\n\n\n\n0\n2025-05-26\nMonday\n1\n1\n1769\n48.65\n908\n77.42\n70297.36\n40521.91\n...\nThunderstorms\n4\n0\nNaN\n0\n0\nNaN\n2\n101\n73.43\n\n\n1\n2025-05-27\nTuesday\n1\n0\n1717\n60.58\n677\n69.80\n47254.60\n35298.92\n...\nPartly Cloudy\n2\n0\nNaN\n0\n0\nNaN\n2\n101\n59.30\n\n\n2\n2025-05-28\nWednesday\n1\n0\n1600\n30.84\n1107\n70.09\n77589.63\n36495.95\n...\nPartly Cloudy\n2\n0\nNaN\n0\n0\nNaN\n3\n103\n82.33\n\n\n3\n2025-05-29\nThursday\n1\n0\n1037\n19.02\n840\n72.30\n60732.00\n22486.65\n...\nShowers\n3\n0\nNaN\n0\n0\nNaN\n2\n101\n91.19\n\n\n4\n2025-05-30\nFriday\n1\n0\n1671\n66.18\n565\n68.11\n38482.15\n40705.15\n...\nPartly Cloudy\n2\n0\nNaN\n0\n0\nNaN\n1\n104\n58.67\n\n\n\n\n5 rows × 27 columns\n\n\n\n\nlobster_25.shape\n\n(99, 27)\n\n\nThere are 99 rows and 27 columns in this Lobsterland data set.\n\nCategorical variables: Date, Day of Week, season_week_rel, is_holiday, Precipitation, Weather Type, Weather Code, Promo Flag, Promo Type, Cruise Docked, Is_Special_Event, Special_Events, Attraction_Tier, Zone_ID\nNumeric variables: Total visitors, Passholder percentage, Day Tickets Sold, Avg Ticket Price, Gate Revenue, Revenue Food, Revenue Merch, Revenue Arcade, Total Revenue, Total Labor Hours, International Visitors, High temperature, Per Capita Spend\n\n\nChecking for NaN values.\n\nlobster_25.isnull().sum()\n\nDate                       0\nDay_of_Week                0\nseason_week_rel            0\nIs_Holiday                 0\nTotal_Visitors             0\nPassholder_Percentage      0\nDay_Tickets_Sold           0\nAvg_Ticket_Price           0\nGate_Revenue               0\nRevenue_Food               0\nRevenue_Merch              5\nRevenue_Arcade             0\nTotal_Revenue              0\nTotal_Labor_Hours          0\nInternational_Visitors     5\nHigh_Temperature           0\nPrecipitation              0\nWeather_Type               0\nWeather_Code               0\nPromo_Flag                 0\nPromo_Type                79\nCruise_Docked              0\nIs_Special_Event           0\nSpecial_Events            84\nAttraction_Tier            0\nZone_ID                    0\nPer_Capita_Spend           0\ndtype: int64\n\n\nTo check for NaN values, I used the code lobster25.isna().sum(). This showed the number of missing values in each column. The columns that contain missing values are Revenue_Merch with 5 missing entries, International_Visitors with 5 missing entries, Promo_Type with 79 missing entries, and Special_Events with 84 missing entries. These missing values are understandable because the columns represent optional events, promotions, or international visitors.\n\nFor numeric variables like Revenue_Merch and International_Visitors, which are recorded in numbers, NaN likely means no merchandise revenue or no international visitors that day, so it can be replaced with 0.\nAs for categorial variables like Promo_Type and Special_Events, NaN means no promotion/special event was active, so it’s better to fill them with descriptive text rather than 0. So, for Promo_Type, I replace NaN with “No Promo” and for Special_Events with “No Event” respectively.\n\n\n# Create and copy from the original dataframe so not to mess with it\nlobster25 = lobster_25.copy() \n\n# Filling numeric variables with NaN to 0\nlobster25['Revenue_Merch'] = lobster25['Revenue_Merch'].fillna(0.0)\nlobster25['International_Visitors'] = lobster25['International_Visitors'].fillna(0.0)\n\n\nlobster25[\"Promo_Type\"] = lobster25[\"Promo_Type\"].fillna(\"No Promo\")\nlobster25[\"Special_Events\"] = lobster25[\"Special_Events\"].fillna(\"No Event\")\n\n\n# Double-checking to see if there's still NaN in the data set\nlobster25.isnull().sum()\n\nDate                      0\nDay_of_Week               0\nseason_week_rel           0\nIs_Holiday                0\nTotal_Visitors            0\nPassholder_Percentage     0\nDay_Tickets_Sold          0\nAvg_Ticket_Price          0\nGate_Revenue              0\nRevenue_Food              0\nRevenue_Merch             0\nRevenue_Arcade            0\nTotal_Revenue             0\nTotal_Labor_Hours         0\nInternational_Visitors    0\nHigh_Temperature          0\nPrecipitation             0\nWeather_Type              0\nWeather_Code              0\nPromo_Flag                0\nPromo_Type                0\nCruise_Docked             0\nIs_Special_Event          0\nSpecial_Events            0\nAttraction_Tier           0\nZone_ID                   0\nPer_Capita_Spend          0\ndtype: int64\n\n\nI replaced NaNs in Revenue_Merch and International_Visitors with 0, assuming missing values indicate no revenue or no visitors. For Promo_Type and Special_Events, I used “No Promo” and “No Event” because missing entries mean no promotion or event occurred. I made these choices by considering what each column represents. This approach makes the dataset more informative and descriptive.\n\n\nRename the ‘Food_Rev’ column to ‘FoodBev_Rev’.\n\nlobster25 = lobster25.rename(columns={'Revenue_Food':'Revenue_FoodBev'}) #rename the columns\n# There is no 'Food_Rev' so I'm assuming it's 'Revenue_Food' and replace it with 'Revenue_FoodBev'\nprint(lobster25.columns)\n\nIndex(['Date', 'Day_of_Week', 'season_week_rel', 'Is_Holiday',\n       'Total_Visitors', 'Passholder_Percentage', 'Day_Tickets_Sold',\n       'Avg_Ticket_Price', 'Gate_Revenue', 'Revenue_FoodBev', 'Revenue_Merch',\n       'Revenue_Arcade', 'Total_Revenue', 'Total_Labor_Hours',\n       'International_Visitors', 'High_Temperature', 'Precipitation',\n       'Weather_Type', 'Weather_Code', 'Promo_Flag', 'Promo_Type',\n       'Cruise_Docked', 'Is_Special_Event', 'Special_Events',\n       'Attraction_Tier', 'Zone_ID', 'Per_Capita_Spend'],\n      dtype='object')\n\n\n\n\nIn the dataset, there are four components of total revenue: gate revenue, revenue food, revenue merch, and revenue arcade. Generate a table that shows average spending in each of those areas, based on Special Event types.\n\navg_rev_by_event = lobster25.groupby('Special_Events')[['Gate_Revenue','Revenue_FoodBev','Revenue_Merch','Revenue_Arcade']].mean()\nprint(avg_rev_by_event)\n\n                         Gate_Revenue  Revenue_FoodBev  Revenue_Merch  \\\nSpecial_Events                                                          \nBeatles Tribute         146630.870000     86767.966667   21431.026667   \nMusicFest               107296.750000     83544.713333   33065.490000   \nNight Glow              107957.660000     59590.776667   31730.376667   \nNo Event                 73213.523333     40692.877500   15931.317500   \nTaylor Swift Lookalike   96895.336667     67165.716667   33479.076667   \nVintage Days            114238.410000     51074.406667   24561.976667   \n\n                        Revenue_Arcade  \nSpecial_Events                          \nBeatles Tribute           10488.050000  \nMusicFest                  9745.910000  \nNight Glow                 8637.820000  \nNo Event                   3624.482500  \nTaylor Swift Lookalike     9691.783333  \nVintage Days               6821.316667  \n\n\nTable analysis: Stand out points & limitations  Looking at the table, it’s clear that special events like Beatles Tribute and MusicFest bring in noticeably higher revenue across all areas compared to days with no events. This makes sense, as popular events tend to draw bigger crowds and encourage people to spend more on tickets, food, and merchandise. Some of the standout numbers are probably because certain events are more well-known or heavily promoted. That said, this analysis only looks at averages and doesn’t consider other factors like weather, day of the week, or seasonal trends. Because of these other influences, it’s hard to say for sure how much of the revenue boost is truly caused by the events themselves.\n\n\nA ‘Date’ variable type. Use the info() function to determine the data type for the‘Date’ variable.\n\nlobster25['Date'].info()\n\n&lt;class 'pandas.core.series.Series'&gt;\nRangeIndex: 99 entries, 0 to 98\nSeries name: Date\nNon-Null Count  Dtype \n--------------  ----- \n99 non-null     object\ndtypes: object(1)\nmemory usage: 924.0+ bytes\n\n\nPython currently views this variable as an object.\n\n# Converting the variable into a datetime object.\nlobster25['Date'] = pd.to_datetime(lobster25['Date'])\n\n# Check to see if the conversion is successful\nprint(lobster25['Date'].dtype)\n\ndatetime64[ns]\n\n\nConverting a column to datetime lets Python treat it as a real date, enabling operations like filtering by month, extracting weekdays, calculating durations, and plotting time-series data accurately. Without it, dates are just text and harder to analyze.\n\n\nUsing the groupby() function, along with the describe() function, explore the relationship between day of the week and total revenue.\n\nlobster25.groupby('Day_of_Week')[['Total_Revenue']].describe()\n\n\n\n\n\n\n\n\nTotal_Revenue\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nDay_of_Week\n\n\n\n\n\n\n\n\n\n\n\n\nFriday\n14.0\n124348.351429\n25766.281104\n68899.07\n113828.1850\n126337.460\n134174.9625\n180408.42\n\n\nMonday\n15.0\n123881.241333\n30711.357993\n88753.13\n101210.0300\n110997.990\n143166.6050\n202565.15\n\n\nSaturday\n14.0\n233435.688571\n63186.045667\n132174.43\n209613.2550\n227397.180\n238614.4925\n423019.37\n\n\nSunday\n14.0\n222040.763571\n51689.800794\n127310.71\n211008.1775\n245217.160\n251046.1250\n284520.15\n\n\nThursday\n14.0\n110808.970000\n23883.366451\n63247.57\n94773.4125\n112346.540\n128805.5325\n147562.14\n\n\nTuesday\n14.0\n110803.409286\n28858.768417\n64017.91\n95816.2750\n106621.260\n132957.8850\n166042.42\n\n\nWednesday\n14.0\n111780.540000\n24591.350227\n75302.24\n92629.4175\n111242.325\n127296.6300\n165467.64\n\n\n\n\n\n\n\nLooking at the table, it’s pretty noticeable that weekends bring in much higher revenue than weekdays. Saturday averages around $233,000 and Sunday about $222,000, while most weekdays hover between $110,000 and $125,000. This makes sense since more people are likely to visit the park on their days off. The bigger variation in weekend revenues probably comes from special events or particularly busy days. Overall, the day of the week has a noticeable effect, with weekends clearly driving the biggest crowds and sales.\n\n\nLobster Land management has decided that they won’t be using the Zone_ID variable. Remove it, and demonstrate that it’s no longer part of the dataset going forward.\n\nlobster25 = lobster25.drop(columns=['Zone_ID'], errors='ignore')\n# Double-check\nprint(lobster25.columns)\n\nIndex(['Date', 'Day_of_Week', 'season_week_rel', 'Is_Holiday',\n       'Total_Visitors', 'Passholder_Percentage', 'Day_Tickets_Sold',\n       'Avg_Ticket_Price', 'Gate_Revenue', 'Revenue_FoodBev', 'Revenue_Merch',\n       'Revenue_Arcade', 'Total_Revenue', 'Total_Labor_Hours',\n       'International_Visitors', 'High_Temperature', 'Precipitation',\n       'Weather_Type', 'Weather_Code', 'Promo_Flag', 'Promo_Type',\n       'Cruise_Docked', 'Is_Special_Event', 'Special_Events',\n       'Attraction_Tier', 'Per_Capita_Spend'],\n      dtype='object')"
  },
  {
    "objectID": "python_notebooks/AD654_Nhat_Tran_Assignment_1.html#part-ii-visualization",
    "href": "python_notebooks/AD654_Nhat_Tran_Assignment_1.html#part-ii-visualization",
    "title": "Data Exploration & Visualization",
    "section": "Part II: Visualization",
    "text": "Part II: Visualization\n\nUsing any plotting tool in Python, generate a bar plot that shows Promo_Type on one axis and average gate revenue on the other. Be sure to give your plot a clear, descriptive title.\n\nsns.barplot(x = \"Promo_Type\", y = \"Gate_Revenue\", data = lobster25, hue=\"Promo_Type\", palette=\"Set2\", dodge=False, edgecolor='black');\nplt.xlabel(\"Promotion Type\")\nplt.ylabel(\"Avg. Gate Revenue ($)\")\nplt.title(\"Lobsterland's Average Gate Revenue during Promotion Types\")\nplt.show()\n\n\n\n\n\n\n\n\nThe bar plot shows that days with no promotion, email, or influencer campaigns have similar average gate revenue, while bundle promotions have noticeably lower revenue. Surprisingly, bundles seem less effective than doing nothing or running other promotions, possibly because they attract fewer people or are offered on slower days. Overall, not all promotions necessarily increase gate revenue.\n\n\nLobster Land management wants to better understand how visitor counts vary across the days of the week. Generate a barplot that shows average total visitors by day of Week.\n\n# Version 1: y-axis starting at 0\nsns.barplot(x = \"Day_of_Week\", y = \"Gate_Revenue\", data = lobster25, color='skyblue', edgecolor='black');\nplt.xlabel(\"Day of the Week\")\nplt.ylabel(\"Avg Gate Revenue ($)\")\nplt.title(\"Lobsterland's Average Gate Revenue on Days of the Week\")\nplt.ylim(0)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Version 2: y-axis zoomed to highlight differences\n\navg_values = lobster25.groupby(\"Day_of_Week\")[\"Gate_Revenue\"].mean()\n\nsns.barplot(x=\"Day_of_Week\", y=\"Gate_Revenue\", data=lobster25, color='skyblue', edgecolor='black')\nplt.xlabel(\"Day of the Week\")\nplt.ylabel(\"Avg Gate Revenue ($)\")\nplt.title(\"Lobsterland's Average Gate Revenue on Days of the Week\")\n\n# Set y-axis limits slightly beyond min and max average values\nplt.ylim(avg_values.min() * 0.95, avg_values.max() * 1.05)\nplt.show()\n\n\n\n\n\n\n\n\nBoth plots display the same visitor data, but they feel very different. The first plot, with the y-axis starting at 0, shows the weekend boost in visitors clearly but keeps the differences in perspective. The second plot, with a zoomed-in y-axis, makes those differences look much larger and more dramatic than they really are. This kind of change can easily influence how people interpret the importance of the trend. That’s why, for bar charts, it’s usually best to start the axis at 0 — it helps present the data honestly and avoids misleading your audience.\n\n\nNext, generate a histogram of per capita spend across all visitors in the 2025 season. Be sure to give your plot a clear, descriptive title.\n\n#Version 1: Non-adjusted bins\nsns.histplot(data=lobster25, x=\"Per_Capita_Spend\", color='skyblue', edgecolor='black')\nplt.xlabel(\"Per Capita Spend ($)\")\nplt.title(\"Lobsterland Per Capita Spend in 2025\")\nplt.show()\n\n\n\n\n\n\n\n\nThe histogram shows that most visitors spend between about $70 and $85, with spending peaking around $80. The distribution is roughly bell-shaped, suggesting that per capita spending is fairly consistent across visitors, with only a few spending much lower or higher than the average.\n\n#Version 2: 50 bins\nsns.histplot(data=lobster25, x=\"Per_Capita_Spend\", bins=50, color='skyblue', edgecolor='black')\nplt.xlabel(\"Per Capita Spend ($)\")\nplt.title(\"Lobsterland Per Capita Spend in 2025\")\nplt.show()\n\n\n\n\n\n\n\n\nThe second histogram, with 50 bins, shows a lot more detail about how people spend. Instead of the smooth, bell-like shape in the first plot, you can see smaller peaks and dips, almost like there are a few “favorite” spending amounts that visitors cluster around. Someone looking at this version might notice that spending habits are a bit more varied and not perfectly smooth, which isn’t as clear in the first histogram.\n\n\nOne member of the Lobster Land Board of Directors thinks that people tend to buy more merch on hot days. Let’s see whether the data supports this theory! Generate a scatterplot with the day’s high temperature on the x-axis, and with total merch revenue on the y-axis. Be sure to give your plot a clear, descriptive title.\n\nsns.scatterplot(x = \"High_Temperature\", y = \"Revenue_Merch\", data = lobster25, color='skyblue');\nplt.xlabel(\"High Temperature\")\nplt.ylabel(\"Avg. Merch Revenue ($)\")\nplt.title(\"Lobsterland's Merch Revenue on Hot Temperature Day\")\nplt.show()\n\n\n\n\n\n\n\n\nThe scatterplot shows no clear relationship between high temperature and merch revenue — the points are spread fairly randomly across the temperature range. This suggests that hotter days do not consistently lead to higher merch sales. It’s possible that factors like crowd size, special events, or promotions play a bigger role in driving merch revenue than temperature alone.\n\n\nCreate a boxplot that shows day of week on one axis and passholder percentage on the other. Be sure to give your plot a clear, descriptive title.\n\nsns.boxplot(x=\"Day_of_Week\", y=\"Passholder_Percentage\", data=lobster25, color='skyblue')\nplt.xlabel(\"Day of the Week\")\nplt.ylabel(\"Passholder Percentage (%)\")\nplt.title(\"Lobsterland's Passholder Percentage during Day of the Week\")\nplt.show()\n\n\n\n\n\n\n\n\nThe boxplot shows that weekdays, especially Wednesday and Thursday, tend to have higher passholder percentages compared to weekends. Saturday has the lowest median and a narrower range, meaning fewer passholders visit that day. This might be because weekends attract more casual or one-time visitors, while passholders prefer visiting during less crowded weekdays.\n\n\nCreate a barplot that shows average daily total visitors by month.\n\n#create a new 'Month' column extracted from 'Date'\nlobster25['Month'] = lobster25['Date'].dt.month_name()\n\nmonthly_avg = lobster25.groupby('Month', as_index=False)['Total_Visitors'].mean()\n\nsns.barplot(x=\"Month\", \n            y=\"Total_Visitors\", \n            data=monthly_avg, \n            hue=\"Month\", \n            palette=\"Set2\", \n            dodge=False, \n            edgecolor='black')\nplt.xlabel(\"Month\")\nplt.ylabel(\"Avg. Daily Total Visitors\")\nplt.title(\"Average Daily Total Visitors at Lobsterland by Month\")\nplt.show()\n\n\n\n\n\n\n\n\nThe bar plot shows that average daily visitors are highest in September and lowest in May and July. There’s a clear upward trend from August into September, suggesting strong late-season attendance. I would tell management that September appears to be a key month for drawing visitors, which could be a good time to focus marketing campaigns or launch special fall events to maximize engagement and revenue.\n\n\nGenerate a barplot that shows special events on one axis and average international visitors on the other.\n\navg_visitor = lobster25.groupby(\"Special_Events\")[\"International_Visitors\"].mean().sort_values(ascending=False)\nprint(avg_visitor)\n\nSpecial_Events\nTaylor Swift Lookalike    236.666667\nMusicFest                 201.333333\nBeatles Tribute           194.000000\nVintage Days              178.666667\nNight Glow                172.000000\nNo Event                  167.738095\nName: International_Visitors, dtype: float64\n\n\n\norder_event = avg_visitor.index\nprint(order_event)\n\nIndex(['Taylor Swift Lookalike', 'MusicFest', 'Beatles Tribute',\n       'Vintage Days', 'Night Glow', 'No Event'],\n      dtype='object', name='Special_Events')\n\n\n\nsns.barplot(x=\"Special_Events\", \n            y=\"International_Visitors\", \n            data=lobster25, \n            order=order_event, \n            errorbar=None, \n            hue=\"Special_Events\", \n            palette=\"Set3\", \n            dodge=False, \n            edgecolor='black')\nplt.xlabel(\"Special Events\")\nplt.ylabel(\"Avg. International Visitors\")\nplt.xticks(rotation=45)\nplt.title(\"Lobsterland's Average International Visitors on Special Events\")\nplt.show()\n\n\n\n\n\n\n\n\nThe barplot shows that the Taylor Swift Lookalike event attracts the highest average number of international visitors, followed by MusicFest and Beatles Tribute. Days with no events have the lowest international attendance. This suggests that special events, especially music-related ones, may encourage more international guests to visit. However, we cannot assume the events directly cause the increase, other factors like peak tourist season or holidays could also explain the higher numbers. This is why the relationship should be viewed as a correlation, not proof of causation.\n\n\nCreate a faceted bar plot of food & beverage revenue, with facets for weather type, and bars representing day of the week.\n\nsns.catplot(x=\"Day_of_Week\", \n            y=\"Revenue_FoodBev\", \n            col=\"Weather_Type\", \n            data=lobster25, \n            kind=\"bar\",\n            color='skyblue', \n            edgecolor='black',\n            errorbar=None).fig.suptitle(\"Food & Beverage Revenue by Day of the Week, Faceted by Weather Type\", fontsize=16, y=1.05)\nplt.xlabel(\"Day of the Week\")\nplt.ylabel(\"Food & Beverage Revenue\")\nplt.show()\n\n\n\n\n\n\n\n\nThis plot shows that food and beverage sales really take off on weekends, especially when the weather is nice or just a bit cloudy. Thunderstorms seem to keep people away no matter the day, while sunny or partly cloudy Saturdays and Sundays bring in the biggest revenue. It’s interesting that even rainy weekends still perform better than most weekdays. However, this visualization does not show the number of observations (sample size) for each weather-day combination, making it hard to know whether some patterns are based on only a few data points.\n\n\nCreate a lineplot that shows season_week_rel on the x-axis, and aggregate (total) Gold Zone spending for that week on the y-axis.\n\nlobster25[['Attraction_Tier','Per_Capita_Spend']]\n\n\n\n\n\n\n\n\nAttraction_Tier\nPer_Capita_Spend\n\n\n\n\n0\n2\n73.43\n\n\n1\n2\n59.30\n\n\n2\n3\n82.33\n\n\n3\n2\n91.19\n\n\n4\n1\n58.67\n\n\n...\n...\n...\n\n\n94\n2\n81.26\n\n\n95\n1\n79.43\n\n\n96\n3\n82.08\n\n\n97\n2\n68.70\n\n\n98\n1\n84.26\n\n\n\n\n99 rows × 2 columns\n\n\n\n\n# Filter attraction tier to gold zone which is 1, copy() from lobster25() dataframe\ngold_zone = lobster25.query('Attraction_Tier==1').copy() \n\n# Double-check to see if there's any zone other than 1\nprint(gold_zone['Attraction_Tier'].unique())\n\n[1]\n\n\n\n# Add a 'Total_Spend' column to record the total spending of gold zone by multiplying total visitors by per visitor spend\ngold_zone['Total_Spend'] = gold_zone['Total_Visitors']*gold_zone['Per_Capita_Spend']\n\n# Double-check to see if the tier and the calculation is correct\ngold_zone[['Attraction_Tier','Total_Spend','season_week_rel']].tail()\n\n\n\n\n\n\n\n\nAttraction_Tier\nTotal_Spend\nseason_week_rel\n\n\n\n\n88\n1\n131917.20\n13\n\n\n89\n1\n209116.95\n13\n\n\n91\n1\n110994.19\n14\n\n\n95\n1\n138287.63\n14\n\n\n98\n1\n202561.04\n15\n\n\n\n\n\n\n\n\n# Aggregrate the total spending of gol\ngold_spend_weekly = gold_zone.groupby('season_week_rel')['Total_Spend'].sum().reset_index()\n\n# Plot the line chart\nsns.lineplot(data=gold_spend_weekly, x='season_week_rel',y='Total_Spend', color='skyblue')\nplt.xlabel(\"Week of the Season\")\nplt.ylabel(\"Total Spending\")\nplt.title(\"Lobsterland's Gold Zone Weekly Total Spending of the Season\")\nplt.show()\n\n\n\n\n\n\n\n\nThis line plot shows that Gold Zone spending starts low during opening week and rises quickly, peaking around Weeks 4–5. After that, there’s a big drop, though Week 7 shows a nice little rebound before things taper off again toward the end of the season. This tells us guests spend the most when excitement is fresh and maybe when special events happen. I’d suggest management double down on promotions early in the season and figure out what drove that Week 7 spike so we can recreate it."
  },
  {
    "objectID": "python_notebooks/AD654_Nhat_Tran_Assignment_3.html",
    "href": "python_notebooks/AD654_Nhat_Tran_Assignment_3.html",
    "title": "Assignment 3 - Statistical Testing",
    "section": "",
    "text": "Nhat Tran  METAD654: Marketing Analytics  Professor Gregory Page"
  },
  {
    "objectID": "python_notebooks/AD654_Nhat_Tran_Assignment_3.html#part-i-ab-testing-for-user-spending",
    "href": "python_notebooks/AD654_Nhat_Tran_Assignment_3.html#part-i-ab-testing-for-user-spending",
    "title": "Assignment 3 - Statistical Testing",
    "section": "Part I: A/B Testing for User Spending",
    "text": "Part I: A/B Testing for User Spending\n\n# Import necessary functions to use\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom scipy.stats import chisquare\n\n\nlobster_run = pd.read_csv(\"/Users/nhattran/Downloads/MSBA/METAD654 - Marketing Analytics/assignment 3/lobster_run.csv\")\n\n\nlobster_run.head()\n\n\n\n\n\n\n\n\nuserid\nversion\nsum_gamerounds\nretention_1\nretention_7\nuser_spend\n\n\n\n\n0\n1\nGulf of Maine\n3\nFalse\nFalse\n17.09\n\n\n1\n2\nGulf of Maine\n38\nTrue\nFalse\n15.30\n\n\n2\n7\nGulf of Maine\n0\nFalse\nFalse\n13.67\n\n\n3\n12\nGulf of Maine\n0\nFalse\nFalse\n16.92\n\n\n4\n14\nGulf of Maine\n39\nTrue\nFalse\n8.42\n\n\n\n\n\n\n\n\nlobster_run.isnull().sum()\n\nuserid            0\nversion           0\nsum_gamerounds    0\nretention_1       0\nretention_7       0\nuser_spend        0\ndtype: int64\n\n\n\nlobster_run.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 90189 entries, 0 to 90188\nData columns (total 6 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   userid          90189 non-null  int64  \n 1   version         90189 non-null  object \n 2   sum_gamerounds  90189 non-null  int64  \n 3   retention_1     90189 non-null  bool   \n 4   retention_7     90189 non-null  bool   \n 5   user_spend      90189 non-null  float64\ndtypes: bool(2), float64(1), int64(2), object(1)\nmemory usage: 2.9+ MB\n\n\n\n# Change datatype of version in the dataset to categorical variable\nlobster_run[['version']] = lobster_run[['version']].astype('category')\nlobster_run.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 90189 entries, 0 to 90188\nData columns (total 6 columns):\n #   Column          Non-Null Count  Dtype   \n---  ------          --------------  -----   \n 0   userid          90189 non-null  int64   \n 1   version         90189 non-null  category\n 2   sum_gamerounds  90189 non-null  int64   \n 3   retention_1     90189 non-null  bool    \n 4   retention_7     90189 non-null  bool    \n 5   user_spend      90189 non-null  float64 \ndtypes: bool(2), category(1), float64(1), int64(2)\nmemory usage: 2.3 MB\n\n\n\nlobster_run.describe()\n\n\n\n\n\n\n\n\nuserid\nsum_gamerounds\nuser_spend\n\n\n\n\ncount\n90189.000000\n90189.000000\n90189.000000\n\n\nmean\n45095.000000\n51.872457\n15.015899\n\n\nstd\n26035.466051\n195.050858\n2.747268\n\n\nmin\n1.000000\n0.000000\n4.050000\n\n\n25%\n22548.000000\n5.000000\n13.170000\n\n\n50%\n45095.000000\n16.000000\n15.010000\n\n\n75%\n67642.000000\n51.000000\n16.870000\n\n\nmax\n90189.000000\n49854.000000\n28.000000\n\n\n\n\n\n\n\n\n# Show the row with the maximum sum_gamerounds\nmax_rounds = lobster_run.loc[lobster_run['sum_gamerounds'].idxmax()]\nprint(max_rounds)\n\nuserid                    57703\nversion           Gulf of Maine\nsum_gamerounds            49854\nretention_1               False\nretention_7               False\nuser_spend                13.36\nName: 28650, dtype: object\n\n\n\n# Filter rows where sum_gamerounds &gt; 2000\nhigh_rounds = lobster_run[lobster_run['sum_gamerounds'] &gt; 2000]\nprint(high_rounds)\n\n       userid         version  sum_gamerounds  retention_1  retention_7  \\\n3927     7913   Gulf of Maine            2961         True        False   \n21685   43672   Gulf of Maine            2438         True        False   \n23037   46345   Gulf of Maine            2251         True        False   \n28650   57703   Gulf of Maine           49854        False        False   \n43118   87008   Gulf of Maine            2156         True        False   \n47967    6537  North Atlantic            2015         True         True   \n59444   29418  North Atlantic            2640         True        False   \n63193   36934  North Atlantic            2124         True         True   \n68948   48189  North Atlantic            2294         True         True   \n89253   88329  North Atlantic            2063         True         True   \n\n       user_spend  \n3927        11.28  \n21685       12.57  \n23037       18.00  \n28650       13.36  \n43118       17.68  \n47967       10.41  \n59444       15.47  \n63193       15.37  \n68948       16.28  \n89253       18.10  \n\n\n\n# Filter rows where sum_gamerounds &gt; 3000\nhigh_rounds = lobster_run[lobster_run['sum_gamerounds'] &gt; 3000]\nprint(high_rounds)\n\n       userid        version  sum_gamerounds  retention_1  retention_7  \\\n28650   57703  Gulf of Maine           49854        False        False   \n\n       user_spend  \n28650       13.36  \n\n\n\nmax_spend = lobster_run.loc[lobster_run['user_spend'].idxmax()]\nprint(max_spend)\n\nuserid                     35202\nversion           North Atlantic\nsum_gamerounds                42\nretention_1                False\nretention_7                False\nuser_spend                  28.0\nName: 62331, dtype: object\n\n\nAfter running some basic checks on the dataset, I noticed an extreme outlier in sum_gamerounds - a value of 49,854 for userid = 57703, which is the only record exceeding 3,000 rounds. What’s interesting is that this user’s user_spend is actually lower than the average, and they didn’t return to the app after one day (retention_1) or seven days (retention_7) following the download. This would mean the user supposedly completed nearly 50,000 game rounds in a single day, which is highly unrealistic. It’s likely due to a data error or possibly a case of hacking. Because this outlier is so extreme compared to other high-ranking users and this is a large dataset (90189 entries), I decided to remove it to avoid distorting the analysis.\n\nlobster_run = lobster_run[lobster_run['sum_gamerounds'] &lt;= 3000]\n\n\nA. Generate a histogram to view the user_spend variable.\n\nplt.figure(figsize=(10, 6))\nplt.hist(lobster_run['user_spend'], bins=50, color='skyblue', edgecolor='black')\nplt.title('Distribution of User Spend')\nplt.xlabel('User Spend (USD)')\nplt.ylabel('Number of Users')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\na. Describe your plot in 1-2 sentences.\nThe Distribution of User Spend histogram shows a clear bell-shaped pattern centered around $15, indicating that most users spend roughly this amount in the app. The distribution is fairly symmetric, with fewer users spending much less or more - ranging from less than \\$5 to more than \\$25, suggesting that spending behavior is consistent across the player base. Understanding this distribution can help the Lobster Land team design pricing strategies and in-app offers that align with typical user spending habits.\n\n\n\nB. Generate another histogram that depicts user_spend, but this time, use version as a color or hue variable.\n\n# Plot histogram with hue by version\nplt.figure(figsize=(10, 6))\nsns.histplot(data=lobster_run, x='user_spend', hue='version', bins=50, palette='Set2', multiple='stack', stat='count', edgecolor='black')\nplt.title('User Spend Distribution by Version')\nplt.xlabel('User Spend (USD)')\nplt.ylabel('Number of Users')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\na. Describe your plot in 1-2 sentences.\nThe User Spend Distribution by Version histogram shows the distribution of user spend by Gulf of Maine vs. North Atlantic version, with both versions following a similar bell-shaped pattern centered around $15. However, users of the Gulf of Maine version appear to spend slightly more on average than those using the North Atlantic version. This suggests that the Gulf of Maine version may encourage higher in-app spending or greater engagement among players. The overall similarity shape between the two distributions indicates that spending behavior is consistent across versions, with only modest differences in average spend.\n\n\n\nC. Conduct a statistical test to check whether the user_spend variable is normally distributed.\n\n# Run statistical test to check if user_spend is normally distributed\nuspend_result = stats.normaltest(lobster_run['user_spend'])\n\nprint(f\"Statistic = {uspend_result.statistic:.4f}\")\nprint(f\"p-value   = {uspend_result.pvalue:.4f}\")\n\nStatistic = 0.2353\np-value   = 0.8890\n\n\n\na. What is your test’s null hypothesis?\nThe test’s null hypothesis:  H0: The user_spend variable is normally distributed.  H1: The user_spend variable is not normally distributed. \nThis hypothesis tests whether the observed distribution of user_spend significantly deviates from a normal distribution, which is important for determining if standard statistical methods can be appropriately applied. Under H0, any deviation from normality is due to random variation, implying the data follow a normal pattern. Under H1, the deviations are systematic, indicating that user spend is not normally distributed.\n\nalpha = 0.05\nif uspend_result.pvalue &gt; alpha:\n    print(\"Fail to reject H0: user_spend looks normally distributed.\")\nelse:\n    print(\"Reject H0: user_spend is not normally distributed.\")\n\nFail to reject H0: user_spend looks normally distributed.\n\n\n\n\nb . Based on the p-value that you see as a result, what can you say about the distribution of this variable?\nSince the p-value = 0.8890, much greater than the common significance level of 0.05 - the test fail to reject the null hypothesis. This indicates that there is no strong statistical evidence suggesting the Lobster Land’s user spending deviates from normality.\n\n\n\nD. Next, conduct an appropriate statistical test to check whether version has a significant impact on user spending. You can use any alpha value that you prefer to use for the significance threshold.\n\n# Extract places from version where player begins\ngulf_of_maine_spend = lobster_run[lobster_run['version'] == 'Gulf of Maine']['user_spend']\nnorth_atlantic_spend = lobster_run[lobster_run['version'] == 'North Atlantic']['user_spend']\n\n# Perform one-way Analysis of Variance (ANOVA) to compare user_spend across 2 versions\nversion_anova = stats.f_oneway(gulf_of_maine_spend, north_atlantic_spend)\nprint(f\"Statistic = {version_anova.statistic:.4f}\")\nprint(f\"p-value   = {version_anova.pvalue:.4f}\")\n\nStatistic = 2.5264\np-value   = 0.1120\n\n\n\na. What was your null hypothesis?\nThe test’s null hypothesis:  H0: The mean user_spend is equal between Gulf of Maine and North Atlantic versions.  H1: The mean user_spend is different between the two versions.  In this analysis, I tested whether the product version (Gulf of Maine vs. North Atlantic) had a statistically significant effect on user spending. The null hypothesis (H0) assumed that the average user_spend was the same for both versions.\n\nalpha = 0.05\nif version_anova.pvalue &gt; 0.05:\n    print(\"Fail to reject H0: No significant difference in user_spend between versions.\")\nelse:\n    print(\"Reject H0: Version has a significant impact on user_spend.\")\n\nFail to reject H0: No significant difference in user_spend between versions.\n\n\n\n\nb . Based on the test, what will you conclude?\nThe ANOVA is chosen in this case because it allows me to compare the means of the user_spend variable across two product versions (Gulf of Maine and North Atlantic) to see if any observed differences are statistically significant. The test produced an F-statistic of 2.5264 and a p-value of 0.1120. Since the p-value is greater than the 0.05 significance level, I failed to reject the null hypothesis. This suggests that there isn’t a meaningful difference in average user spending between the two product versions, and any variation observed is likely due to random chance rather than an actual version effect."
  },
  {
    "objectID": "python_notebooks/AD654_Nhat_Tran_Assignment_3.html#part-ii-ab-testing-two-app-versions",
    "href": "python_notebooks/AD654_Nhat_Tran_Assignment_3.html#part-ii-ab-testing-two-app-versions",
    "title": "Assignment 3 - Statistical Testing",
    "section": "Part II: A/B Testing Two App Versions",
    "text": "Part II: A/B Testing Two App Versions\n\nE. Next, generate some summary stats here – group the observations by ’version’and then compare the retention_1 and the retention_7 outcomes.\n\nretention_summary = lobster_run.groupby('version',observed=True)[['retention_1', 'retention_7']].mean().round(6)\nprint(retention_summary)\n\n                retention_1  retention_7\nversion                                 \nGulf of Maine      0.448198      0.19562\nNorth Atlantic     0.442283      0.18200\n\n\n\na. How would you describe these results in general?\nFrom the results, the average retention rates for both Day 1 and Day 7 are slightly higher in the Gulf of Maine version compared to the North Atlantic version. - Day 1 retention: Gulf of Maine (44.82%) vs. North Atlantic (44.23%) - Day 7 retention: Gulf of Maine (19.56%) vs. North Atlantic (18.20%) \nAlthough the differences are minimal, the fact that they appear at both time points hints at a subtle but consistent pattern. Users of the Gulf of Maine version might be finding something slightly more engaging or user-friendly, which helps keep them around a bit longer. While this isn’t enough to draw firm conclusions without a statistical test, the trend suggests there could be small design or content differences that make the Gulf of Maine version more appealing over time.\n\n\n\nF. Use a binomial z-test to compare retention_1 between the two versions of the game.\n\nfrom statsmodels.stats.proportion import proportions_ztest\n# Count retained users per version\nretained_count_rt1 = lobster_run.groupby('version', observed=True)['retention_1'].sum()\nprint(\"Retained users (retention_1):\")\nprint(retained_count_rt1)\n\n# Count total users per version\nuser_total_rt1 = lobster_run.groupby('version', observed=True)['retention_1'].count()\nprint(\"\\nTotal users per version (retention_1):\")\nprint(user_total_rt1)\n\nRetained users (retention_1):\nversion\nGulf of Maine     20034\nNorth Atlantic    20119\nName: retention_1, dtype: int64\n\nTotal users per version (retention_1):\nversion\nGulf of Maine     44699\nNorth Atlantic    45489\nName: retention_1, dtype: int64\n\n\nFor retention day one, Gulf of Maine retained 20,034 out of 44,699 users, while North Atlantic retained 20,119 out of 45,489 users, showing nearly identical retention performance across both versions. This suggests that initial engagement is consistent regardless of which version users started with.\n\n# Run binomial z-test to compare the 2 versions\nz_stat1, p_value1 = proportions_ztest(count=retained_count_rt1, nobs=user_total_rt1)\nprint(\"Binomial Z-Test Results (retention_1):\")\nprint(f\"Z-statistic = {z_stat1:.4f}\")\nprint(f\"p-value     = {p_value1:.4f}\")\n\nBinomial Z-Test Results (retention_1):\nZ-statistic = 1.7871\np-value     = 0.0739\n\n\n\nalpha = 0.05\nif p_value1 &gt; 0.05:\n    print(\"Fail to reject H0: No significant difference in day-1 retention between versions.\")\nelse:\n    print(\"Reject H0: Version has a significant impact on day-1 retention.\")\n\nFail to reject H0: No significant difference in day-1 retention between versions.\n\n\n\na. What will be the null hypothesis of your test?\nThe test’s null hypothesis:  H0: Day-1 retention rates are equal between Gulf of Maine and North Atlantic versions.  H1: Day-1 retention rates are different between versions. \nThe results show a Z-statistic of 1.7871 and a p-value of 0.0739. Since this p-value is slightly above the 0.05 threshold, there isn’t enough evidence to say that the two versions differ in a statistically significant way. Though p-value is close enough to suggest a possible pattern. In other words, while the difference is not significant, it hints that the Gulf of Maine version might be performing a bit better, and Lobster Land should keep an eye on it for future marketing strategy.\n\n\nb. Next, pick either version (Gulf of Maine or North Atlantic).\nIn this case, I chose a slighly better performer which is ‘Gulf of Maine’.\n\ngom_version = 'Gulf of Maine'\n\n\n\nc. Then, find the number of users of your chosen version who stayed with the game for one day.\n\n# The total users in Gulf of Maine version\ngom_total = lobster_run[lobster_run['version'] == gom_version].shape[0]\nprint(f\"The total users in Gulf of Maine is: {gom_total}\")\n\n# Filter out the retained users for Gulf of Maine after day 1\ngom_retained1 = lobster_run[(lobster_run['version'] == gom_version) & \n    (lobster_run['retention_1'] == True)].shape[0]\n\nprint(f\"The actual retained users after Day 1 in Gulf of Maine is: {gom_retained1}\")\n\nThe total users in Gulf of Maine is: 44699\nThe actual retained users after Day 1 in Gulf of Maine is: 20034\n\n\n\n\nd. Next, find the expected number of users who would have stayed with the game for one day, if your null hypothesis were true.\n\n# Find the average retention_1 rate across all users of Lobster Land\navg_retention1 = lobster_run['retention_1'].mean()\n\n# The expected number of users from Gulf of Maine that stayed after one day if the hypothesis were true\nexpect_retained1 = gom_total * avg_retention1\n\n# Round number of users to the nearest integer\nexpect_retained1_round = round(expect_retained1)\nprint(f\"The expected retained users in Gulf of Maine after Day 1 under H0: {expect_retained1_round}\")\n\nThe expected retained users in Gulf of Maine after Day 1 under H0: 19901\n\n\n\n\nWhat is this expected value? How did you arrive at this value?\nThe expected value shows how many users from the Gulf of Maine version I would expect to stay after Day 1 if the version had no effect on retention. To calculate this, I multiplied the total number of Gulf of Maine users by the overall day 1 retention rate across all users. I used the overall rate because, under the null hypothesis, I assume retention is independent of version. This gives me a baseline to compare against the actual retention in Gulf of Maine and helps me assess whether the observed difference is meaningful.\nAfter the calculation, the expected retained users in Gulf of Maine after day 1 is 19901 users. This version retained 20,034 users, compared to the expected 19,901 under the null hypothesis, meaning it held on to about 133 more users than anticipated. While this difference is small relative to the total user base, it suggests a slight positive deviation in retention performance for Gulf of Maine.\n\n\ne. Now, conduct a binomial z-test to determine whether the retention_1 for the version you chose is meaningfully different from the expected number, given your null hypothesis.\n\n# Run binomial z-test\nz_stat, p_value = proportions_ztest(count=gom_retained1, nobs=gom_total, value=avg_retention1)\n\n# Step 6: Print results\nprint(\"Binomial Z-Test for Gulf of Maine retention_1 vs expected under H0:\")\nprint(f\"Z-statistic = {z_stat:.4f}\")\nprint(f\"p-value     = {p_value:.4f}\")\n\nBinomial Z-Test for Gulf of Maine retention_1 vs expected under H0:\nZ-statistic = 1.2684\np-value     = 0.2047\n\n\n\n\nf. Based on the test, what will you conclude?\nBased on the binomial z-test, the day 1 retention rate for the Gulf of Maine version does not differ significantly from what would be expected under the null hypothesis. The p-value of 0.2047 is greater than the conventional 0.05 threshold, so I fail to reject the null hypothesis. This result indicates that, given the available data, there isn’t sufficient evidence to conclude that the Gulf of Maine version has an effect on day 1 retention relative to the overall average. Lobster Land could improve this analysis by gathering more data or testing targeted design changes to better understand what drives early user retention.\n\n\n\nG. Again using a binomial z-test, compare the retention_7 outcomes for the two app versions.\n\n# Count retained users per version by retention_7\nretained_count_rt7 = lobster_run.groupby('version', observed=True)['retention_7'].sum()\nprint(\"Retained users (retention_7):\")\nprint(retained_count_rt7)\n\n# Count total users per version by retention 7\nuser_total_rt7 = lobster_run.groupby('version', observed=True)['retention_7'].count()\nprint(\"\\nTotal users per version (retention_7):\")\nprint(user_total_rt7)\n\nRetained users (retention_7):\nversion\nGulf of Maine     8744\nNorth Atlantic    8279\nName: retention_7, dtype: int64\n\nTotal users per version (retention_7):\nversion\nGulf of Maine     44699\nNorth Atlantic    45489\nName: retention_7, dtype: int64\n\n\n\n# Run binomial z-test to compare the 2 versions\nz_stat7, p_value7 = proportions_ztest(count=retained_count_rt7, nobs=user_total_rt7)\nprint(\"Binomial Z-Test Results (retention_7):\")\nprint(f\"Z-statistic = {z_stat7:.4f}\")\nprint(f\"p-value     = {p_value7:.4f}\")\n\nBinomial Z-Test Results (retention_7):\nZ-statistic = 5.2260\np-value     = 0.0000\n\n\n\nalpha = 0.05\nif p_value7 &gt; 0.05:\n    print(\"Fail to reject H0: No significant difference in day-7 retention between versions.\")\nelse:\n    print(\"Reject H0: Version has a significant impact on day-7 retention.\")\n\nReject H0: Version has a significant impact on day-7 retention.\n\n\n\na. What will be the null hypothesis of your test?\nThe test’s null hypothesis:  H0: Day-7 retention rates are equal between Gulf of Maine and North Atlantic versions.  H1: Day-7 retention rates are different between versions. \nI conducted a binomial z-test to compare day-7 retention between the Gulf of Maine and North Atlantic versions. The p-value was approximately 0.000, which is well below the standard significance level of 0.05. Therefore, I reject the null hypothesis and conclude that there is a statistically significant difference in day-7 retention between the two versions. This result suggests that users playing the Gulf of Maine version are more likely to remain active after 7 days compared to those playing the North Atlantic version.\n\n\nb. Next, pick either version (Gulf of Maine or North Atlantic).\nI still chose a slighly better performer which is ‘Gulf of Maine’.\n\ngom_version = 'Gulf of Maine'\n\n\n\nc. Then, find the number of users of your chosen version who stayed with the game for one day.\n\n# Filter out the retained users for Gulf of Maine after day 1\ngom_retained7 = lobster_run[(lobster_run['version'] == gom_version) & \n    (lobster_run['retention_7'] == True)].shape[0]\n\nprint(f\"The actual retained users after Day 7 in Gulf of Maine is: {gom_retained7}\")\n\nThe actual retained users after Day 7 in Gulf of Maine is: 8744\n\n\n\n\nd. Next, find the expected number of users who would have stayed with the game for one day, if your null hypothesis were true.\n\n# Find the average retention 7 rate across all users of Lobster Land\navg_retention7 = lobster_run['retention_7'].mean()\n\n# The expected number of users from Gulf of Maine that stayed after one day if the hypothesis were true\nexpect_retained7 = gom_total * avg_retention7\n# Round number of users to the nearest integer\nexpect_retained7_round = round(expect_retained7)\nprint(f\"The expected retained users in Gulf of Maine after Day 7 under H0: {expect_retained7_round}\")\n\nThe expected retained users in Gulf of Maine after Day 7 under H0: 8437\n\n\n\n\nWhat is this expected value? How did you arrive at this value?\nThe expected value here refers to the number of Gulf of Maine users I would anticipate being retained under the null hypothesis (H0), which assumes that day‑7 retention in Gulf of Maine is the same as the overall average retention rate across all users.\nTo calculate this, I take the total number of Gulf of Maine users and multiply by the overall day‑7 retention rate (the pooled proportion across both versions). This gives an expected retained count of 8,437 users. In other words, if Gulf of Maine behaved exactly like the overall user base, Lobster Land would expect about 8,437 users to remain by day 7.\n\n\ne. Now, conduct a binomial z-test to determine whether the retention_1 for the version you chose is meaningfully different from the expected number, given your null hypothesis.\n\n# Run binomial z-test\nz_stat, p_value = proportions_ztest(count=gom_retained7, nobs=gom_total, value=avg_retention7)\n\n# Step 6: Print results\nprint(\"Binomial Z-Test for Gulf of Maine retention_7 vs expected under H0:\")\nprint(f\"Z-statistic = {z_stat:.4f}\")\nprint(f\"p-value     = {p_value:.4f}\")\n\nBinomial Z-Test for Gulf of Maine retention_7 vs expected under H0:\nZ-statistic = 3.6613\np-value     = 0.0003\n\n\n\n\nf. Based on the test, what will you conclude?\nThe binomial z-test for Gulf of Maine’s day‑7 retention produced a Z‑statistic of 3.6613 with a p‑value of 0.0003, well below the 0.05 threshold, suggesting retention is higher than expected under the null hypothesis. Gulf of Maine retained 20,034 users by day 1 and continued to outperform expectations by day 7, with 8,437 users retained compared to the expected count.\nThis suggests that while early retention looked similar across versions, Gulf of Maine is delivering a stronger long‑term engagement advantage. For Lobster Land, this is a clear signal that the Gulf of Maine version could be leveraged more aggressively in marketing and product strategy.\n\n\n\nH. Write a couple sentences of thoughts and/or recommendations for Lobster Land based on the overall results that you have found from the Super Lobster Run data and the statistical tests.\nThe Super Lobster Run experiment shows that the starting level didn’t have a major impact on day‑1 retention or spending, with both Gulf of Maine and North Atlantic versions retaining roughly 20,034 users initially. Retention differences on day 1 were not statistically significant (Z = 1.79, p = 0.074), but by day‑7, the Gulf of Maine version retained 8,437 users, showing a clear and statistically significant advantage (Z = 5.23, p &lt; 0.05). This suggests that while early engagement was similar, Gulf of Maine may help maintain stronger long‑term retention.\nBased on these findings, Lobster Land could consider emphasizing the Gulf of Maine level marketing strategies to leverage its retention advantage. They might also experiment with transferring successful design elements from Gulf of Maine - such as level design, difficulty pacing, or visual - into North Atlantic or future levels. Additionally, offering limited-time promotion booster packages for $0.99 or applying seasonal/holiday theme could encourage spending and interaction, potentially increasing engagement across all versions. To better understand the results, it would be useful to monitor retention beyond day‑7, track whether higher retention leads to increased long-term spending, and gather qualitative feedback to learn what aspects of Gulf of Maine are driving engagement.\n\n\nPart III: Chi-Square Goodness of Fit\n\n# The data collected from the survey  Roller Coasters, Water Rides, Lobsterama Restaurant, Marine Life Exhibit, Lobsterland Merchandise Shop\nobserved = [150, 120, 90, 80, 60]\ntotal = sum(observed)\nprint(f\"The total of observed visitors from their favorite attraction at Lobster Land is: {total}.\")\n\nexpected = [0.30 * total, 0.25 * total, 0.15 * total, 0.15 * total, 0.15 * total]\n\nprint(\"\\nThe expected visitors at Lobster Land is:\")\nprint(f\"Roller Coasters: {expected[0]}\")\nprint(f\"Water Rides: {expected[1]}\")\nprint(f\"Lobsterama Restaurant: {expected[2]}\")\nprint(f\"Marine Life Exhibit: {expected[3]}\")\nprint(f\"Lobsterland Merchandise Shop: {expected[4]}\")\n\nThe total of observed visitors from their favorite attraction at Lobster Land is: 500.\n\nThe expected visitors at Lobster Land is:\nRoller Coasters: 150.0\nWater Rides: 125.0\nLobsterama Restaurant: 75.0\nMarine Life Exhibit: 75.0\nLobsterland Merchandise Shop: 75.0\n\n\n\n\nA. What is the null hypothesis of this test? What is the alternative hypothesis?\nH0 - Null Hypothesis: The distribution of visitor preferences across the five attractions at Lobster Land matches the expected proportions provided by management: - 30% Roller Coasters - 25% Water Rides - 15% Lobsterama Restaurant - 15% Marine Life Exhibit - 15% Merchandise Shop\nH1 - Alternative hypothesis: The distribution of visitor preferences does not match the expected proportions.\nThe null hypothesis (H0) assumes that the distribution of visitor preferences across Lobster Land’s attractions: roller coasters, water rides, the seafood-themed food court, marine life exhibit, and lobster-themed merchandise shop which matches the proportions expected by the park’s management. Whereas, the alternative hypothesis (H1) suggests that visitor preferences differ enough from these expected proportions that the differences are unlikely to occur by chance, indicating that certain attractions may be more or less popular than anticipated, which could inform resource allocation and marketing strategies.\n\na. Under the null hypothesis, what is our expected number of respondents for roller coasters, water rides, Lobsterama Restaurant, Marine Life Exhibit, and the Lobsterland Merchandise Shop?\nSince a total of 500 survey responses were collected, I calculated the expected number of respondents for each attraction by applying the expected percentages to this total. Lobster Land expects 30% of visitors to prefer roller coasters, which equals 150 respondents. For water rides, the expected proportion is 25%, resulting in 125 expected respondents. The remaining three attractions, Lobsterama Restaurant, the Marine Life Exhibit, and the Lobsterland Merchandise Shop, are each expected to receive 15% of the votes, corresponding to 75 respondents per attraction.\n\n\n\nB. Using both the expected values and the actual numbers that came from the survey respondents, run a chi-square goodness of fit test to assess your null hypothesis. Show your results.\n\n# Conduct a chi-square goodness of fit\nchisq, pvalue = chisquare(f_obs=observed, f_exp=expected)\n\nprint(\"\\nThe chi-square value is: \", round(chisq, 4))\nprint(\"The p-value is: \", round(pvalue, 4))\n\n\nThe chi-square value is:  6.5333\nThe p-value is:  0.1627\n\n\n\nalpha = 0.05\nif pvalue &gt; alpha:\n    print(\"Fail to reject H0: The observed distribution does not differ significantly from the expected distribution.\")\nelse:\n    print(\"Reject H0: The observed distribution differ significantly from the expected distribution.\")\n\nFail to reject H0: The observed distribution does not differ significantly from the expected distribution.\n\n\n\na. What is the p-value of this test? Based on this value, what will you conclude?\nThe p-value of this chi-square goodness of fit test is 0.1627.\nSince this p-value = 0.1627 is greater than the common significance level of 0.05, I fail to reject the null hypothesis. This means there is no statistically significant difference between the observed distribution of visitor preferences and the expected distribution provided by Lobster Land management.\nIn other words, the survey results do not provide enough evidence to conclude that visitor preferences differ from what management anticipated. The observed variation is likely due to random chance rather than a meaningful shift in attraction popularity. To improve the reliability of this test, Lobster Land could collect a larger sample of visitor responses to reduce sampling error and increase statistical power. Additionally, the management team could refine the expected distribution by providing more recent attendance data or seasonal trends, ensuring that the comparison reflects current visitor behavior more accurately.\n\n\n\nC. Demonstrate where the chi-square number from your test came from. Use Jupyter Notebook or Colab to do this, but do not use any Python libraries or modules.\nBased on the results and datas that I retrieve from part A and B, once I have both the observed (O) and expected (E) counts, I am able to calculate the contribution of each cell to the overall chi-square statistic using this formula:\n\nThe calculations for each attraction at Lobster Land are as follows: - Roller Coasters: (150 − 150)^2/150 = 0.00 - Water Rides: (120 − 125)^2/125 = 0.20 - Lobsterama Restaurant: (90 − 75)^2/75 = 3.00 - Marine Life Exhibit: (80 − 75)^2/75 = 0.33 - Merchandise Shop: (60 − 75)^2/75 = 3.00\nFinally, I sum these five contributions to obtain the overall chi-square statistic:  X^2 = 0.00 + 0.20 + 3.00 + 0.33 + 3.00 = 6.53\nThe degrees of freedom in a chi‑square goodness‑of‑fit test are calculated as the number of categories minus one. Since there are 5 attraction categories at Lobster Land, the degrees of freedom are 5 − 1 = 4. With 4 degrees of freedom, the corresponding p‑value is approximately 0.1627. Since this p‑value is greater than the 0.05 threshold, I fail to reject the null hypothesis. In practical terms, this means the observed visitor preferences are not significantly different from the expected proportions, and the distribution of favorite attractions at Lobster Land is broadly consistent with the assumed model.\nThe results show that visitors’ attraction preferences are largely in line with what Lobster Land’s management expected, with only small differences in how often people chose the Lobsterama Restaurant and Merchandise Shop. The chi-square value of 6.53 and p-value of 0.1627 suggest that these variations aren’t statistically significant, or the overall pattern of visitor preferences are pretty consistent with the original expectations. Though, as Lobster Land expands its engagement through the “Super Lobster Run” game, it could benefit from collecting data from a larger and more diverse group of users throughout the year to identify any seasonal shifts in interest. Updating expected values to reflect new attractions or marketing efforts could also help the park fine-tune its offerings and digital strategy."
  },
  {
    "objectID": "python_notebooks/AD654_Nhat_Tran_Assignment_3.html#part-iv-wildcard",
    "href": "python_notebooks/AD654_Nhat_Tran_Assignment_3.html#part-iv-wildcard",
    "title": "Assignment 3 - Statistical Testing",
    "section": "Part IV: Wildcard",
    "text": "Part IV: Wildcard\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\nThe place I chose to introduce is one of my favorite Vietnamese spots that truly hits home - Pho Ga Thanh Thanh (Gà means chicken in Vietnamese), located in South Philadelphia, where I lived for seven years. I chose this spot because, coming from Hanoi, I grew up with Northern-style pho, which is lighter, cleaner, and we do offer chicken-based pho. Most Vietnamese restaurants in the U.S. tend to serve the southern version, which leans heavier and usually features beef, so finding a place in Philadelphia that specializes in chicken pho felt both nostalgic and refreshing.\nLooking at the word cloud from Yelp reviews of this restaurant, a few clear themes emerge that reveal what attracts customers to the restaurant and where there might be room to improve. The most common words, like “pho,” “chicken,” “broth,” and “noodles,” confirm that the restaurant’s signature dish is the highlight for customers. The words “delicious,” “fresh,” and “sauce” demonstrate how much people care about the flavor and excellence of their ingredients. Meanwhile, “Philadelphia” and “Philly” suggest that the restaurant has become a familiar part of the local dining scene, resonating with the city’s community and food culture.\nThe largest words, “pho” and “chicken”, confirm that the restaurant’s specialty is what keeps people coming back. Mentions of “broth” and “noodles” show that customers pay attention to the balance of flavor and texture, which are important to a great bowl of pho. Positive comments like “fresh” and “delicious” reinforce the restaurant’s consistent quality.\nThe words “order” and “experience” and “keep” indicate that service quality and dining atmosphere create subtle effects on customer perceptions. The service quality and ordering process need minor adjustments to create a better dining experience. The restaurant’s connection to Philadelphia and Philly culture provides a chance to develop its local reputation through authentic Northern Vietnamese food offerings.\nOverall, while the restaurant is already quite well-known in the area, the word cloud analysis suggests there’s still room to refine its marketing strategy. By highlighting its authentic Northern-style chicken pho and creating campaigns that resonate with international students or young professionals missing the taste of home (like me), Pho Ga Thanh Thanh could strengthen its connection with the community and attract an even broader audience."
  }
]