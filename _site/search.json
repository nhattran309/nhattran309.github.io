[
  {
    "objectID": "python_notebooks/AD654_Nhat_Tran_Assignment_1.html",
    "href": "python_notebooks/AD654_Nhat_Tran_Assignment_1.html",
    "title": "Data Exploration & Visualization",
    "section": "",
    "text": "# Importing necessary functions\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nlobster_25 = pd.read_csv(\"data/lobster_25.csv\")\n\n\nlobster_25.head()\n\n\n\n\n\n\n\n\nDate\nDay_of_Week\nseason_week_rel\nIs_Holiday\nTotal_Visitors\nPassholder_Percentage\nDay_Tickets_Sold\nAvg_Ticket_Price\nGate_Revenue\nRevenue_Food\n...\nWeather_Type\nWeather_Code\nPromo_Flag\nPromo_Type\nCruise_Docked\nIs_Special_Event\nSpecial_Events\nAttraction_Tier\nZone_ID\nPer_Capita_Spend\n\n\n\n\n0\n2025-05-26\nMonday\n1\n1\n1769\n48.65\n908\n77.42\n70297.36\n40521.91\n...\nThunderstorms\n4\n0\nNaN\n0\n0\nNaN\n2\n101\n73.43\n\n\n1\n2025-05-27\nTuesday\n1\n0\n1717\n60.58\n677\n69.80\n47254.60\n35298.92\n...\nPartly Cloudy\n2\n0\nNaN\n0\n0\nNaN\n2\n101\n59.30\n\n\n2\n2025-05-28\nWednesday\n1\n0\n1600\n30.84\n1107\n70.09\n77589.63\n36495.95\n...\nPartly Cloudy\n2\n0\nNaN\n0\n0\nNaN\n3\n103\n82.33\n\n\n3\n2025-05-29\nThursday\n1\n0\n1037\n19.02\n840\n72.30\n60732.00\n22486.65\n...\nShowers\n3\n0\nNaN\n0\n0\nNaN\n2\n101\n91.19\n\n\n4\n2025-05-30\nFriday\n1\n0\n1671\n66.18\n565\n68.11\n38482.15\n40705.15\n...\nPartly Cloudy\n2\n0\nNaN\n0\n0\nNaN\n1\n104\n58.67\n\n\n\n\n5 rows √ó 27 columns\n\n\n\n\nlobster_25.shape\n\n(99, 27)\n\n\nThere are 99 rows and 27 columns in this Lobsterland data set.\n\nCategorical variables: Date, Day of Week, season_week_rel, is_holiday, Precipitation, Weather Type, Weather Code, Promo Flag, Promo Type, Cruise Docked, Is_Special_Event, Special_Events, Attraction_Tier, Zone_ID\nNumeric variables: Total visitors, Passholder percentage, Day Tickets Sold, Avg Ticket Price, Gate Revenue, Revenue Food, Revenue Merch, Revenue Arcade, Total Revenue, Total Labor Hours, International Visitors, High temperature, Per Capita Spend\n\n\n\n\nlobster_25.isnull().sum()\n\nDate                       0\nDay_of_Week                0\nseason_week_rel            0\nIs_Holiday                 0\nTotal_Visitors             0\nPassholder_Percentage      0\nDay_Tickets_Sold           0\nAvg_Ticket_Price           0\nGate_Revenue               0\nRevenue_Food               0\nRevenue_Merch              5\nRevenue_Arcade             0\nTotal_Revenue              0\nTotal_Labor_Hours          0\nInternational_Visitors     5\nHigh_Temperature           0\nPrecipitation              0\nWeather_Type               0\nWeather_Code               0\nPromo_Flag                 0\nPromo_Type                79\nCruise_Docked              0\nIs_Special_Event           0\nSpecial_Events            84\nAttraction_Tier            0\nZone_ID                    0\nPer_Capita_Spend           0\ndtype: int64\n\n\nTo check for NaN values, I used the code lobster25.isna().sum(). This showed the number of missing values in each column. The columns that contain missing values are Revenue_Merch with 5 missing entries, International_Visitors with 5 missing entries, Promo_Type with 79 missing entries, and Special_Events with 84 missing entries. These missing values are understandable because the columns represent optional events, promotions, or international visitors.\n\nFor numeric variables like Revenue_Merch and International_Visitors, which are recorded in numbers, NaN likely means no merchandise revenue or no international visitors that day, so it can be replaced with 0.\nAs for categorial variables like Promo_Type and Special_Events, NaN means no promotion/special event was active, so it‚Äôs better to fill them with descriptive text rather than 0. So, for Promo_Type, I replace NaN with ‚ÄúNo Promo‚Äù and for Special_Events with ‚ÄúNo Event‚Äù respectively.\n\n\n# Create and copy from the original dataframe so not to mess with it\nlobster25 = lobster_25.copy() \n\n# Filling numeric variables with NaN to 0\nlobster25['Revenue_Merch'] = lobster25['Revenue_Merch'].fillna(0.0)\nlobster25['International_Visitors'] = lobster25['International_Visitors'].fillna(0.0)\n\n\nlobster25[\"Promo_Type\"] = lobster25[\"Promo_Type\"].fillna(\"No Promo\")\nlobster25[\"Special_Events\"] = lobster25[\"Special_Events\"].fillna(\"No Event\")\n\n\n# Double-checking to see if there's still NaN in the data set\nlobster25.isnull().sum()\n\nDate                      0\nDay_of_Week               0\nseason_week_rel           0\nIs_Holiday                0\nTotal_Visitors            0\nPassholder_Percentage     0\nDay_Tickets_Sold          0\nAvg_Ticket_Price          0\nGate_Revenue              0\nRevenue_Food              0\nRevenue_Merch             0\nRevenue_Arcade            0\nTotal_Revenue             0\nTotal_Labor_Hours         0\nInternational_Visitors    0\nHigh_Temperature          0\nPrecipitation             0\nWeather_Type              0\nWeather_Code              0\nPromo_Flag                0\nPromo_Type                0\nCruise_Docked             0\nIs_Special_Event          0\nSpecial_Events            0\nAttraction_Tier           0\nZone_ID                   0\nPer_Capita_Spend          0\ndtype: int64\n\n\nI replaced NaNs in Revenue_Merch and International_Visitors with 0, assuming missing values indicate no revenue or no visitors. For Promo_Type and Special_Events, I used ‚ÄúNo Promo‚Äù and ‚ÄúNo Event‚Äù because missing entries mean no promotion or event occurred. I made these choices by considering what each column represents. This approach makes the dataset more informative and descriptive.\n\n\n\n\nlobster25 = lobster25.rename(columns={'Revenue_Food':'Revenue_FoodBev'}) #rename the columns\n# There is no 'Food_Rev' so I'm assuming it's 'Revenue_Food' and replace it with 'Revenue_FoodBev'\nprint(lobster25.columns)\n\nIndex(['Date', 'Day_of_Week', 'season_week_rel', 'Is_Holiday',\n       'Total_Visitors', 'Passholder_Percentage', 'Day_Tickets_Sold',\n       'Avg_Ticket_Price', 'Gate_Revenue', 'Revenue_FoodBev', 'Revenue_Merch',\n       'Revenue_Arcade', 'Total_Revenue', 'Total_Labor_Hours',\n       'International_Visitors', 'High_Temperature', 'Precipitation',\n       'Weather_Type', 'Weather_Code', 'Promo_Flag', 'Promo_Type',\n       'Cruise_Docked', 'Is_Special_Event', 'Special_Events',\n       'Attraction_Tier', 'Zone_ID', 'Per_Capita_Spend'],\n      dtype='object')\n\n\n\n\n\n\navg_rev_by_event = lobster25.groupby('Special_Events')[['Gate_Revenue','Revenue_FoodBev','Revenue_Merch','Revenue_Arcade']].mean()\nprint(avg_rev_by_event)\n\n                         Gate_Revenue  Revenue_FoodBev  Revenue_Merch  \\\nSpecial_Events                                                          \nBeatles Tribute         146630.870000     86767.966667   21431.026667   \nMusicFest               107296.750000     83544.713333   33065.490000   \nNight Glow              107957.660000     59590.776667   31730.376667   \nNo Event                 73213.523333     40692.877500   15931.317500   \nTaylor Swift Lookalike   96895.336667     67165.716667   33479.076667   \nVintage Days            114238.410000     51074.406667   24561.976667   \n\n                        Revenue_Arcade  \nSpecial_Events                          \nBeatles Tribute           10488.050000  \nMusicFest                  9745.910000  \nNight Glow                 8637.820000  \nNo Event                   3624.482500  \nTaylor Swift Lookalike     9691.783333  \nVintage Days               6821.316667  \n\n\nTable analysis: Stand out points & limitations  Looking at the table, it‚Äôs clear that special events like Beatles Tribute and MusicFest bring in noticeably higher revenue across all areas compared to days with no events. This makes sense, as popular events tend to draw bigger crowds and encourage people to spend more on tickets, food, and merchandise. Some of the standout numbers are probably because certain events are more well-known or heavily promoted. That said, this analysis only looks at averages and doesn‚Äôt consider other factors like weather, day of the week, or seasonal trends. Because of these other influences, it‚Äôs hard to say for sure how much of the revenue boost is truly caused by the events themselves.\n\n\n\n\nlobster25['Date'].info()\n\n&lt;class 'pandas.core.series.Series'&gt;\nRangeIndex: 99 entries, 0 to 98\nSeries name: Date\nNon-Null Count  Dtype \n--------------  ----- \n99 non-null     object\ndtypes: object(1)\nmemory usage: 924.0+ bytes\n\n\nPython currently views this variable as an object.\n\n# Converting the variable into a datetime object.\nlobster25['Date'] = pd.to_datetime(lobster25['Date'])\n\n# Check to see if the conversion is successful\nprint(lobster25['Date'].dtype)\n\ndatetime64[ns]\n\n\nConverting a column to datetime lets Python treat it as a real date, enabling operations like filtering by month, extracting weekdays, calculating durations, and plotting time-series data accurately. Without it, dates are just text and harder to analyze.\n\n\n\n\nlobster25.groupby('Day_of_Week')[['Total_Revenue']].describe()\n\n\n\n\n\n\n\n\nTotal_Revenue\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nDay_of_Week\n\n\n\n\n\n\n\n\n\n\n\n\nFriday\n14.0\n124348.351429\n25766.281104\n68899.07\n113828.1850\n126337.460\n134174.9625\n180408.42\n\n\nMonday\n15.0\n123881.241333\n30711.357993\n88753.13\n101210.0300\n110997.990\n143166.6050\n202565.15\n\n\nSaturday\n14.0\n233435.688571\n63186.045667\n132174.43\n209613.2550\n227397.180\n238614.4925\n423019.37\n\n\nSunday\n14.0\n222040.763571\n51689.800794\n127310.71\n211008.1775\n245217.160\n251046.1250\n284520.15\n\n\nThursday\n14.0\n110808.970000\n23883.366451\n63247.57\n94773.4125\n112346.540\n128805.5325\n147562.14\n\n\nTuesday\n14.0\n110803.409286\n28858.768417\n64017.91\n95816.2750\n106621.260\n132957.8850\n166042.42\n\n\nWednesday\n14.0\n111780.540000\n24591.350227\n75302.24\n92629.4175\n111242.325\n127296.6300\n165467.64\n\n\n\n\n\n\n\nLooking at the table, it‚Äôs pretty noticeable that weekends bring in much higher revenue than weekdays. Saturday averages around $233,000 and Sunday about $222,000, while most weekdays hover between $110,000 and $125,000. This makes sense since more people are likely to visit the park on their days off. The bigger variation in weekend revenues probably comes from special events or particularly busy days. Overall, the day of the week has a noticeable effect, with weekends clearly driving the biggest crowds and sales.\n\n\n\n\nlobster25 = lobster25.drop(columns=['Zone_ID'], errors='ignore')\n# Double-check\nprint(lobster25.columns)\n\nIndex(['Date', 'Day_of_Week', 'season_week_rel', 'Is_Holiday',\n       'Total_Visitors', 'Passholder_Percentage', 'Day_Tickets_Sold',\n       'Avg_Ticket_Price', 'Gate_Revenue', 'Revenue_FoodBev', 'Revenue_Merch',\n       'Revenue_Arcade', 'Total_Revenue', 'Total_Labor_Hours',\n       'International_Visitors', 'High_Temperature', 'Precipitation',\n       'Weather_Type', 'Weather_Code', 'Promo_Flag', 'Promo_Type',\n       'Cruise_Docked', 'Is_Special_Event', 'Special_Events',\n       'Attraction_Tier', 'Per_Capita_Spend'],\n      dtype='object')"
  },
  {
    "objectID": "python_notebooks/AD654_Nhat_Tran_Assignment_1.html#i.-exploratory-data-analysis-exploration-manipulation",
    "href": "python_notebooks/AD654_Nhat_Tran_Assignment_1.html#i.-exploratory-data-analysis-exploration-manipulation",
    "title": "Data Exploration & Visualization",
    "section": "",
    "text": "# Importing necessary functions\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nlobster_25 = pd.read_csv(\"data/lobster_25.csv\")\n\n\nlobster_25.head()\n\n\n\n\n\n\n\n\nDate\nDay_of_Week\nseason_week_rel\nIs_Holiday\nTotal_Visitors\nPassholder_Percentage\nDay_Tickets_Sold\nAvg_Ticket_Price\nGate_Revenue\nRevenue_Food\n...\nWeather_Type\nWeather_Code\nPromo_Flag\nPromo_Type\nCruise_Docked\nIs_Special_Event\nSpecial_Events\nAttraction_Tier\nZone_ID\nPer_Capita_Spend\n\n\n\n\n0\n2025-05-26\nMonday\n1\n1\n1769\n48.65\n908\n77.42\n70297.36\n40521.91\n...\nThunderstorms\n4\n0\nNaN\n0\n0\nNaN\n2\n101\n73.43\n\n\n1\n2025-05-27\nTuesday\n1\n0\n1717\n60.58\n677\n69.80\n47254.60\n35298.92\n...\nPartly Cloudy\n2\n0\nNaN\n0\n0\nNaN\n2\n101\n59.30\n\n\n2\n2025-05-28\nWednesday\n1\n0\n1600\n30.84\n1107\n70.09\n77589.63\n36495.95\n...\nPartly Cloudy\n2\n0\nNaN\n0\n0\nNaN\n3\n103\n82.33\n\n\n3\n2025-05-29\nThursday\n1\n0\n1037\n19.02\n840\n72.30\n60732.00\n22486.65\n...\nShowers\n3\n0\nNaN\n0\n0\nNaN\n2\n101\n91.19\n\n\n4\n2025-05-30\nFriday\n1\n0\n1671\n66.18\n565\n68.11\n38482.15\n40705.15\n...\nPartly Cloudy\n2\n0\nNaN\n0\n0\nNaN\n1\n104\n58.67\n\n\n\n\n5 rows √ó 27 columns\n\n\n\n\nlobster_25.shape\n\n(99, 27)\n\n\nThere are 99 rows and 27 columns in this Lobsterland data set.\n\nCategorical variables: Date, Day of Week, season_week_rel, is_holiday, Precipitation, Weather Type, Weather Code, Promo Flag, Promo Type, Cruise Docked, Is_Special_Event, Special_Events, Attraction_Tier, Zone_ID\nNumeric variables: Total visitors, Passholder percentage, Day Tickets Sold, Avg Ticket Price, Gate Revenue, Revenue Food, Revenue Merch, Revenue Arcade, Total Revenue, Total Labor Hours, International Visitors, High temperature, Per Capita Spend\n\n\n\n\nlobster_25.isnull().sum()\n\nDate                       0\nDay_of_Week                0\nseason_week_rel            0\nIs_Holiday                 0\nTotal_Visitors             0\nPassholder_Percentage      0\nDay_Tickets_Sold           0\nAvg_Ticket_Price           0\nGate_Revenue               0\nRevenue_Food               0\nRevenue_Merch              5\nRevenue_Arcade             0\nTotal_Revenue              0\nTotal_Labor_Hours          0\nInternational_Visitors     5\nHigh_Temperature           0\nPrecipitation              0\nWeather_Type               0\nWeather_Code               0\nPromo_Flag                 0\nPromo_Type                79\nCruise_Docked              0\nIs_Special_Event           0\nSpecial_Events            84\nAttraction_Tier            0\nZone_ID                    0\nPer_Capita_Spend           0\ndtype: int64\n\n\nTo check for NaN values, I used the code lobster25.isna().sum(). This showed the number of missing values in each column. The columns that contain missing values are Revenue_Merch with 5 missing entries, International_Visitors with 5 missing entries, Promo_Type with 79 missing entries, and Special_Events with 84 missing entries. These missing values are understandable because the columns represent optional events, promotions, or international visitors.\n\nFor numeric variables like Revenue_Merch and International_Visitors, which are recorded in numbers, NaN likely means no merchandise revenue or no international visitors that day, so it can be replaced with 0.\nAs for categorial variables like Promo_Type and Special_Events, NaN means no promotion/special event was active, so it‚Äôs better to fill them with descriptive text rather than 0. So, for Promo_Type, I replace NaN with ‚ÄúNo Promo‚Äù and for Special_Events with ‚ÄúNo Event‚Äù respectively.\n\n\n# Create and copy from the original dataframe so not to mess with it\nlobster25 = lobster_25.copy() \n\n# Filling numeric variables with NaN to 0\nlobster25['Revenue_Merch'] = lobster25['Revenue_Merch'].fillna(0.0)\nlobster25['International_Visitors'] = lobster25['International_Visitors'].fillna(0.0)\n\n\nlobster25[\"Promo_Type\"] = lobster25[\"Promo_Type\"].fillna(\"No Promo\")\nlobster25[\"Special_Events\"] = lobster25[\"Special_Events\"].fillna(\"No Event\")\n\n\n# Double-checking to see if there's still NaN in the data set\nlobster25.isnull().sum()\n\nDate                      0\nDay_of_Week               0\nseason_week_rel           0\nIs_Holiday                0\nTotal_Visitors            0\nPassholder_Percentage     0\nDay_Tickets_Sold          0\nAvg_Ticket_Price          0\nGate_Revenue              0\nRevenue_Food              0\nRevenue_Merch             0\nRevenue_Arcade            0\nTotal_Revenue             0\nTotal_Labor_Hours         0\nInternational_Visitors    0\nHigh_Temperature          0\nPrecipitation             0\nWeather_Type              0\nWeather_Code              0\nPromo_Flag                0\nPromo_Type                0\nCruise_Docked             0\nIs_Special_Event          0\nSpecial_Events            0\nAttraction_Tier           0\nZone_ID                   0\nPer_Capita_Spend          0\ndtype: int64\n\n\nI replaced NaNs in Revenue_Merch and International_Visitors with 0, assuming missing values indicate no revenue or no visitors. For Promo_Type and Special_Events, I used ‚ÄúNo Promo‚Äù and ‚ÄúNo Event‚Äù because missing entries mean no promotion or event occurred. I made these choices by considering what each column represents. This approach makes the dataset more informative and descriptive.\n\n\n\n\nlobster25 = lobster25.rename(columns={'Revenue_Food':'Revenue_FoodBev'}) #rename the columns\n# There is no 'Food_Rev' so I'm assuming it's 'Revenue_Food' and replace it with 'Revenue_FoodBev'\nprint(lobster25.columns)\n\nIndex(['Date', 'Day_of_Week', 'season_week_rel', 'Is_Holiday',\n       'Total_Visitors', 'Passholder_Percentage', 'Day_Tickets_Sold',\n       'Avg_Ticket_Price', 'Gate_Revenue', 'Revenue_FoodBev', 'Revenue_Merch',\n       'Revenue_Arcade', 'Total_Revenue', 'Total_Labor_Hours',\n       'International_Visitors', 'High_Temperature', 'Precipitation',\n       'Weather_Type', 'Weather_Code', 'Promo_Flag', 'Promo_Type',\n       'Cruise_Docked', 'Is_Special_Event', 'Special_Events',\n       'Attraction_Tier', 'Zone_ID', 'Per_Capita_Spend'],\n      dtype='object')\n\n\n\n\n\n\navg_rev_by_event = lobster25.groupby('Special_Events')[['Gate_Revenue','Revenue_FoodBev','Revenue_Merch','Revenue_Arcade']].mean()\nprint(avg_rev_by_event)\n\n                         Gate_Revenue  Revenue_FoodBev  Revenue_Merch  \\\nSpecial_Events                                                          \nBeatles Tribute         146630.870000     86767.966667   21431.026667   \nMusicFest               107296.750000     83544.713333   33065.490000   \nNight Glow              107957.660000     59590.776667   31730.376667   \nNo Event                 73213.523333     40692.877500   15931.317500   \nTaylor Swift Lookalike   96895.336667     67165.716667   33479.076667   \nVintage Days            114238.410000     51074.406667   24561.976667   \n\n                        Revenue_Arcade  \nSpecial_Events                          \nBeatles Tribute           10488.050000  \nMusicFest                  9745.910000  \nNight Glow                 8637.820000  \nNo Event                   3624.482500  \nTaylor Swift Lookalike     9691.783333  \nVintage Days               6821.316667  \n\n\nTable analysis: Stand out points & limitations  Looking at the table, it‚Äôs clear that special events like Beatles Tribute and MusicFest bring in noticeably higher revenue across all areas compared to days with no events. This makes sense, as popular events tend to draw bigger crowds and encourage people to spend more on tickets, food, and merchandise. Some of the standout numbers are probably because certain events are more well-known or heavily promoted. That said, this analysis only looks at averages and doesn‚Äôt consider other factors like weather, day of the week, or seasonal trends. Because of these other influences, it‚Äôs hard to say for sure how much of the revenue boost is truly caused by the events themselves.\n\n\n\n\nlobster25['Date'].info()\n\n&lt;class 'pandas.core.series.Series'&gt;\nRangeIndex: 99 entries, 0 to 98\nSeries name: Date\nNon-Null Count  Dtype \n--------------  ----- \n99 non-null     object\ndtypes: object(1)\nmemory usage: 924.0+ bytes\n\n\nPython currently views this variable as an object.\n\n# Converting the variable into a datetime object.\nlobster25['Date'] = pd.to_datetime(lobster25['Date'])\n\n# Check to see if the conversion is successful\nprint(lobster25['Date'].dtype)\n\ndatetime64[ns]\n\n\nConverting a column to datetime lets Python treat it as a real date, enabling operations like filtering by month, extracting weekdays, calculating durations, and plotting time-series data accurately. Without it, dates are just text and harder to analyze.\n\n\n\n\nlobster25.groupby('Day_of_Week')[['Total_Revenue']].describe()\n\n\n\n\n\n\n\n\nTotal_Revenue\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nDay_of_Week\n\n\n\n\n\n\n\n\n\n\n\n\nFriday\n14.0\n124348.351429\n25766.281104\n68899.07\n113828.1850\n126337.460\n134174.9625\n180408.42\n\n\nMonday\n15.0\n123881.241333\n30711.357993\n88753.13\n101210.0300\n110997.990\n143166.6050\n202565.15\n\n\nSaturday\n14.0\n233435.688571\n63186.045667\n132174.43\n209613.2550\n227397.180\n238614.4925\n423019.37\n\n\nSunday\n14.0\n222040.763571\n51689.800794\n127310.71\n211008.1775\n245217.160\n251046.1250\n284520.15\n\n\nThursday\n14.0\n110808.970000\n23883.366451\n63247.57\n94773.4125\n112346.540\n128805.5325\n147562.14\n\n\nTuesday\n14.0\n110803.409286\n28858.768417\n64017.91\n95816.2750\n106621.260\n132957.8850\n166042.42\n\n\nWednesday\n14.0\n111780.540000\n24591.350227\n75302.24\n92629.4175\n111242.325\n127296.6300\n165467.64\n\n\n\n\n\n\n\nLooking at the table, it‚Äôs pretty noticeable that weekends bring in much higher revenue than weekdays. Saturday averages around $233,000 and Sunday about $222,000, while most weekdays hover between $110,000 and $125,000. This makes sense since more people are likely to visit the park on their days off. The bigger variation in weekend revenues probably comes from special events or particularly busy days. Overall, the day of the week has a noticeable effect, with weekends clearly driving the biggest crowds and sales.\n\n\n\n\nlobster25 = lobster25.drop(columns=['Zone_ID'], errors='ignore')\n# Double-check\nprint(lobster25.columns)\n\nIndex(['Date', 'Day_of_Week', 'season_week_rel', 'Is_Holiday',\n       'Total_Visitors', 'Passholder_Percentage', 'Day_Tickets_Sold',\n       'Avg_Ticket_Price', 'Gate_Revenue', 'Revenue_FoodBev', 'Revenue_Merch',\n       'Revenue_Arcade', 'Total_Revenue', 'Total_Labor_Hours',\n       'International_Visitors', 'High_Temperature', 'Precipitation',\n       'Weather_Type', 'Weather_Code', 'Promo_Flag', 'Promo_Type',\n       'Cruise_Docked', 'Is_Special_Event', 'Special_Events',\n       'Attraction_Tier', 'Per_Capita_Spend'],\n      dtype='object')"
  },
  {
    "objectID": "python_notebooks/AD654_Nhat_Tran_Assignment_1.html#ii.-visualization",
    "href": "python_notebooks/AD654_Nhat_Tran_Assignment_1.html#ii.-visualization",
    "title": "Data Exploration & Visualization",
    "section": "II. Visualization",
    "text": "II. Visualization\n\nUsing any plotting tool in Python, generate a bar plot that shows Promo_Type on one axis and average gate revenue on the other. Be sure to give your plot a clear, descriptive title.\n\nsns.barplot(x = \"Promo_Type\", y = \"Gate_Revenue\", data = lobster25, hue=\"Promo_Type\", palette=\"Set2\", dodge=False, edgecolor='black');\nplt.xlabel(\"Promotion Type\")\nplt.ylabel(\"Avg. Gate Revenue ($)\")\nplt.title(\"Lobsterland's Average Gate Revenue during Promotion Types\")\nplt.show()\n\n\n\n\n\n\n\n\nThe bar plot shows that days with no promotion, email, or influencer campaigns have similar average gate revenue, while bundle promotions have noticeably lower revenue. Surprisingly, bundles seem less effective than doing nothing or running other promotions, possibly because they attract fewer people or are offered on slower days. Overall, not all promotions necessarily increase gate revenue.\n\n\nLobster Land management wants to better understand how visitor counts vary across the days of the week. Generate a barplot that shows average total visitors by day of Week.\n\n# Version 1: y-axis starting at 0\nsns.barplot(x = \"Day_of_Week\", y = \"Gate_Revenue\", data = lobster25, color='skyblue', edgecolor='black');\nplt.xlabel(\"Day of the Week\")\nplt.ylabel(\"Avg Gate Revenue ($)\")\nplt.title(\"Lobsterland's Average Gate Revenue on Days of the Week\")\nplt.ylim(0)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Version 2: y-axis zoomed to highlight differences\n\navg_values = lobster25.groupby(\"Day_of_Week\")[\"Gate_Revenue\"].mean()\n\nsns.barplot(x=\"Day_of_Week\", y=\"Gate_Revenue\", data=lobster25, color='skyblue', edgecolor='black')\nplt.xlabel(\"Day of the Week\")\nplt.ylabel(\"Avg Gate Revenue ($)\")\nplt.title(\"Lobsterland's Average Gate Revenue on Days of the Week\")\n\n# Set y-axis limits slightly beyond min and max average values\nplt.ylim(avg_values.min() * 0.95, avg_values.max() * 1.05)\nplt.show()\n\n\n\n\n\n\n\n\nBoth plots display the same visitor data, but they feel very different. The first plot, with the y-axis starting at 0, shows the weekend boost in visitors clearly but keeps the differences in perspective. The second plot, with a zoomed-in y-axis, makes those differences look much larger and more dramatic than they really are. This kind of change can easily influence how people interpret the importance of the trend. That‚Äôs why, for bar charts, it‚Äôs usually best to start the axis at 0 ‚Äî it helps present the data honestly and avoids misleading your audience.\n\n\nNext, generate a histogram of per capita spend across all visitors in the 2025 season. Be sure to give your plot a clear, descriptive title.\n\n#Version 1: Non-adjusted bins\nsns.histplot(data=lobster25, x=\"Per_Capita_Spend\", color='skyblue', edgecolor='black')\nplt.xlabel(\"Per Capita Spend ($)\")\nplt.title(\"Lobsterland Per Capita Spend in 2025\")\nplt.show()\n\n\n\n\n\n\n\n\nThe histogram shows that most visitors spend between about $70 and $85, with spending peaking around $80. The distribution is roughly bell-shaped, suggesting that per capita spending is fairly consistent across visitors, with only a few spending much lower or higher than the average.\n\n#Version 2: 50 bins\nsns.histplot(data=lobster25, x=\"Per_Capita_Spend\", bins=50, color='skyblue', edgecolor='black')\nplt.xlabel(\"Per Capita Spend ($)\")\nplt.title(\"Lobsterland Per Capita Spend in 2025\")\nplt.show()\n\n\n\n\n\n\n\n\nThe second histogram, with 50 bins, shows a lot more detail about how people spend. Instead of the smooth, bell-like shape in the first plot, you can see smaller peaks and dips, almost like there are a few ‚Äúfavorite‚Äù spending amounts that visitors cluster around. Someone looking at this version might notice that spending habits are a bit more varied and not perfectly smooth, which isn‚Äôt as clear in the first histogram.\n\n\nOne member of the Lobster Land Board of Directors thinks that people tend to buy more merch on hot days. Let‚Äôs see whether the data supports this theory! Generate a scatterplot with the day‚Äôs high temperature on the x-axis, and with total merch revenue on the y-axis. Be sure to give your plot a clear, descriptive title.\n\nsns.scatterplot(x = \"High_Temperature\", y = \"Revenue_Merch\", data = lobster25, color='skyblue');\nplt.xlabel(\"High Temperature\")\nplt.ylabel(\"Avg. Merch Revenue ($)\")\nplt.title(\"Lobsterland's Merch Revenue on Hot Temperature Day\")\nplt.show()\n\n\n\n\n\n\n\n\nThe scatterplot shows no clear relationship between high temperature and merch revenue ‚Äî the points are spread fairly randomly across the temperature range. This suggests that hotter days do not consistently lead to higher merch sales. It‚Äôs possible that factors like crowd size, special events, or promotions play a bigger role in driving merch revenue than temperature alone.\n\n\nCreate a boxplot that shows day of week on one axis and passholder percentage on the other. Be sure to give your plot a clear, descriptive title.\n\nsns.boxplot(x=\"Day_of_Week\", y=\"Passholder_Percentage\", data=lobster25, color='skyblue')\nplt.xlabel(\"Day of the Week\")\nplt.ylabel(\"Passholder Percentage (%)\")\nplt.title(\"Lobsterland's Passholder Percentage during Day of the Week\")\nplt.show()\n\n\n\n\n\n\n\n\nThe boxplot shows that weekdays, especially Wednesday and Thursday, tend to have higher passholder percentages compared to weekends. Saturday has the lowest median and a narrower range, meaning fewer passholders visit that day. This might be because weekends attract more casual or one-time visitors, while passholders prefer visiting during less crowded weekdays.\n\n\nCreate a barplot that shows average daily total visitors by month.\n\n#create a new 'Month' column extracted from 'Date'\nlobster25['Month'] = lobster25['Date'].dt.month_name()\n\nmonthly_avg = lobster25.groupby('Month', as_index=False)['Total_Visitors'].mean()\n\nsns.barplot(x=\"Month\", \n            y=\"Total_Visitors\", \n            data=monthly_avg, \n            hue=\"Month\", \n            palette=\"Set2\", \n            dodge=False, \n            edgecolor='black')\nplt.xlabel(\"Month\")\nplt.ylabel(\"Avg. Daily Total Visitors\")\nplt.title(\"Average Daily Total Visitors at Lobsterland by Month\")\nplt.show()\n\n\n\n\n\n\n\n\nThe bar plot shows that average daily visitors are highest in September and lowest in May and July. There‚Äôs a clear upward trend from August into September, suggesting strong late-season attendance. I would tell management that September appears to be a key month for drawing visitors, which could be a good time to focus marketing campaigns or launch special fall events to maximize engagement and revenue.\n\n\nGenerate a barplot that shows special events on one axis and average international visitors on the other.\n\navg_visitor = lobster25.groupby(\"Special_Events\")[\"International_Visitors\"].mean().sort_values(ascending=False)\nprint(avg_visitor)\n\nSpecial_Events\nTaylor Swift Lookalike    236.666667\nMusicFest                 201.333333\nBeatles Tribute           194.000000\nVintage Days              178.666667\nNight Glow                172.000000\nNo Event                  167.738095\nName: International_Visitors, dtype: float64\n\n\n\norder_event = avg_visitor.index\nprint(order_event)\n\nIndex(['Taylor Swift Lookalike', 'MusicFest', 'Beatles Tribute',\n       'Vintage Days', 'Night Glow', 'No Event'],\n      dtype='object', name='Special_Events')\n\n\n\nsns.barplot(x=\"Special_Events\", \n            y=\"International_Visitors\", \n            data=lobster25, \n            order=order_event, \n            errorbar=None, \n            hue=\"Special_Events\", \n            palette=\"Set3\", \n            dodge=False, \n            edgecolor='black')\nplt.xlabel(\"Special Events\")\nplt.ylabel(\"Avg. International Visitors\")\nplt.xticks(rotation=45)\nplt.title(\"Lobsterland's Average International Visitors on Special Events\")\nplt.show()\n\n\n\n\n\n\n\n\nThe barplot shows that the Taylor Swift Lookalike event attracts the highest average number of international visitors, followed by MusicFest and Beatles Tribute. Days with no events have the lowest international attendance. This suggests that special events, especially music-related ones, may encourage more international guests to visit. However, we cannot assume the events directly cause the increase, other factors like peak tourist season or holidays could also explain the higher numbers. This is why the relationship should be viewed as a correlation, not proof of causation.\n\n\nCreate a faceted bar plot of food & beverage revenue, with facets for weather type, and bars representing day of the week.\n\nsns.catplot(x=\"Day_of_Week\", \n            y=\"Revenue_FoodBev\", \n            col=\"Weather_Type\", \n            data=lobster25, \n            kind=\"bar\",\n            color='skyblue', \n            edgecolor='black',\n            errorbar=None).fig.suptitle(\"Food & Beverage Revenue by Day of the Week, Faceted by Weather Type\", fontsize=16, y=1.05)\nplt.xlabel(\"Day of the Week\")\nplt.ylabel(\"Food & Beverage Revenue\")\nplt.show()\n\n\n\n\n\n\n\n\nThis plot shows that food and beverage sales really take off on weekends, especially when the weather is nice or just a bit cloudy. Thunderstorms seem to keep people away no matter the day, while sunny or partly cloudy Saturdays and Sundays bring in the biggest revenue. It‚Äôs interesting that even rainy weekends still perform better than most weekdays. However, this visualization does not show the number of observations (sample size) for each weather-day combination, making it hard to know whether some patterns are based on only a few data points.\n\n\nCreate a lineplot that shows season_week_rel on the x-axis, and aggregate (total) Gold Zone spending for that week on the y-axis.\n\nlobster25[['Attraction_Tier','Per_Capita_Spend']]\n\n\n\n\n\n\n\n\nAttraction_Tier\nPer_Capita_Spend\n\n\n\n\n0\n2\n73.43\n\n\n1\n2\n59.30\n\n\n2\n3\n82.33\n\n\n3\n2\n91.19\n\n\n4\n1\n58.67\n\n\n...\n...\n...\n\n\n94\n2\n81.26\n\n\n95\n1\n79.43\n\n\n96\n3\n82.08\n\n\n97\n2\n68.70\n\n\n98\n1\n84.26\n\n\n\n\n99 rows √ó 2 columns\n\n\n\n\n# Filter attraction tier to gold zone which is 1, copy() from lobster25() dataframe\ngold_zone = lobster25.query('Attraction_Tier==1').copy() \n\n# Double-check to see if there's any zone other than 1\nprint(gold_zone['Attraction_Tier'].unique())\n\n[1]\n\n\n\n# Add a 'Total_Spend' column to record the total spending of gold zone by multiplying total visitors by per visitor spend\ngold_zone['Total_Spend'] = gold_zone['Total_Visitors']*gold_zone['Per_Capita_Spend']\n\n# Double-check to see if the tier and the calculation is correct\ngold_zone[['Attraction_Tier','Total_Spend','season_week_rel']].tail()\n\n\n\n\n\n\n\n\nAttraction_Tier\nTotal_Spend\nseason_week_rel\n\n\n\n\n88\n1\n131917.20\n13\n\n\n89\n1\n209116.95\n13\n\n\n91\n1\n110994.19\n14\n\n\n95\n1\n138287.63\n14\n\n\n98\n1\n202561.04\n15\n\n\n\n\n\n\n\n\n# Aggregrate the total spending of gol\ngold_spend_weekly = gold_zone.groupby('season_week_rel')['Total_Spend'].sum().reset_index()\n\n# Plot the line chart\nsns.lineplot(data=gold_spend_weekly, x='season_week_rel',y='Total_Spend', color='skyblue')\nplt.xlabel(\"Week of the Season\")\nplt.ylabel(\"Total Spending\")\nplt.title(\"Lobsterland's Gold Zone Weekly Total Spending of the Season\")\nplt.show()\n\n\n\n\n\n\n\n\nThis line plot shows that Gold Zone spending starts low during opening week and rises quickly, peaking around Weeks 4‚Äì5. After that, there‚Äôs a big drop, though Week 7 shows a nice little rebound before things taper off again toward the end of the season. This tells us guests spend the most when excitement is fresh and maybe when special events happen. I‚Äôd suggest management double down on promotions early in the season and figure out what drove that Week 7 spike so we can recreate it."
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "You can download my CV here: CV (PDF)"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Curriculum Vitae",
    "section": "Education",
    "text": "Education\nBoston University, Boston, MA\nM.S. in Business Analytics | Expected May 2026\nDrexel University, Philadelphia, PA\nB.S. in Economics | 2017 ‚Äì 2021\nMinor: Finance | Concentration: Business Economics"
  },
  {
    "objectID": "cv.html#skills",
    "href": "cv.html#skills",
    "title": "Curriculum Vitae",
    "section": "Skills",
    "text": "Skills\n\nData Analysis Tools: Bloomberg, Power BI, Tableau, Macrobond, Haver, Microsoft Office (Word, Excel, PowerPoint)\n\nProgramming Languages: Python, SQL, Eviews, R\nTechnical Skills: Data Visualizations, Data Manipulation, Database Management, Macro Research, ML Models"
  },
  {
    "objectID": "cv.html#professional-experience",
    "href": "cv.html#professional-experience",
    "title": "Curriculum Vitae",
    "section": "Professional Experience",
    "text": "Professional Experience\n\nBrandywine Global Investment Management, Philadelphia, PA\nGlobal Macro Research Analyst | Feb 2022 ‚Äì Aug 2024\n\nManaged and maintained large economic datasets and visualizations to support research and analysis.\n\nConducted macroeconomic studies and built models to guide investment strategies.\n\nDeveloped client presentations and marketing materials to communicate research insights.\n\n\n\n\nKPIM Joint Stock Company, Hanoi, Vietnam\nBusiness Intelligence Analyst | Mar 2021 ‚Äì Sep 2021\n\nDeveloped dashboards and reports to monitor financial performance and support decision-making.\n\nAnalyzed business data to identify trends, opportunities, and potential risks.\n\nStreamlined reporting processes and coordinated with teams to ensure accuracy.\n\n\n\n\nErnst & Young LLP, Hanoi, Vietnam\nIntern ‚Äì Assurance Services | May 2020 ‚Äì Sep 2020\n\nPerformed financial analysis and evaluation of client performance.\nImplemented process improvements and extracted data-driven insights for advisory projects.\nCollaborated with cross-functional teams to complete engagements efficiently.\n\n\n\n\nCity of Philadelphia ‚Äì Streets Department, Philadelphia, PA\nIntern ‚Äì Office of the Deputy Commissioner of Streets | Apr 2019 ‚Äì Sep 2019\n\nMonitored project budgets and maintained financial records.\n\nOrganized and processed data to enhance reporting accuracy.\nPrepared performance reports to inform departmental decision-making."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I‚Äôm currently pursuing my Master‚Äôs in Business Analytics (MSBA) at Boston University, where I‚Äôm focused on applying data to understand markets and improve decision-making. Prior to this, I studied finance and economics at Drexel University, which sparked my interest in how numbers tell the story behind business trends. I‚Äôve worked on projects involving predictive modeling, marketing analytics, NLP, and data visualization, which has strengthened my ability to connect technical analysis with practical, real-world applications."
  },
  {
    "objectID": "about.html#hi-im-nhat",
    "href": "about.html#hi-im-nhat",
    "title": "About",
    "section": "",
    "text": "I‚Äôm currently pursuing my Master‚Äôs in Business Analytics (MSBA) at Boston University, where I‚Äôm focused on applying data to understand markets and improve decision-making. Prior to this, I studied finance and economics at Drexel University, which sparked my interest in how numbers tell the story behind business trends. I‚Äôve worked on projects involving predictive modeling, marketing analytics, NLP, and data visualization, which has strengthened my ability to connect technical analysis with practical, real-world applications."
  },
  {
    "objectID": "about.html#skills",
    "href": "about.html#skills",
    "title": "About",
    "section": "Skills",
    "text": "Skills\n\nProgramming: Python; SQL; EViews; R\nData Analysis Tools: Bloomberg; Power BI; Tableau; Macrobond; Haver; Microsoft Office: Word, Excel, PowerPoint\nTechnical Skills: Data Visualizations; Data Manipulation; Database Management; Macro Research; ML Models"
  },
  {
    "objectID": "about.html#what-im-interested-in",
    "href": "about.html#what-im-interested-in",
    "title": "About",
    "section": "What I‚Äôm Interested In",
    "text": "What I‚Äôm Interested In\nI‚Äôm passionate about turning raw data into actionable insights that solve real-world business problems, uncover market trends, and create tools that make complex information easy to understand. Outside of work, I love exploring the world through travel, photography, and trying new cuisines, often visiting local restaurants to discover hidden gems."
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "About",
    "section": "Contact",
    "text": "Contact\nFor inquiries or collaboration, please visit Contact ‚Äî happy to connect anytime :)"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Selected Projects",
    "section": "",
    "text": "1. Gender Disparities in Labor Force Analysis (2024)\nWebsite Research Paper PPT\n\nLabor Market Analytics Research Project | Sep 2025 ‚Äì Dec 2025\n\nAnalyzed Lightcast and FRED data in Python, applying regression and ML models to forecast 2024 labor-market trends\nBuilt visualizations to assess skill demand, salary patterns, geographic variation, and gender-based differences across U.S. markets\nSynthesized model results with academic research to deliver actionable insights on hiring behavior and gender disparities\n\n\n\n\n2. Factors That Influence Airbnb Prices in Geneva\nResearch Paper PPT\n\nApplied Machine Learning for Airbnb Pricing in Geneva | Sep 2025 ‚Äì Dec 2025\n\nApplied Python for data visualization, regression, k-NN, classification trees, NLP transformers, and clustering to analyze Airbnb pricing in Geneva\nEngineered features from structured and unstructured listing data to build predictive and market segmentation models\nGenerated actionable insights to inform Airbnb hosts‚Äô pricing decisions and listing strategies, and to support platform-level market segmentation in the Geneva market\n\n\n\n\n3. Bangladesh Garment Industry: Health and Environmental Impacts\nResearch Paper\n\nEconomics Seminar Research Paper | Jan 2021 ‚Äì Mar 2021\n\nConducted a literature review on short- and long-term effects of Bangladesh‚Äôs garment industry on health and environment\nExamined workers‚Äô health, safety evaluations, and child labor issues using historical data\nProposed strategies to mitigate risks while supporting sustainable industry growth"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "I‚Äôm always happy to connect! You can reach me through any of the platforms below:\n\nüíº LinkedIn üíª GitHub üìß Email\n\n\nYou can also return to the About tab to learn more about me."
  },
  {
    "objectID": "contact.html#get-in-touch",
    "href": "contact.html#get-in-touch",
    "title": "Contact",
    "section": "",
    "text": "I‚Äôm always happy to connect! You can reach me through any of the platforms below:\n\nüíº LinkedIn üíª GitHub üìß Email\n\n\nYou can also return to the About tab to learn more about me."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nhat Tran",
    "section": "",
    "text": "I‚Äôm a Business Analytics graduate student at Boston University, with hands-on experience in macro research, financial analytics, and data-driven decision-making. My academic foundation in finance and economics from Drexel University helps me bridge quantitative analysis with practical business insight.\nI specialize in:\n\nDesigning automated workflows for efficient, reproducible analytics\nBuilding predictive and explanatory models\nTransforming complex datasets into clear, actionable insights\nSupporting business and investment decisions with evidence-based recommendations\n\nFeel free to explore the site to learn more about my work, projects, and professional experience."
  },
  {
    "objectID": "certifications.html",
    "href": "certifications.html",
    "title": "Certifications",
    "section": "",
    "text": "Verified certifications that demonstrate expertise in technology, data analytics, and financial markets.\n\n  \n    AWS Academy Cloud Foundations\n  \n\n  \n    Developed a strong understanding of cloud computing concepts, AWS services, architecture, security, and pricing, enhancing my ability to leverage cloud technologies for business innovation and digital transformation.\n\n    \n      AWS\n      Cloud Computing\n      Security\n      Digital Transformation\n    \n\n    \n      View Credential\n    \n  \n\n\n  \n    Bloomberg Market Concepts (BMC)\n  \n\n  \n    Completed the self-paced BMC certification, covering core pillars of the financial markets including economics, currencies, fixed income, and equities through the Bloomberg Terminal. Strengthened financial literacy, market analysis, and data-driven decision-making skills.\n\n    \n      Financial Markets\n      Equities\n      Fixed Income\n      Data Analysis\n    \n\n    \n      View Credential"
  },
  {
    "objectID": "pynotes.html",
    "href": "pynotes.html",
    "title": "Python Notebooks",
    "section": "",
    "text": "Data Exploration & Visualization\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "coursework.html",
    "href": "coursework.html",
    "title": "Relevant Coursework",
    "section": "",
    "text": "M.S. in Business Analytics, Boston University\n\n\n\n\n  \n  \n  Applied data manipulation, querying, and analytics workflows using Python and SQL.\n  \n    Python\n    SQL\n    Data Manipulation\n    ETL\n  \n\n  \n  \n  Core analytical methods, data-driven problem solving, and statistical reasoning.\n  \n    R\n    Machine Learning\n    Power BI\n    Statistical Reasoning\n  \n\n  \n  \n  Scalable data processing, cloud-based analytics architectures, and deployment concepts.\n  \n    Python\n    Machine Learning\n    SQL\n    plotly\n    AWS Cloud Architecture\n  \n\n  \n  \n  Customer segmentation, predictive modeling, and performance measurement using data.\n  \n    Python\n    Tableau\n    Regression\n    Clustering\n    Classification\n    Random Forest\n  \n\n\n\nTechniques for extracting insights from structured and unstructured data using Python, R, SQL, and Power BI.\n\n  Python\n  R\n  SQL\n  NLP\n  Clustering\n  Classification\n\n\n\n\nLearn to leverage risk analytics to identify opportunities, mitigate threats, and enhance enterprise value using advanced analytics tools.\n\n  Python\n  R\n  SQL\n  Optimization\n\n\n  \n  \n  Decision analysis using quantitative models complemented by qualitative frameworks.\n  \n    R\n    Optimization\n    Scenario Modeling\n  \n\n  \n  \n  Financial statement analysis, valuation fundamentals, and analytics applications in finance.\n  \n    Financial Modeling"
  },
  {
    "objectID": "coursework.html#graduate-coursework",
    "href": "coursework.html#graduate-coursework",
    "title": "Relevant Coursework",
    "section": "",
    "text": "M.S. in Business Analytics, Boston University\n\n\n\n\n  \n  \n  Applied data manipulation, querying, and analytics workflows using Python and SQL.\n  \n    Python\n    SQL\n    Data Manipulation\n    ETL\n  \n\n  \n  \n  Core analytical methods, data-driven problem solving, and statistical reasoning.\n  \n    R\n    Machine Learning\n    Power BI\n    Statistical Reasoning\n  \n\n  \n  \n  Scalable data processing, cloud-based analytics architectures, and deployment concepts.\n  \n    Python\n    Machine Learning\n    SQL\n    plotly\n    AWS Cloud Architecture\n  \n\n  \n  \n  Customer segmentation, predictive modeling, and performance measurement using data.\n  \n    Python\n    Tableau\n    Regression\n    Clustering\n    Classification\n    Random Forest\n  \n\n\n\nTechniques for extracting insights from structured and unstructured data using Python, R, SQL, and Power BI.\n\n  Python\n  R\n  SQL\n  NLP\n  Clustering\n  Classification\n\n\n\n\nLearn to leverage risk analytics to identify opportunities, mitigate threats, and enhance enterprise value using advanced analytics tools.\n\n  Python\n  R\n  SQL\n  Optimization\n\n\n  \n  \n  Decision analysis using quantitative models complemented by qualitative frameworks.\n  \n    R\n    Optimization\n    Scenario Modeling\n  \n\n  \n  \n  Financial statement analysis, valuation fundamentals, and analytics applications in finance.\n  \n    Financial Modeling"
  },
  {
    "objectID": "coursework.html#undergraduate-coursework",
    "href": "coursework.html#undergraduate-coursework",
    "title": "Relevant Coursework",
    "section": "Undergraduate Coursework",
    "text": "Undergraduate Coursework\n\n\n  B.S. in Economics, Drexel University\n\n\n\n\n\nMicroeconomics & Macroeconomics\nExplored market behavior, incentives, and macroeconomic dynamics underlying business and policy decisions.\n\n  Economic Theory\n  Market Analysis\n\n\nApplied Econometrics\nUsed regression, hypothesis testing, and time-series techniques to analyze economic and business data.\n\n  Regression\n  Time Series\n  Statistical Analysis\n\n\nDatabase Design & Implementation\nDesigned relational databases and wrote SQL queries to manage, retrieve, and analyze data efficiently.\n\n  SQL\n  Database Management\n  Relational Databases\n\n\nManagement Information Systems\nApplied information systems and database concepts to support business operations and decision-making.\n\n  SQL\n  Business Systems\n\n\nGame Theory Applications\nModeled strategic decision-making and competitive behavior using game-theoretic frameworks.\n\n  Strategic Analysis\n  Decision Theory\n  Economic Modeling\n\n\nIntermediate Corporate Finance\nAnalyzed capital structure, valuation, and investment decisions using financial theory and quantitative methods.\n\n  Valuation\n  Financial Analysis\n\n\nInvestment Securities & Markets\nStudied financial markets, asset pricing, and portfolio construction across major asset classes.\n\n  Financial Markets\n  Asset Pricing\n  Portfolio Analysis"
  }
]